-    --run-command 'useradd zuul'
BREAKS HERE
-  when: ansible_os_family == "RedHat" and ansible_distribution_major_version|int > 5
BREAKS HERE
-          security_group: 'nagios-{{item}}'
BREAKS HERE
-  hosts: proxies-stg:proxies
BREAKS HERE
-      - src: /opt/OPEN_Admin/OPENTLC-OCP3/provision_workshop_env.sh
-      - src: /opt/OPEN_Admin/OPENTLC-OCP3/provision-ose-projects.sh
-      - src: /opt/OPEN_Admin/OPENTLC-OCP3/deploy_scripts
BREAKS HERE
-        openshift_ansible_version: "openshift-ansible-3.10.38-1",
BREAKS HERE
-  - { role: docker_setup, device: '/dev/vdb'}
BREAKS HERE
-       - name: Install pre-defined Undercloud packages for OSP < 13
-             - install.version|default(undercloud_version)|openstack_release < 13
BREAKS HERE
-        project_id: "{{ pid }}"
-        pem_file: "{{ pem }}"
-        service_account_email: "{{ email }}"
-        project_id: "{{ pid }}"
-        pem_file: "{{ pem }}"
-        service_account_email: "{{ email }}"
-        project_id: "{{ pid }}"
-        pem_file: "{{ pem }}"
-        service_account_email: "{{ email }}"
-        project_id: "{{ pid }}"
-        pem_file: "{{ pem }}"
-        service_account_email: "{{ email }}"
-              project_id={{ pid }} pem_file={{ pem }} service_account_email={{ email }}
BREAKS HERE
-      - name: motd lines mathing 'hi'
-      - name: motd lines mathing 'hi'
BREAKS HERE
-            - name: Workarond for BZ #1317312
-              shell: |
-                 source ~/stackrc
-                 nova flavor-delete {{ item[0].name.rstrip('1234567890-').split('-')[-1] }}
-              tags: skip_ansible_lint
-              when: "item[0].name.rstrip('1234567890-').split('-')[-1] in item[1].cmd"
-              with_together:
-                  - "{{ overcloud_facts.nodes | default([]) }}"
-                  - "{{ original_flavors.results }}"
-              ignore_errors: yes
-
BREAKS HERE
-      shell: "oadm policy add-role-to-user -n default osbs-custom-build {{ osbs_koji_prod_username }} && touch /etc/origin/koji-custom-build-policy-added"
-      shell: "oadm policy add-role-to-user -n default osbs-custom-build system:serviceaccount:default:builder && touch /etc/origin/koji-custom-build-policy-added"
BREAKS HERE
-    privatefile: "rabbitmq/{{env}}/pki/private/messaging-bridge{{env_suffix}}.fedoraproject.org.crt"
BREAKS HERE
-    username: "openqa{{ env_suffix }}"
-    queue_name: "openqa{{ env_suffix }}_scheduler"
-    username: "openqa{{ env_suffix }}"
-    queue_name: "openqa{{ env_suffix }}_resultsdb_reporter"
-    username: "openqa{{ env_suffix }}"
-    queue_name: "openqa{{ env_suffix }}_wiki_reporter"
BREAKS HERE
-  when: (plex_prefs.stat.exists == False) and (continuous_integration|bool != True)
-  when: (plex_prefs.stat.exists == False) and (continuous_integration|bool != True)
BREAKS HERE
-    no_docker_storage_setup: false
-      no_docker_storage_setup: "{{no_docker_storage_setup}}"
-  - { role: docker_setup, when: not hostvars['bastion']['no_docker_storage_setup'], device: '/dev/vdb'}
BREAKS HERE
-          cmd_parts: 
BREAKS HERE
-#   [redhat-7.yml, suse-42.yml, ubuntu-16.04.yml]
BREAKS HERE
-    # COMMON ###################################################################
-    - name: "Common: Import EPEL and Remi GPG keys."
-      rpm_key: "key={{ item }} state=present"
-      with_items:
-        - "https://fedoraproject.org/static/0608B895.txt"
-        - "http://rpms.famillecollet.com/RPM-GPG-KEY-remi"
-
-    - name: "Common: Install EPEL and Remi repos."
-      command: "rpm -Uvh --force {{ item.href }} creates={{ item.creates }}"
-      with_items:
-        - href: "http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm"
-          creates: "/etc/yum.repos.d/epel.repo"
-        - href: "http://rpms.famillecollet.com/enterprise/remi-release-6.rpm"
-          creates: "/etc/yum.repos.d/remi.repo"
-
-    - name: "Common: Disable firewall (since this is a dev environment)."
-      service: name=iptables state=stopped enabled=no
-    # SAMPLE APP ###############################################################
-    - name: "Node: Ensure Node.js app folder exists."
-    - name: "Node: Copy example Node.js app to server."
-    - name: "Node: Install app dependencies defined in package.json via npm."
-    - name: "Node: Check list of Node.js apps running."
-    - name: "Node: Start example Node.js app."
-      when: "forever_list.stdout.find('{{ node_apps_location}}/app/app.js') == -1"
BREAKS HERE
-- name: Check for openstack deployment
-  rescue:
-    - name: Notice
-      debug:
-        msg: >-
-          OpenStack setup is not possible, running in without it.
-      when:
-        - not (skydive_openstack_enabled | bool)
BREAKS HERE
-# TODO: This defaults to 'False' upstream, we should set this to 'False' in Liberty
-nova_force_config_drive: True
BREAKS HERE
-      - include_role: name="containers/prepare-templates"
BREAKS HERE
-        chdir: "{{ ansible_user_dir }}/src/{{ zuul.project.canonical_name }}"
-        TEST_EXIT_CODE: "{{ zuul_success }}"
-        src: "{{ ansible_user_dir }}/src/{{ zuul.project.canonical_name }}/logs/"
BREAKS HERE
-        {{ item.0.pg_num }}
-        {{ item.0.pgp_num }}
BREAKS HERE
-    - name: waiting for clean pgs...
-      shell: |
-        test "[""$(docker exec ceph-mon-{{ hostvars[groups[mon_group_name][0]]['ansible_hostname'] }} ceph --cluster {{ cluster }} -s -f json | python -c 'import sys, json; print(json.load(sys.stdin)["pgmap"]["num_pgs"])')""]" = "$(docker exec ceph-mon-{{ hostvars[groups[mon_group_name][0]]['ansible_hostname'] }} ceph --cluster {{ cluster }} -s -f json | python -c 'import sys, json; print [ i["count"] for i in json.load(sys.stdin)["pgmap"]["pgs_by_state"] if i["state_name"] == "active+clean"]')"
-      register: result
-      until: result.rc == 0
-      delegate_to: "{{ groups[mon_group_name][0] }}"
BREAKS HERE
-- include: /srv/web/infra/ansible/playbooks/include/proxies-haproxy.yml
-- include: /srv/web/infra/ansible/playbooks/include/proxies-miscellaneous.yml
BREAKS HERE
-    name: "{{ neutron_packages_list }}"
BREAKS HERE
-      when: ansible_pkg_mgr == 'apt'
-      when: ansible_pkg_mgr == 'apt'
-      when: ansible_pkg_mgr == 'apt'
-      when: ansible_pkg_mgr == 'apt'
-      when: ansible_pkg_mgr == 'apt'
BREAKS HERE
-    SELinux_type: targeted
-    SELinux_mode: enforcing
BREAKS HERE
-    - name: Remove Old MOTD
-      file: 'path={{item}} state=absent'
-      with_items:
-        - /etc/update-motd.d
-    #- name: Import Support Files
-    #  copy: "src={{item}} dest=/etc/update-motd.d/{{item}} force=yes mode=0775"
-    #  with_items:
-    #    - 00-header
-    #    - 10-sysinfo
-    #    - 90-footer
-
-    - name: Import Support Files
-        - 90-footer
-
-    # - name: Symlink
-    #   file:
-    #     src: /var/run/motd
-    #     dest: /etc/mot
-    #     state: link
-    #     force: yes
-
-    # - name: Npm Mods
-    #   npm: 'name={{item}} global=yes'
-    #   with_items:
-    #     - frontail
BREAKS HERE
-      no_log: True
BREAKS HERE
-          build_json_dir: '/usr/share/osbs/',
BREAKS HERE
-          logicalInterconnectUri: "/rest/logical-interconnects/0de81de6-6652-4861-94f9-9c24b2fd0d66"
-             '/rest/ethernet-networks/9e8472ad-5ad1-4cbd-aab1-566b67ffc6a4',
-             '/rest/ethernet-networks/28ea7c1a-4930-4432-854b-30cf239226a2'
-    - name: Do nothing with the Uplink Set when no changes are provided
-      oneview_uplink_set:
-        config: "{{ config }}"
-        state: present
-        data:
-          name: 'Test Uplink Set'
-          networkType: "Ethernet"
-      delegate_to: localhost
-
-    - name: Update the Uplink Set changing the attribute description to 'New Description'
-
-    - name: Do nothing when the Uplink Set is absent
-      oneview_uplink_set:
-        config: "{{ config }}"
-        state: absent
-        data:
-          name: 'Updated Uplink Set'
-      delegate_to: localhost
-      register: deleted
BREAKS HERE
-    ansible_ssh_user: cloud-user
-    ansible_become: yes
BREAKS HERE
-- name: Dump all facts before the installation begins
BREAKS HERE
-    - git: repo={{ tester.rhosqe.git.repo }} version={{ tester.git.rhosqe.revision }}
-    - template: dest="/root/run-tests.sh" src="../templates/run_rhosqe_tests.sh"
BREAKS HERE
-  - name: Wait for host to come back up
BREAKS HERE
-    shell: '/usr/bin/rpm -qa'
-    changed_when: False
-    register: rpmoutput
-  - debug: msg="{{ inventory_hostname}} {{ rpmoutput.results }}"
-    when: rpmoutput is defined and rpmoutput.results|length > 0
BREAKS HERE
-# Prevent users from logging in over ssh if they have an empty password.
-security_sshd_disallow_empty_password: yes                   # RHEL-07-010270
BREAKS HERE
-      name: "{{item}}"
-    with_items:
-      - ansible
-      - python2-dockerfile-parse
BREAKS HERE
-    proxyurl: http://localhost:10035
BREAKS HERE
-  pre_tasks:
-  tasks:
-  environment: "{{ deployment_environment_variables | default({}) }}"
-
-
-  tags:
-    - deploy-dhcpd
BREAKS HERE
-#  roles:
-#    - role: PaloAltoNetworks.paloaltonetworks
BREAKS HERE
-            roles_sshd: "{{ install.version|default(undercloud_version)|openstack_release >= 11 or (tht_version | version_compare('5.2.0', '>=') and tht_release | version_compare('15', '>=')) }}"
BREAKS HERE
-  become: yes
-  
BREAKS HERE
-        
-      ignore_errors: True
BREAKS HERE
-      when: overcloud_deployment_result is defined and overcloud_deployment_result != "0"
-      when: overcloud_deployment_result is defined and overcloud_deployment_result != "0"
-      when: overcloud_deployment_result is defined and overcloud_deployment_result != "0"
-          for failed_deployment in $(heat resource-list --nested-depth 5 overcloud | grep FAILED | grep 'StructuredDeployment ' | cut -d '|' -f3); do heat deployment-show $failed_deployment; done;
-      template: src={{ base_dir }}/khaleesi/playbooks/installer/rdo-manager/templates/show_nodes.sh dest={{ instack_user_home }}/show_nodes.sh mode=0755
-    - name: copy the undercloud id_rsa key back to the slave
-      fetch: src=~/.ssh/id_rsa dest="{{ base_dir }}/khaleesi/id_rsa_undercloud" flat=yes
-
-    - name: copy get-overcloud-nodes.py to undercloud
-      template: src={{ base_dir }}/khaleesi/playbooks/installer/rdo-manager/templates/get-overcloud-nodes.py.j2 dest={{ instack_user_home }}/get-overcloud-nodes.py mode=0755
-
-    - name: fetch overcloud node names and IPs
-      register: overcloud_nodes
-      ignore_errors: yes
-      shell: >
-          source {{ instack_user_home }}/stackrc;
-          python {{ instack_user_home }}/get-overcloud-nodes.py
-
-    - name: add each overcloud node to ansible
-      with_dict: overcloud_nodes.stdout
-      ignore_errors: yes
-      add_host:
-        name={{ item.key }}
-        groups=overcloud
-        ansible_ssh_host={{ item.key }}
-        ansible_fqdn={{ item.value }}
-        ansible_ssh_user="heat-admin"
-        ansible_ssh_private_key_file="{{ lookup('env', 'PWD') }}/id_rsa_undercloud"
-
-- name: regenerate the inventory file after adding hosts
-  hosts: localhost
-  tasks:
-    - name: set_fact for undercloud ip #required for regeneration of ssh.config.ansible
-      set_fact: undercloud_ip={{ hostvars['undercloud']['ansible_default_ipv4']['address'] }}
-
-    - name: create inventory from template
-      template:
-        dest: "{{ lookup('env', 'PWD') }}/{{ tmp.node_prefix }}hosts"
-        src: "{{ base_dir }}/khaleesi/playbooks/provisioner/templates/inventory.j2"
-
-    - name: symlink inventory to a static name
-      file:
-        dest: "{{ lookup('env', 'PWD') }}/hosts"
-        state: link
-        src: "{{ lookup('env', 'PWD') }}/{{ tmp.node_prefix }}hosts"
-
-    - name: regenerate ssh config
-      template: src={{ base_dir }}/khaleesi/playbooks/installer/rdo-manager/templates/ssh_config.j2 dest={{ base_dir }}/khaleesi/ssh.config.ansible mode=0755
-
BREAKS HERE
-            ipv6_postfix: "{{  (install.network.protocol == 'ipv4') | ternary('', '_v6')}}"
BREAKS HERE
-      when:
-      - admin_user is defined
BREAKS HERE
-  hosts: blockerbugs-stg
-  hosts: blockerbugs-stg
BREAKS HERE
-- hosts: master
BREAKS HERE
-    - name: Should not create or update the Fibre Channel Network with no changes provided
-    - name: Should update the Fibre Channel Network changing the attribute autoLoginRedistribution to True
BREAKS HERE
-- name: Create inventory from guest VM data
-  gather_facts: false
-  tasks:
-  - name: Add bastion to dynamic host
-    add_host:
-      name: bastion
-      group: bast
-      ansible_ssh_user: cloud-user
-  - name: Add other hosts to nodes group
-    add_host:
-      name: "{{item.name}}"
-      group: nodes
-      ansible_ssh_user: cloud-user
-    when: item.name != 'bastion'
-    with_items: "{{guests}}"
BREAKS HERE
-    command: subscription-manager repos --disable=*
BREAKS HERE
-      path: "{{user_ssh_dir}}/.ssh/known_hosts"
-      key: "{{ lookup('file', user_ssh_dir+'/.ssh/id_rsa.pub') }}"
BREAKS HERE
-security_shadow_utils_umask: 077                             # RHEL-07-020230
BREAKS HERE
-  - role: nfs/client
-    mnt_dir: '/mnt/fedora_koji'
-    nfs_src_dir: 'fedora_koji'
-    when: not inventory_hostname.startswith('compose-aarch64-01')
-  - role: nfs/client
-    mnt_dir: '/mnt/koji'
-    nfs_src_dir: 'fedora_arm/data'
-    when: inventory_hostname.startswith(('compose-aarch64-01'))
-  - role: nfs/client
-    mnt_dir: '/mnt/koji'
-    nfs_src_dir: 'fedora_ppc/data'
-    when: inventory_hostname.startswith(('compose-ppc64'))
-  - role: nfs/client
-    mnt_dir: '/pub'
-    nfs_src_dir: 'fedora_ftp/fedora.redhat.com/pub'
-    when: inventory_hostname.startswith(('compose-x86-01'))
-    when: not inventory_hostname.startswith(('aarch64-02a'))
BREAKS HERE
-        - (_zuul_cloner_check.rc != 0) or (not item.src | match(".*git.openstack.org.*"))
-      with_items: "{{ roles }}"
BREAKS HERE
-- name: Retreive plex docker iamge
BREAKS HERE
-    - name: Update the Ethernet Network changing bandwidth and purpose
-      oneview_ethernet_network:
-        config: "{{ config }}"
-        state: present
-        data:
-          name: '{{ network_name }}'
-          purpose: Management
-          bandwidth:
-              maximumBandwidth: 3000
-              typicalBandwidth: 2000
-      delegate_to: localhost
-    - name: Reset to the default network connection template
-      oneview_ethernet_network:
-        config: "{{ config }}"
-        state: default_bandwidth_reset
-        data:
-          name: '{{ network_name }}'
-      delegate_to: localhost
-
-    - name: Add Scopes to the Ethernet Network
-      oneview_ethernet_network:
-        config: "{{ config }}"
-        state: scopes_set
-        data:
-          name: "{{ network_name }}"
-          scopeUris: "{{ network_scope_uris }}"
-      delegate_to: localhost
-
-    - name: Rename the Ethernet Network to 'Updated Ethernet Network'
-          name: '{{ network_name }}'
-          newName: 'Updated Ethernet Network'
-      delegate_to: localhost
-
-    - name: Delete the Ethernet Network
-      oneview_ethernet_network:
-        config: "{{ config }}"
-        state: absent
-        data:
-          name: 'Updated Ethernet Network'
-      register: deleted
-    - name: Create Ethernet Networks in bulk
-          vlanIdRange: '1-3,5,7'
-          purpose: General
-          namePrefix: TestNetwork
-          smartLink: false
-          privateNetwork: false
-          bandwidth:
-              maximumBandwidth: 10000
-              typicalBandwidth: 2000
-    - debug: msg="{{ethernet_network_bulk | map(attribute='name') | list }}"
BREAKS HERE
-    - include: "{{ lookup('env', 'PWD') }}/tasks/wait_for_ssh.yml"
-      register: reboot_reason
-    - include: "{{ lookup('env', 'PWD') }}/tasks/reboot.yml"
-      when: reboot_reason|changed
BREAKS HERE
-  tasks:
-    - name: Installing the Nova service (source)
-      include_role:
-        name: "os_nova"
-      vars:
-        nova_novncproxy_git_repo: "{{ openstack_repo_git_url }}/novnc"
-        nova_novncproxy_git_install_branch: "{{ novncproxy_git_install_branch }}"
-        nova_spicehtml5_git_repo: "{{ openstack_repo_git_url }}/spice-html5"
-        nova_spicehtml5_git_install_branch: "{{ spicehtml5_git_install_branch }}"
-        nova_management_address: "{{ management_address }}"
-        nova_cinder_rbd_inuse: "{{ hostvars['localhost']['nova_cinder_rbd_inuse'] }}"
-      when: install_method == "source"
-
-    - name: Installing the Nova service (distro)
-      include_role:
-        name: "os_nova"
-      vars:
-        nova_management_address: "{{ management_address }}"
-        nova_cinder_rbd_inuse: "{{ hostvars['localhost']['nova_cinder_rbd_inuse'] }}"
-      when: install_method == "distro"
-    - name: Configuring system crontab
-      include_role:
-        name: "system_crontab_coordination"
BREAKS HERE
-        - not mon_containerized_deployment
-        - not mon_containerized_deployment
-        - mon_containerized_deployment
-      when: not mon_containerized_deployment
-      when: mon_containerized_deployment
-      when: not mon_containerized_deployment
-      when: mon_containerized_deployment
-      when: not osd_containerized_deployment
-        - not osd_containerized_deployment
-      when: not osd_containerized_deployment
-        - not osd_containerized_deployment
-        - osd_containerized_deployment
-      when: not osd_containerized_deployment
-      when: osd_containerized_deployment
-      when: not osd_containerized_deployment
-      when: osd_containerized_deployment
-        - not mds_containerized_deployment
-        - not mds_containerized_deployment
-        - mds_containerized_deployment
-        - not rgw_containerized_deployment
-        - not rgw_containerized_deployment
-        - rgw_containerized_deployment
BREAKS HERE
-    - name: Install lvm2 yum package
-      yum:
-        - ansible_pkg_mgr == 'yum'
BREAKS HERE
-    number_of_tower_servers: 1
BREAKS HERE
-- include: swift_check_hashes.yml
-  static: no
-- include: swift_pre_install.yml
-  static: no
-- include: swift_install.yml
-  static: no
-- include: swift_post_install.yml
-  static: no
-- include: swift_calculate_addresses.yml
-  static: no
-- include: swift_storage_hosts.yml
-  static: no
-- include: swift_proxy_hosts.yml
-  static: no
-- include: "swift_init_{{ ansible_service_mgr }}.yml"
-  static: no
-- include: swift_service_setup.yml
-  static: no
-- include: swift_key_setup.yml
-  static: no
-- include: swift_rings.yml
-  static: no
BREAKS HERE
----
-- name: Group by distribution for workarounds-prep
-  hosts: all:!localhost
-  tasks:
-    - group_by: key={{ ansible_distribution }}
-
-- name: Group by distribution-version for workarounds-prep
-  hosts: all:!localhost
-  tasks:
-    - group_by: key={{ ansible_distribution }}-{{ ansible_distribution_major_version|int }}
-
-- name: Group hosts by product rdo,rhos
-  hosts: all:!localhost
-  tasks:
-    - group_by: key={{ product.name }}
-      when: product is defined
-  tags:
-    - prep
-
-- name: Group hosts by product version for juno, icehouse, havana
-  hosts: all:!localhost
-  tasks:
-    - group_by: key={{ product.version }}
-      when: product.version is defined
-  tags:
-    - prep
-
-- name: Group hosts by product short_version for RHOS 40, 50, 60
-  hosts: all:!localhost
-  tasks:
-    - group_by: key={{ product.short_version }}
-      when: product.short_version is defined
-  tags:
-    - prep
-
-- name: Group hosts by rdopkg skip_packstack tag
-  hosts: all:!localhost
-  tasks:
-    - group_by: key={{ rdopkg.skip_packstack_run }}
-      when: rdopkg.skip_packstack_run is defined
-
-- name: Group hosts by rdopkg skip_prep tag
-  hosts: all:!localhost
-  tasks:
-    - group_by: key={{ rdopkg.skip_prep }}
-      when: rdopkg.skip_prep is defined
-
-- name: Group hosts by rdopkg skip_rdoupdate tag
-  hosts: all:!localhost
-  tasks:
-    - group_by: key={{ rdopkg.skip_rdoupdate }}
-      when: rdopkg.skip_rdoupdate is defined
-
-- name: Group hosts by rdopkg skip_rdoupdate_preinstall tag
-  hosts: all:!localhost
-  tasks:
-    - group_by: key={{ rdopkg.skip_rdoupdate_preinstall }}
-      when: rdopkg.skip_rdoupdate_preinstall is defined
-
-- name: Group hosts by rdopkg skip_rdoupdate_tempest tag
-  hosts: all:!localhost
-  tasks:
-    - group_by: key={{ rdopkg.skip_rdoupdate_tempest }}
-      when: rdopkg.skip_rdoupdate_tempest is defined
-
-- name: Group hosts by rdopkg skip_tempest_setup tag
-  hosts: all:!localhost
-  tasks:
-    - group_by: key={{ rdopkg.skip_tempest_setup }}
-      when: rdopkg.skip_tempest_setup is defined
BREAKS HERE
-      - name: add hosts to host list
-        add_host:
-            name: "{{ hypervisor.name | default(hypervisor_name) }}"
-            groups: "{{ hypervisor.groups|default([hypervisor_name]) | join(',') }}"
-            ansible_ssh_user: "{{ hypervisor.user }}"
-            ansible_ssh_host: "{{ hypervisor.address }}"
-            ansible_ssh_private_key_file: "{{ hypervisor.key }}"
-      - package:
-            name: libvirt-python
-        register: install_libvirt
-      # Don't cleanup if the libvirtd process is unavailable
-      - include: tasks/cleanup.yml
-        when: not install_libvirt|changed
BREAKS HERE
-    - demo_username: demo
-    - demo_password: "{{ hostvars[controller_name].demo_password | default('redhat') }}"
-    - demo_tenant_name: demo
BREAKS HERE
-- name: Common configuration
-    - name: Ensure the cell directory exists
-      file:
-        path='/usr/share/gridengine/default/common'
-        state=directory
-  roles:
-    - role: nfs-server
-      NFS_EXPORTS:
-        - path: '/home'
-          clients: '{{groups.gridengine_clients}}'
-          options: 'rw,no_root_squash,async'
-        - path: '/usr/share/gridengine/default/common'
-          clients: '{{groups.gridengine_clients + groups.gridengine_master}}'
-          options: 'rw,no_root_squash'
-  tasks:
BREAKS HERE
-    queue_name: "openqa_relvalconsumer{{ relvalconsumer_env_suffix }}_scheduler"
BREAKS HERE
-    - iptables-service
BREAKS HERE
-    - name: Ensure python is installed
-        if which apt-get >/dev/null && ! which python >/dev/null ; then
BREAKS HERE
-
-- name: gather facts and check if using systemd
-  - name: are we using systemd
-    shell: "if [ -d /usr/lib/systemd ] ; then find /usr/lib/systemd/system -name 'ceph*' | wc -l ; else echo 0 ; fi"
-    register: systemd_unit_files
-
-    when:
-      - ansible_os_family == 'RedHat'
-      - systemd_unit_files.stdout != "0"
-    when:
-      - ansible_os_family == 'RedHat'
-      - systemd_unit_files.stdout != "0"
-    when: ansible_os_family == 'RedHat'
-# Ubuntu 14.04
-    when: ansible_distribution == 'Ubuntu'
-    when:
-      - ansible_os_family == 'RedHat'
-      - systemd_unit_files.stdout != "0"
-    when:
-      - ansible_os_family == 'RedHat'
-      - systemd_unit_files.stdout != "0"
-    when: ansible_os_family == 'RedHat'
-# Ubuntu 14.04
-    when: ansible_distribution == 'Ubuntu'
-    when:
-      - ansible_os_family == 'RedHat'
-      - systemd_unit_files.stdout != "0"
-    when:
-      - ansible_os_family == 'RedHat'
-      - systemd_unit_files.stdout != "0"
-# Ubuntu 14.04
-    when: ansible_distribution == 'Ubuntu'
-    when:
-      - ansible_os_family == 'RedHat'
-      - systemd_unit_files.stdout != "0"
-    when:
-      - ansible_os_family == 'RedHat'
-      - systemd_unit_files.stdout != "0"
-    when: ansible_os_family == 'RedHat'
-# Ubuntu 14.04
-    when: ansible_distribution == 'Ubuntu'
-    when:
-      - ansible_os_family == 'RedHat'
-      - systemd_unit_files.stdout != "0"
-    when:
-      - ansible_os_family == 'RedHat'
-      - systemd_unit_files.stdout != "0"
-# before infernalis release, using sysvinit scripts
-# we use this test so we do not have to know which RPM contains the boot script
-# or where it is placed.
-    when: ansible_os_family == 'RedHat'
-# Ubuntu 14.04
-    when: ansible_distribution == 'Ubuntu'
-    when:
-      - ansible_os_family == 'RedHat'
-      - systemd_unit_files.stdout != "0"
-    when:
-      - ansible_os_family == 'RedHat'
-      - systemd_unit_files.stdout != "0"
-    when: ansible_os_family == 'RedHat'
-    when: ansible_distribution == 'Ubuntu'
-# When set to true both groups of packages are purged.
-# This can cause problem with qemu-kvm
-  - name: remove from SysV
-    when: ansible_distribution == 'Ubuntu'
-  - name: remove Upstart and SysV files
-    when: ansible_distribution == 'Ubuntu'
-  - name: remove Upstart and apt logs and cache
-  - name: purge RPM cache in /tmp
BREAKS HERE
-      version_to_upgrade_from: "1.12.0"
-      version_to_upgrade_from: "1.12.0"
-      version_to_upgrade_from: "1.12.0"
BREAKS HERE
-  when: rhel7stig_gui
-  notify: restart ssh
-  when: rhel7stig_gui
-  when: rhel7stig_gui
-  notify: restart ssh
-      name='Run AIDE integrity check weekly'
-      minute={{ rhel7stig_aide_cron["aide_minute"] | default('05') }}
-      hour={{ rhel7stig_aide_cron["aide_hour"] | default('04') }}
-      day={{ rhel7stig_aide_cron["aide_day"] | default('*') }}
-      month={{ rhel7stig_aide_cron["aide_month"] | default('*') }}
-      weekday={{ rhel7stig_aide_cron["aide_weekday"] | default('*') }}
-      job="{{ rhel7stig_aide_cron["aide_job"] }}"
-  command: "true"
-  notify: restart ssh
-  when: "'yes' in rhel_07_040180_audit.stdout"
-  notify: restart ssh
-  notify: restart ssh
-  notify: restart ssh
-  notify: restart ssh
-  notify: restart ssh
-  notify: restart ssh
-  notify: restart ssh
-  notify: restart ssh
-  notify: restart ssh
-  notify: restart ssh
-  notify: restart ssh
-  notify: restart ssh
-  notify: restart ssh
-      - rhel_07_040500_audit.stat.exists
-      - rhel_07_040500_audit.stat.exists
-      - rhel_07_040500_audit.stat.exists
-  when: not rhel7stig_gui
-  when: not rhel7stig_system_is_router
-  when: rhel7stig_gui
-  when: rhel7stig_gui
-  when: "'enabled' in rhel_07_041010_audit.stdout"
BREAKS HERE
-      when: upcloud_created_instances is defined and item['item'] | json_query( '[*].group' ) | length == 1
BREAKS HERE
-    - cassandra
BREAKS HERE
-        become=yes
-  gather_facts: yes
-    debug: yes
BREAKS HERE
-nova_s3_service_publicuri_proto: "{{ nova_s3_service_proto }}"
-nova_s3_service_adminuri_proto: "{{ nova_s3_service_proto }}"
-nova_s3_service_internaluri_proto: "{{ nova_s3_service_proto }}"
-nova_v3_service_publicuri_proto: "{{ nova_v3_service_proto }}"
-nova_v3_service_adminuri_proto: "{{ nova_v3_service_proto }}"
-nova_v3_service_internaluri_proto: "{{ nova_v3_service_proto }}"
-nova_v21_service_publicuri_proto: "{{ nova_v21_service_proto }}"
-nova_v21_service_adminuri_proto: "{{ nova_v21_service_proto }}"
-nova_v21_service_internaluri_proto: "{{ nova_v21_service_proto }}"
-nova_service_publicuri_proto: "{{ nova_service_proto }}"
-nova_service_adminuri_proto: "{{ nova_service_proto }}"
-nova_service_internaluri_proto: "{{ nova_service_proto }}"
-nova_ec2_service_publicuri_proto: "{{ nova_ec2_service_proto }}"
-nova_ec2_service_adminuri_proto: "{{ nova_ec2_service_proto }}"
-nova_ec2_service_internaluri_proto: "{{ nova_ec2_service_proto }}"
BREAKS HERE
-    server_aliases: bzr.fedorahosted.org hg.fedorahosted.org svn.fedorahosted.org 
BREAKS HERE
-          - 'PermitTTY no'
-          - 'PermitTTY no'
BREAKS HERE
-- name: Setup Amazon S3 Website for Student Login
-  hosts: localhost
-  connection: local
-    - { role: aws_workshop_login_page, when: create_login_page is defined and create_login_page }
-- name: Setup Host routes for ansible control node and host1 when in networking mode
-  hosts: "managed_nodes:control_nodes"
-    - { role: network_hostroutes, when: networking is defined and networking }
BREAKS HERE
-  - name: set up a port-forward from our local node to a running Pod
BREAKS HERE
-  when: rhel_07_010030
-- name: "MEDIUM | RHEL-07-010050 | PATCH | The operating system must display the Standard Mandatory DoD Notice and Consent Banner before granting local or remote access to the system via a command line user logon."
-  lineinfile:
-      dest: /etc/ssh/sshd_config
-      regexp: '^#?Banner'
-      line: Banner /etc/issue
-      validate: /usr/sbin/sshd -tf %s
-  notify: restart sshd
-  when: rhel_07_010050
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-010050
-      - ssh
-      - dod_logon_banner
-
-  when: rhel_07_010060
-  when: rhel_07_010061
-  when: rhel_07_010070
-  when: rhel_07_010080
-  when: rhel_07_010081
-  when: rhel_07_010082
-      line: ucredit = -1
-      line: lcredit = -1
-      line: dcredit = -1
-      line: ocredit = -1
-      line: difok = 8
-      line: minclass = 4
-      line: maxclassrepeat = 4
-      line: ENCRYPT_METHOD SHA512
-      line: PASS_MIN_DAYS 1
-      line: PASS_MAX_DAYS 60
-  lineinfile:
-      dest: /etc/pam.d/system-auth
-      regexp: '^(password\s+sufficient\s+pam_unix.so((?!remember=5).)+)$'
-      line: '\1 remember=5'
-      backrefs: yes
-      regexp: '^#?minlen'
-      line: minlen = 15
-- name: "MEDIUM | RHEL-07-010320 | PATCH | Accounts subject to three unsuccessful login attempts within 15 minutes must be locked for the maximum configurable period."
-  command: "true"
-  changed_when: no
-  when: rhel_07_010320
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-010330 | PATCH | If three unsuccessful logon attempts within 15 minutes occur the associated account must be locked."
-  command: "true"
-  changed_when: no
-  when: rhel_07_010330
-  tags:
-      - cat2
-      - medium
-      - patch
-      - notimplemented
-  when: rhel_07_010340
-      line: FAIL_DELAY 4
-      regexp: ^#?permituserenvironment
-      line: permituserenvironment no
-      regexp: '^#?HostbasedAuthentication'
-  command: "true"
-  changed_when: no
-      - notimplemented
-- name: "MEDIUM | RHEL-07-020030 | PATCH | A file integrity tool must verify the baseline operating system configuration at least weekly."
-      name: 'Run AIDE integrity check weekly'
-      job: "{{ rhel7stig_aide_cron.job }}"
-  when: rhel_07_020030
-      - aide
-
-- name: "MEDIUM | RHEL-07-020040 | PATCH | Designated personnel must be notified if baseline configurations are changed in an unauthorized manner."
-  yum:
-      name: aide
-      state: present
-  notify: init aide
-  when: rhel_07_020040
-  tags:
-      - cat2
-      - medium
-      - patch
-  command: "true"
-  changed_when: no
-      - notimplemented
-  command: "true"
-  changed_when: no
-      - notimplemented
-  command: "true"
-  changed_when: no
-      - notimplemented
-- name: "MEDIUM | RHEL-07-021110 | PATCH | If the cron.allow file exists it must be owned by root."
-  command: "true"
-  changed_when: no
-      - notimplemented
-      regexp: (?i)^#?Ciphers
-- block:
-    - name: "MEDIUM | RHEL-07-040160 | PATCH | All network connections associated with a communication session must be terminated at the  the session or after 10 minutes of inactivity from the user at a command prompt, except to fulfill documented and validated mission requirements."
-      lineinfile:
-        create: yes
-        dest: "{{ rhel7stig_shell_session_timeout.file }}"
-        regexp: ^#?TMOUT
-        line: "TMOUT={{rhel7stig_shell_session_timeout.timeout}}"
-
-    - name: "MEDIUM | RHEL-07-040160 | PATCH | All network connections associated with a communication session must be terminated at the  the session or after 10 minutes of inactivity from the user at a command prompt, except to fulfill documented and validated mission requirements."
-      lineinfile:
-        create: yes
-        dest: "{{ rhel7stig_shell_session_timeout.file }}"
-        regexp: ^#?readonly TMOUT$
-        line: readonly TMOUT
-
-    - name: "MEDIUM | RHEL-07-040160 | PATCH | All network connections associated with a communication session must be terminated at the  the session or after 10 minutes of inactivity from the user at a command prompt, except to fulfill documented and validated mission requirements."
-      lineinfile:
-        create: yes
-        dest: "{{ rhel7stig_shell_session_timeout.file }}"
-        regexp: ^#?export TMOUT$
-        line: export TMOUT
-  command: "true"
-  changed_when: no
-      - notimplemented
-      regexp: ^#?ClientAliveInterval
-      regexp: ^#?RhostsRSAAuthentication
-      regexp: ^#?ClientAliveCountMax
-      regexp: ^#?IgnoreRhosts
-      regexp: ^#?PrintLastLog
-      regexp: ^#?PermitRootLogin
-      regexp: ^#?IgnoreUserKnownHosts
-      regexp: (?i)^#?MACs
-      regexp: (?i)^#?gssapiauthentication
-      regexp: (?i)^#?kerberosauthentication
-      regexp: (?i)^#?strictmodes
-      regexp: (?i)^#?useprivilegeseparation
-      regexp: (?i)^#?compression
-- name: "MEDIUM | RHEL-07-040670 | PATCH | Network interfaces must not be in promiscuous mode."
-  command: "true"
-  changed_when: no
-  when: rhel_07_040670
-      - notimplemented
-- name: "MEDIUM | RHEL-07-041001 | PATCH | The operating system must have the required packages for multifactor authentication installed."
-  yum:
-     name:
-       - esc
-       - pam_pkcs11
-       - authconfig-gtk
-     state: present
-  when:
-      - rhel7stig_gui
-      - rhel_07_041001
-  command: "true"
-  changed_when: no
-  when: rhel_07_041002
-      - notimplemented
BREAKS HERE
-    url: localhost/datagrepper/raw
BREAKS HERE
-- name: Retrieving CA Certificate
-  slurp:
-    src: "{{ letsencrypt_ca_file_path }}"
-  register: ca_file_b64
-
-- set_fact:
-    ca_file: "{{ ca_file_b64.content | b64decode }}"
-
-- name: Retrieve master named certificate
-  shell: ls "{{ letsencrypt_ca_file_dir }}" | grep -o "^master.*cer$"
-  register: master_named_certificate
-
-- name: Append CA Certificate to Master Named Certificate
-  blockinfile: |
-    dest="{{ letsencrypt_ca_file_dir }}/{{ master_named_certificate.stdout }}" backup=yes
-    content={{ ca_file }}
-
BREAKS HERE
-        path: "{{ jenkins_home }}/plugins/greenballs"
BREAKS HERE
-    name: {{ item }}
BREAKS HERE
-      internal_network: "{{ prefix }}{{ node_dict.external_network }}"
BREAKS HERE
-    - apt: name=build-essentials
BREAKS HERE
-# Detect whether the init system is upstart of systemd.
-- name: Check init system
-  command: cat /proc/1/comm
-  changed_when: false
-  register: _pid1_name
-  tags:
-    - always
-
-- name: Set the name of pid1
-  set_fact:
-    pid1_name: "{{ _pid1_name.stdout }}"
-  tags:
-    - always
-
BREAKS HERE
-              {%-   if folder_path | match("^" ~ zuul_git_src_dir ~ "/openstack/openstack-ansible-(?!tests).*") %}
-            dest: "{{ lookup('env', 'ANSIBLE_ROLE_DEP_DIR') }}/{{ item | regex_replace('openstack/openstack-ansible-', '') }}"
BREAKS HERE
-      - name: Print installed repositores
-        shell: "yum repolist -d 7"
-
BREAKS HERE
-- name: "MEDIUM | RHEL-07-040180 | PATCH | The operating system must implement cryptography to protect the integrity of Lightweight Directory Access Protocol (LDAP) authentication communications."
-  command: "true"
-  changed_when: no
-  when:
-      - "'yes' in rhel_07_040180_audit.stdout"
-      - rhel_07_040180
BREAKS HERE
-    # NOTE(mhayden): Using package_state=present on CentOS 7 should allow for
-    - name: Use present for package_state on CentOS 7
-        package_state: "{{ (ansible_os_family == 'RedHat') | ternary('present', 'latest') }}"
BREAKS HERE
-
BREAKS HERE
-  tags: doveco
-  tags: dovecot2
BREAKS HERE
-- block:
-    - name: "MEDIUM | RHEL-07-040180 | PATCH | The operating system must implement cryptography to protect the integrity of Lightweight Directory Access Protocol (LDAP) authentication communications."
-      changed_when: no
-      ignore_errors: yes
-      check_mode: no
BREAKS HERE
-      shell: ping -c 3 8.8.8.8
BREAKS HERE
-# Set glance_default_store to "swift" if using Cloud Files or swift backend
BREAKS HERE
-    - include: common-tasks/package-cache-proxy.yml
BREAKS HERE
-    test_debian_java_version: '1.9.0'
-      oracle_java_version: "{{ test_redhat_java_version }}"
BREAKS HERE
-nexus_anonymous_access: true
-  write_policy: allow # allow_once or allow # Note: nexus restore wont work with 'allow_once'
-  write_policy: allow # allow_once or allow # Note: nexus restore wont work with 'allow_once'
-  write_policy: allow # allow or allow
-  write_policy: allow # allow_once or allow # Note: nexus restore wont work with 'allow_once'
-  write_policy: allow # allow_once or allow # Note: nexus restore wont work with 'allow_once'
-  write_policy: allow # allow_once or allow # Note: nexus restore wont work with 'allow_once'
-  write_policy: allow # allow_once or allow # Note: nexus restore wont work with 'allow_once'
BREAKS HERE
-  - fedmsg/base
BREAKS HERE
-    - name: Locate the largest writable data disk if mnaio_data_disk is not set
-      shell: >
-        lsblk -brndo NAME,TYPE,FSTYPE,RO,SIZE | awk '/d[b-z]+ disk +0/{ if ($4>m){m=$4; d=$1}}; END{print d}'
-      register: lsblk
-      changed_when: false
-
-    - name: Setup the data disk partition
-      parted:
-        device: "/dev/{{ mnaio_data_disk | default(lsblk.stdout) }}"
-        label: gpt
-        number: 1
-        name: data1
-        state: present
-      register: _add_partition
-
-    - name: Prepare the data disk file system
-      filesystem:
-        fstype: ext4
-        dev: "/dev/{{ mnaio_data_disk | default(lsblk.stdout) }}1"
-        force: yes
-        - _add_partition is changed
-
-    - name: Mount the data disk
-      mount:
-        src: "/dev/{{ mnaio_data_disk | default(lsblk.stdout) }}1"
-        path: /data
-        state: mounted
-        fstype: ext4
BREAKS HERE
-  - name: Ensure the Meteor directory environment exists
-    file: name=/opt/{{ item.dest }} state=directory
-    with_items:
-      - dest: meteor
-      - dest: meteor/bin
-
-  - name: Fetch Meteor tarball
-    get_url:
-      url: "{{ rocket_chat_meteor_tarball_remote }}"
-      dest: /opt/meteor/meteor-{{ rocket_chat_meteor_version }}.tar.gz
-
-  - name: Unarchive the Meteor tarball
-    shell: >
-      tar xf
-      /opt/meteor/meteor-{{ rocket_chat_meteor_version }}.tar.gz
-    args:
-      chdir: /opt/meteor
-      creates: /opt/meteor/.meteor
-
-  - name: "Register the Meteor executable's real path"
-    command: readlink /opt/meteor/.meteor/meteor
-    register: rocket_chat_meteor_bin_path
-    changed_when: False
-
-  - name: Link the Meteor environment data to more sensible destinations
-    file:
-      src: /opt/meteor/.meteor/{{ item }}
-      dest: /opt/meteor/bin/meteor
-      state: link
-    with_items: rocket_chat_meteor_bin_path.stdout_lines
-
-  - name: Ensure Rocket.Chat source code is present
-    git:
-      repo: https://github.com/RocketChat/Rocket.Chat
-      dest: /var/www/rocket.chat
-      version: "{{ rocket_chat_build_version }}"
-    # This doesn't currently work due to a bug in Ansible 1.8 onwards:
-    # https://github.com/ansible/ansible/issues/13182
-    # This has been fixed in Ansible 2.0
-    #notify: Upgrade Rocket.Chat
-    tags: build
-
-  - name: Build the Rocket.Chat application using Meteor
-    shell: /opt/meteor/bin/meteor build --server {{ rocket_chat_service_host }} --directory /var/www/rocket.chat
-    args:
-      chdir: /var/www/rocket.chat
-      creates: /var/www/rocket.chat/bundle
-    tags: build
-    npm: state=present path=/var/www/rocket.chat/bundle/programs/server
BREAKS HERE
-nova_rpc_backend: rabbit
-nova_quota_networks: 3
BREAKS HERE
-        - "/opt/var"
BREAKS HERE
-- name: "After overcloud container workarounds (remove it ASAP)"
-      - install.version|default(hostvars[groups['undercloud'][0]].undercloud_version) | openstack_release > 11
BREAKS HERE
-                  'source_storage_type': item[dr_source_map + 'storage_type'] | default('EMPTY_ELEMENT', true),
-                  'dest_storage_type': item[dr_target_host + 'storage_type'] | default('EMPTY_ELEMENT', true),
BREAKS HERE
-  template: src="{{ item }}" dest="/home/copr/provision/{{ item }}"
-  - templates/provision/*.yml
-- name: put provisioning files
-  # synchronize doesn't work at lockbox due to some rsync/scp error, todo: revert when resolved
-  # synchronize: src="provision/files/" dest="/home/copr/provision/files/"
-  copy: src="provision/files" dest="/home/copr/provision/files"
-  tags:
-  - provision_config
-
BREAKS HERE
-    # - role: jetstream
-    #   jetstream_cleanup: yes
-    #   when: jetstream is defined
BREAKS HERE
-    - debug: var=hostvars
-
BREAKS HERE
-        - name: Create /etc/pcidp
-          file: path=/etc/pcidp state=directory
-
-        - name: Configure SR-IOV DP allocation pool
-          template:
-            src: roles/network-multus/templates/sriovdp-config.json
-            dest: /etc/pcidp/config.json
-
BREAKS HERE
-      name: "ceph-osd-prepare-{{ ansible_hostname }}-dev{{ item | regex_replace('/', '') }}"
-      name: "ceph-osd-{{ ansible_hostname }}-dev{{ item | regex_replace('/', '') }}"
-      --name ceph-osd-zap-{{ ansible_hostname }}-dev{{ item | regex_replace('/', '') }} \
-      docker ps | grep -sq ceph-osd-zap-{{ ansible_hostname }}-dev
-      name: "ceph-osd-zap-{{ ansible_hostname }}-dev{{ item | regex_replace('/', '') }}"
BREAKS HERE
-    - include: tasks/file-servers/tftp.yml
BREAKS HERE
-# Test that users/projects etc are consistent on both keystone hosts
-- name: Playbook for functional testing keystone
-  hosts: keystone_all
-  user: root
-  gather_facts: false
-    - name: Check the keystone api
-      uri:
-        url: "http://localhost:{{ item }}"
-        status_code: 300
-      register: result
-      until: result.status == 300
-      retries: 5
-      delay: 10
-      with_items:
-        - 5000
-        - 35357
-      keystone:
-        command: get_user
-        user_name: "{{ item }}"
-        endpoint: "{{ keystone_service_adminurl }}"
-        login_user: "{{ keystone_admin_user_name }}"
-        login_password: "{{ keystone_auth_admin_password }}"
-        login_project_name: "{{ keystone_admin_tenant_name }}"
-      no_log: true
-      keystone:
-        command: get_project
-        project_name: "{{ item }}"
-        endpoint: "{{ keystone_service_adminurl }}"
-        login_user: "{{ keystone_admin_user_name }}"
-        login_password: "{{ keystone_auth_admin_password }}"
-        login_project_name: "{{ keystone_admin_tenant_name }}"
-      no_log: true
-
-  vars_files:
-    - common/test-vars.yml
BREAKS HERE
-          - set_fact:
-              _nginx_users: "{{ _nginx_users | combine({row.0: row.1}) }}"
-            vars:
-              row: "{{item.split(':')}}"
-            loop: "{{users_list|flatten}}"
-          - debug: var=_nginx_users
BREAKS HERE
-    - name: Source gitlab server
-      command: cat ./gitlab_server.out
-      register: gitlab_fqdn
-
-    - name: Set gitlab fqdn fact
-      set_fact:
-        gitlab_server: "{{ gitlab_fqdn.stdout_lines }}"
-
-      debug: msg="{{ gitlab }}"
-        url: "{{ gitlab_fqdn }}/api/v4/projects"
-        url: "{{ gitlab_fqdn }}/api/v4/users"
BREAKS HERE
-  - cat /etc/nginx/sites-enabled/foo
-  - cat /etc/nginx/sites-enabled/bar
-  - cat /etc/nginx/conf.d/proxy
-  - cat /etc/nginx/conf.d/upstream 
-  - ls /etc/nginx/sites-enabled
BREAKS HERE
-      content: "{{ hostvars['pgbdr01.stg.phx2.fedoraproject.org']['drop_script'].stdout }}"
BREAKS HERE
-# This playbook deploys a whole new stack with the current `deploy_flag` (see
-# main.yml)
-  # unique identifier (deploy_flag) that will be used to create unique OpenShift
-  # object names & tags and connect them. This identifier contains the
-    set_fact: deploy_flag="{{ lookup('pipe', 'date +%F-%H-%M-%S-$(cat /proc/sys/kernel/random/uuid | cut -c -8)') }}"
-  - debug: msg="Deploying {{ project_name }} - {{ domain_name }} ({{ deploy_flag }})"
BREAKS HERE
-- debug:
-    msg: "|||FEDORA|||| {{ ansible_distribution_major_version }}"
-
BREAKS HERE
-    logical_switch_group_name: 'Logical Switch Group Cisco Nexus 55xx'  # Logical Switch Group which the logical switch is derived from
-    ip_address_switch_1: '172.18.16.1'  # IP address/hostname of Switch 1
-    ip_address_switch_2: '172.18.16.2'  # IP address/hostname of Switch 2
-    ssh_username: ''  # SSH Username
-    ssh_password: ''  # SSH Password
-    - name: Update the Logical Switch name and credentials
-            newName: 'Test Logical Switch - Renamed'
-    - name: Reclaim the top-of-rack switches in the logical switch
-      oneview_logical_switch:
-        config: "{{ config }}"
-        state: refreshed
-        data:
-          logicalSwitch:
-            name: 'Test Logical Switch - Renamed'
-      delegate_to: localhost
-
-    - name: Delete the Logical Switch
-      oneview_logical_switch:
-        config: "{{ config }}"
-        state: absent
-        data:
-          logicalSwitch:
-            name: 'Test Logical Switch - Renamed'
-      delegate_to: localhost
BREAKS HERE
-  hosts: ceph-iscsi-gw
BREAKS HERE
-  when:
-    - neutron_developer_mode | bool
-
-- name: Clone requirements git repository
-  git:
-    repo: "{{ neutron_requirements_git_repo }}"
-    dest: "/opt/requirements"
-    clone: yes
-    update: yes
-    version: "{{ neutron_requirements_git_install_branch }}"
-  when:
-    - neutron_developer_mode | bool
-
-- name: Add constraints to pip_install_options fact for developer mode
-  set_fact:
-    pip_install_options_fact: "{{ pip_install_options|default('') }} --constraint /opt/developer-pip-constraints.txt --constraint /opt/requirements/upper-constraints.txt"
-  when:
-    - neutron_developer_mode | bool
-
-- name: Set pip_install_options_fact when not in developer mode
-  set_fact:
-    pip_install_options_fact: "{{ pip_install_options|default('') }}"
-  when:
-    - not neutron_developer_mode | bool
-    extra_args: "{{ pip_install_options_fact }}"
-    extra_args: "{{ pip_install_options_fact }}"
BREAKS HERE
-- name: Create and deploy BGP configurations
-  - include_vars: 
-      file: "{{ services }}"
-      name: vpnv4
-
-  - include_vars: "{{ item }}"
-    with_first_found:
-      - nodes.yml
-      - "{{ inventory_dir }}/nodes.yml"
-    tags: [ configs,deploy ]
-
-  - name: Create configuration directory
-    local_action: file path={{configs}} state=directory
-    run_once: true
-
-  - set_fact: customers="{{ lookup('template','list-of-customers.json.j2') }}"
-
-  - name: Create configurations
-    template: src=vpnv4-config.j2 dest={{configs}}/{{inventory_hostname}}.vpnv4.cfg
-    tags: [ configs ]
-
-  - name: Deploy configurations
-    ios_config:
-      provider: "{{ios_provider}}"
-      src: "{{configs}}/{{inventory_hostname}}.vpnv4.cfg"
BREAKS HERE
-    command: "subscription-manager repos --enable=rhel-7-server-rpms --enable=rhel-7-server-extras-rpms --enable=rhel-7-server-ose-{{ose_ver}}-rpms --enable=rhel-7-fast-datapath-rpms --enable=rhel-7-server-optional-rpms"
BREAKS HERE
-    - patch_rhel
BREAKS HERE
-
-    vm_name: zimswintestvm
-      name: '{{resource_group}}'
-      name: '{{vm_name}}'
-      resource_group: '{{resource_group}}'
-      name: '{{vm_name}}'
-      virtual_network_name: '{{vm_name}}'
-      resource_group: '{{resource_group}}'
-      name: '{{vm_name}}'
-      resource_group: '{{resource_group}}'
-      admin_password: '{{vm_password}}'
-      name: '{{vm_name}}'
-      resource_group: '{{resource_group}}'
-      storage_account_name: '{{vm_name}}'
-      virtual_network_name: '{{vm_name}}'
-      subnet_name: '{{vm_name}}'
-      resource_group: '{{resource_group}}'
-      virtual_machine_name: '{{vm_name}}'
BREAKS HERE
-  sudo: yes
-      sudo: yes
-    - ansible-rundeck
BREAKS HERE
-- name: Apply role ceph
-  hosts:
-    - ceph-mon
-    - ceph-osd
-    - ceph-rgw
-  serial: '{{ serial|default("0") }}'
-  roles:
-    - { role: ceph,
-        tags: ceph,
-        when: enable_ceph | bool }
-
BREAKS HERE
-    - debug: var=mysql_replication_master
BREAKS HERE
- 
BREAKS HERE
-        path: /etc/systemd/system/ceph-mds@.service
-        state: absent
-        path: /etc/systemd/system/ceph-rgw@.service
-        state: absent
-        path: /etc/systemd/system/ceph-rbd-mirror@.service
-        state: absent
-        path: /etc/systemd/system/ceph-osd@.service
-        state: absent
-        path: /etc/systemd/system/ceph-mon@.service
-        state: absent
BREAKS HERE
-      - block:
-
-          - name: update IP of overcloud nodes
-            vars:
-                # Here can't be used {{ path_venv }}, because it's not a Jinja template
-                ansible_python_interpreter: "/var/tmp/venv_shade/bin/python"
-            os_server_facts:
-                cloud: undercloud
-                # Required for SSL
-                validate_certs: no
-            delegate_to: "{{ groups.shade | first }}"
-
-          # todo(obaranov) Remove hosts from the 'unused' group
-          # Currently ansible does not allow to remove existing host from a group
-          - name: add hosts to host list
-            add_host:
-                name: "{{ item.name }}"
-                # only add groups for new nodes. don't touch existing nodes' groups
-                groups: "{{ ( item.name in groups.all ) | ternary(omit,
-                    ['overcloud_nodes', 'openstack_nodes', item.name.split('-')[0]] | join(',')
-                   ) }}"
-                ansible_ssh_user: "{{ user }}"
-                ansible_ssh_pass: ""
-                ansible_ssh_host: "{{ item.accessIPv4 }}"
-                ansible_ssh_private_key_file: "{{ overcloud_pkey }}"
-            with_items: "{{ openstack_servers }}"
-          - name: list ironic nodes
-            # Use CSV format as JSON not supported in old versions
-            shell: |
-                source ~/stackrc
-                openstack baremetal {{ 'node' if install.version|default(undercloud_version)|openstack_release >= 10 else '' }} list -f csv
-            register: ironic_node_list
-            tags: skip_ansible_lint
-            # Bug in Liberty version: No "node list" in openstack client, and no formatter in ironic client.
-            when: install.version|default(undercloud_version)|openstack_release != 8
-          - name: try to find unused nodes and add them into a new unused group
-            # Set empty list for Liberty version
-            with_items: "{{ ironic_node_list.stdout|default('')|from_csv }}"
-            vars:
-                # take ironic UUID if name is missing
-                host_id: "{{ item.Name or item.UUID }}"
-            when:
-                - host_id not in openstack_servers|map(attribute='name')|list
-                - host_id in groups.all
-            add_host:
-                name: "{{ host_id }}"
-                # groups can only be added
-                groups: unused
-            path_venv: "/var/tmp/venv_shade"
BREAKS HERE
-  tags: [configuration, ssh-keys, ssh-keys-known-hosts]
BREAKS HERE
-    - { role: bie-index,        bie_index_version: "1.4.1" }
BREAKS HERE
-      schema_check: 
-        data: "{{ data }}"
BREAKS HERE
-    - { role: ceph-mgr, when: "ceph_release_num.{{ ceph_release }} >= ceph_release_num.luminous" }
BREAKS HERE
-  when: not ansible_distribution == "Debian"
-  when: ansible_distribution == "Debian" and ansible_distribution_major_version|int <= 7
-  when: ansible_distribution_major_version|int >= 7
BREAKS HERE
-      mode: 0640
BREAKS HERE
-    - name: database users permissions
-      postgresql_user: name=${item}app db=$item
-                       priv=SELECT,INSERT,UPDATE,DELETE
-      with_items:
-        - hyperkitty
-        - kittystore
-        - postorius
BREAKS HERE
-    - playbooks/test-vars.yml
BREAKS HERE
-    when: inventory_hostname in groups['master'] or inventory_hostname in groups['node']
BREAKS HERE
-  static: no
BREAKS HERE
-  dnf: state=absent name=["{{ base_pkgs_erase }}"]
-  dnf: state=present name=["{{ base_pkgs_inst }}"]
BREAKS HERE
-            - name: 'openstack/openstack-ansible-(.*)'
BREAKS HERE
-    - name: Check if IPv6 mod enabled on host
-      shell: "lsmod | grep ipv6"
-      register: ipv6_mod
-          - ipv6_mod.rc != 0
-          - ipv6_sys.rc != 0
BREAKS HERE
-# This playbook contains operations to perform package management task on
BREAKS HERE
-    - name: Prepare ceph disks
-      script: "{{ kolla_ansible_full_src_dir }}/tests/setup_ceph_disks.sh"
BREAKS HERE
-  hosts: "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_nfs') | replace('-', '_') }}"
BREAKS HERE
-security_disallow_blank_password_login: yes             # RHEL-07-010260
-security_reset_perm_ownership: yes                # RHEL-07-010010
BREAKS HERE
-  post_tasks:
-    - name: Stat 20auto-upgrades file
-      stat:
-        path: /etc/apt/apt.conf.d/20auto-upgrades
-      register: auto_upgrades_file
-      when:
-        - not check_mode
-        - stig_version == 'rhel6'
-        - ansible_pkg_mgr == 'apt'
-    - name: Slurp contents of 50unattended-upgrades file
-      slurp:
-        src: /etc/apt/apt.conf.d/50unattended-upgrades
-      register: unattended_upgrades_file_encoded
-      when:
-        - not check_mode
-        - stig_version == 'rhel6'
-        - ansible_pkg_mgr == 'apt'
-    - name: Decode slurp'd 50-unattended-upgrades file
-      set_fact:
-        unattended_upgrades_file: "{{ unattended_upgrades_file_encoded.content | b64decode }}"
-      when:
-        - not check_mode
-        - stig_version == 'rhel6'
-        - ansible_pkg_mgr == 'apt'
-    - name: Ensure auto updates has been enabled
-      assert:
-        that:
-          - auto_upgrades_file.stat.exists
-      when:
-        - not check_mode
-        - stig_version == 'rhel6'
-        - ansible_pkg_mgr == 'apt'
-    - name: Ensure that auto update notifications has been enabled
-      assert:
-        that:
-          - "'\nUnattended-Upgrade::Mail \"root\";\n' in unattended_upgrades_file"
-      when:
-        - not check_mode
-        - stig_version == 'rhel6'
-        - ansible_pkg_mgr == 'apt'
BREAKS HERE
-  include_tasks: neutron_install_source.yml
-- include_tasks: neutron_apparmor.yml
BREAKS HERE
-            postgresql_port: "{{ qe_quay_database_port|d('5432') }}"
-        postgresql_port: "{{ qe_clair_database_port|d('5433') }}"
BREAKS HERE
-            1) Visit https://apiserver-service-catalog.{{ openshift_hostname }}
BREAKS HERE
-# Copyright 2017, Rackspace US, Inc.
-      command: "virsh destroy {{ hostvars[item]['server_hostname'] }}"
-    - name: Create the VM template
-      template:
-        src: kvm/kvm-vm.xml
-        dest: "/etc/libvirt/qemu/{{ hostvars[item]['server_hostname'] }}.xml"
-        mode: 0644
-        owner: root
-        group: root
-      when:
-        - hostvars[item]['server_vm'] | default(false) | bool
-      with_items: "{{ groups['pxe_servers'] }}"
-
-      command: "virsh define /etc/libvirt/qemu/{{ hostvars[item]['server_hostname'] }}.xml"
-    - name: Create the VM
-      command: "virsh create /etc/libvirt/qemu/{{ hostvars[item]['server_hostname'] }}.xml"
-      failed_when: false
-      command: "virsh start {{ hostvars[item]['server_hostname'] }}"
-- name: Wait for deploy host
-    - name: Wait for connectivity 1
-      local_action:
-        module: wait_for
-        host: "{{ ansible_host }}"
-        connect_timeout: 10
-        port: 22
-        sleep: 20
-        timeout: 1500
-        state: started
-        search_regex: OpenSSH
-
-    - name: copy host keys
BREAKS HERE
-- name: create beaker user
-  mysql_user: name={{beaker_server_admin_user}} password={{beaker_server_admin_pass}} priv=beaker.*:ALL,GRANT state=present
BREAKS HERE
-        when: "install.version|openstack_release >= 12"
-        when: "install.version|openstack_release >= 12"
BREAKS HERE
-#  gather_facts: yes
-  gather_facts: no
-        - mailutils
-    # - role: plone
-    # - role: haproxy
-    #   when: install_loadbalancer
-    # - role: varnish
-    #   when: install_proxycache
BREAKS HERE
-- hosts: biocache-cassandra
-    - biocache-cassandra
BREAKS HERE
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      vars:
-        user_name: "{{ tacker_galera_user }}"
-        password: "{{ tacker_container_mysql_password }}"
-        login_host: "{{ tacker_galera_address }}"
-        db_name: "{{ tacker_galera_database }}"
-      when: inventory_hostname == groups['tacker_all'][0]
-
-  vars:
-    tacker_galera_address: "{{ internal_lb_vip_address }}"
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - tacker
BREAKS HERE
-    - "./env_vars.yml"
-    - "./env_secret_vars.yml"
BREAKS HERE
-- hosts: deploy
BREAKS HERE
-  - include: "{{ tasks }}/yumrepos.yml"
BREAKS HERE
-      when: install_glusterfs|bool
BREAKS HERE
-# This playbook create the ACME controler in a project.
-# The definition files are lookup form github maintainer repository.
BREAKS HERE
-          {%- for pin in local_packages.results.0.item.role_requirement_files.default.global_pins.pinned_packages %}
-      with_items: "{{ local_packages.results.0.item.role_requirement_files.default.global_pins.pinned_packages }}"
BREAKS HERE
-      static:
BREAKS HERE
-            name: selinux
-            name: selinux
BREAKS HERE
-        when: enable_central_logging | bool }
-        when: enable_central_logging | bool }
BREAKS HERE
-  - { role: ansible-role-adauth, tags: [ 'auth' ] }
-  - { role: ansible-role-pam, tags: [ 'auth' ] }
BREAKS HERE
-    zeppelin_comiple_flag: '-Pcassandra-spark-1.6 -Phadoop-2.6 -Pyarn -Ppyspark -DskipTests -P build-distr'
BREAKS HERE
-    - {role: postgresql, pg_version: "9.6"}
BREAKS HERE
-      - "{{ raw_journal_devices }}"
-      - "{{ raw_journal_devices }}"
BREAKS HERE
-  - { openvpn/client
BREAKS HERE
-- include: nova_init_common.yml
-- include: nova_db_setup.yml
-  when: inventory_hostname == groups['nova_api_os_compute'][0]
BREAKS HERE
-            _r_docker_labels: "{{ labels.combine(docker_band_lbls) }}"
-          band.service.title: SSL Renewer
-          band.service.def_position: 0x6
BREAKS HERE
-    - name: restart ceph rgws with systemd
-        state: restarted
-    - name: restart ceph rgws with sysvinit
-        state: restarted
-    - name: restart ceph rgws with upstart
-        state: restarted
BREAKS HERE
-security_rhel7_remove_shosts_files: yes                      # RHEL-07-040330
BREAKS HERE
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      vars:
-        user_name: "{{ magnum_galera_user }}"
-        password: "{{ magnum_galera_password }}"
-        login_host: "{{ magnum_galera_address }}"
-        db_name: "{{ magnum_galera_database_name }}"
-      when: inventory_hostname == groups['magnum_all'][0]
-
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - "magnum"
BREAKS HERE
-    selinux_change_running: 1
BREAKS HERE
-        osbs_environment: [
-          ],
BREAKS HERE
-  - { role: ansible-role-pdsh-machines, tags: [ 'pdsh', 'machines' ] }
BREAKS HERE
-    - { role: php, use_php56: true, use_xdebug: true }
BREAKS HERE
-- include: repo-clone-mirror.yml
BREAKS HERE
-- name:  Setup undercloud and overcloud vms
-  gather_facts: yes
-- name:  Inventory the undercloud
-  gather_facts: yes
-      inventory: undercloud
-- name:  Install undercloud and deploy overcloud
-  gather_facts: no
-
BREAKS HERE
-
BREAKS HERE
-        region_name: RegionOne
BREAKS HERE
-    - name: get current fsid
-      register: cluster_uuid
-        dest: "{{ fetch_directory }}/{{ fsid }}/{{ ceph_conf_key_directory }}/"
BREAKS HERE
-    - Restart Keystone APIs
-    - Restart service
-    - Restart Keystone APIs
-    - Restart service
-    - Restart Keystone APIs
-    - Restart service
BREAKS HERE
-          koji_hub: 'https://koji.fedoraproject.org/kojihub'
BREAKS HERE
-   - { role: nfs/client, mnt_dir: '/srv/taskotron/',  nfs_src_dir: 'fedora_taskotron_dev', nfs_mount_opts: 'rw,hard,bg,intr,noatime,nodev,nosuid,nfsvers=4', when: deployment_type == 'dev' }
-   - { role: nfs/client, mnt_dir: '/srv/taskotron/',  nfs_src_dir: 'fedora_taskotron_stg', nfs_mount_opts: 'rw,hard,bg,intr,noatime,nodev,nosuid,nfsvers=4', when: deployment_type == 'stg' }
-   - { role: nfs/client, mnt_dir: '/srv/taskotron/',  nfs_src_dir: 'fedora_taskotron_prod', nfs_mount_opts: 'rw,hard,bg,intr,noatime,nodev,nosuid,nfsvers=4', when: deployment_type == 'prod' }
BREAKS HERE
-    dest="/srv/lobste.rs/http/{{ item.file }}"
-    - { file: 'public', mode: '0600' }
BREAKS HERE
-        url: '{{ ARTIFACT_URL }}'
-        dest: '/tmp/{{ APP_NAME }}'
BREAKS HERE
-- hosts: all 
-  gather_facts: yes 
-    - name: Baseline Machine 
-      service: name=docker state=started 
-      user: 
-          group=ansiblelab 
-          generate_ssh_key=yes 
-      shell: docker network create -d bridge --internal --subnet={{ cidr }} ansiblelab_nw
-          
-        path: "{{ item }}" 
-        dockerfile: Dockerfile 
-      shell: docker build -t ansible_lab/{{ item }}:latest {{ item }}
-      user: 
BREAKS HERE
-    - stripes-docker
BREAKS HERE
-- name: Check galera cluster status
-    - name: Check if mysql is running
-      command: /usr/bin/mysqladmin --defaults-file=/etc/mysql/debian.cnf ping
-      ignore_errors: true
-      register: mysql_running
-
-    - fail:
-        msg: "The cluster may be broken, mysql is not running but appears to be installed. Fix it before re-running the playbook or override with 'openstack-ansible -e galera_ignore_cluster_state=true galera-install.yml'."
-      when:
-        - not galera_ignore_cluster_state | bool
-        - mysql_running.rc == 1
-        - mysql_running.stderr | search("Check that mysqld is running and that the socket")
-
-    - name: Gather mysql facts
-      mysql_status_facts:
-      ignore_errors: true
-
-    - fail:
-        msg: "The cluster may be broken, mysql is running but unable to gather mysql facts. Fix it before re-running the playbook or override with 'openstack-ansible -e galera_ignore_cluster_state=true galera-install.yml'."
-      when:
-        - not galera_ignore_cluster_state | bool
-        - mysql_running.rc == 0
-        - "{{ 'mysql_status' not in hostvars[inventory_hostname] }}"
-
-    - fail:
-        msg: "wsrep_incoming_addresses does not match across the cluster. Please fix before re-running the playbooks or override with 'openstack-ansible -e galera_ignore_cluster_state=true galera-install.yml'."
-      with_items: play_hosts
-      when:
-        - not galera_ignore_cluster_state | bool
-        - mysql_running.rc == 0
-        - hostvars[item]['mysql_running']['rc'] == 0
-        - hostvars[inventory_hostname]['mysql_status']['wsrep_incoming_addresses'] != hostvars[item]['mysql_status']['wsrep_incoming_addresses']
-
-    - set_fact:
-        galera_existing_cluster: false
-
-    - set_fact:
-        galera_existing_cluster: true
-      with_items: play_hosts
-      when: hostvars[item].mysql_running.rc == 0
-  vars:
-    galera_ignore_cluster_state: false
-  tags:
-    - discover-galera-cluster-state
-
-- name: Install galera server
-  hosts: galera_all
-  serial: 1
-  max_fail_percentage: 20
-  user: root
-  pre_tasks:
-    ansible_ssh_host: "{{ container_address }}"
-    is_metal: "{{ properties.is_metal|default(false) }}"
BREAKS HERE
-    - fail: msg="Installed ansible version {{ ansible_version.full }}, but ansible version >= 2.4 required"
-      when: ansible_version.full | version_compare('2.4.0.0', '<')
BREAKS HERE
-    apt: "name={{item}} state=present"
-    with_items:
BREAKS HERE
-- name: digicert cert hotfix
-  copy: src=digicert-intermediate.pem dest=/etc/pki/ca-trust/source/anchors/digicert-intermediate.pem
BREAKS HERE
-  hosts: nagios-new
BREAKS HERE
-    serviceport: 443-tcp
BREAKS HERE
-    no_log: 'no'
BREAKS HERE
-      shell: ls -1 {{ item }}
-      changed_when: false
-      with_items:
-        - /etc/ceph/*
-        - /var/log/ceph/*
-        src: "{{ item }}"
-      with_items:
-        - "{{ ceph_collect.stdout_lines }}"
BREAKS HERE
-        when: "'hypervisor' not in groups and undercloud_public_ip"
BREAKS HERE
-         - "spark_defaults['sparn_executor_memory'] : {{ hostvars['localhost']['spark_defaults']['spark_executor_memory'] }}"
BREAKS HERE
-            - "install.version|openstack_release > 9"
BREAKS HERE
-        nfs_host: groups['support'].0
BREAKS HERE
-      src: "{{ item }}"
BREAKS HERE
-  - name: delete extra diska images
-      path: "/var/lib/libvirt/images/{{}}.img"
BREAKS HERE
-  when: jenkins_ui.content.find('Jenkins') == -1
BREAKS HERE
-      - name: increase stack_action_timeout to 4 hours (workaround for bz 1243365)
-        sudo: yes
-        command: openstack-config --set /etc/heat/heat.conf DEFAULT stack_action_timeout 14400
-        when: workarounds.enabled is defined and workarounds.enabled|bool
-
-      - name: restart openstack-heat-engine (workaround for bz 1243365)
-        sudo: yes
-        command: systemctl restart openstack-heat-engine
-        when: workarounds.enabled is defined and workarounds.enabled|bool
-
BREAKS HERE
-tuned_profile: 'virtual-host'
BREAKS HERE
-      when: trex_instance_config | default(True)
BREAKS HERE
-  - virtualenv-tools
BREAKS HERE
-    openstack_confd_entries: "{{ confd_overrides[scenario] }}"
-    bootstrap_host_scenario: "{{ scenario }}"
-    scenario: "{{ lookup('env','SCENARIO') | default('aio', true) }}"
-      upgrade:
-        # This starts as an AIO box, then an upgrade is run
-        - name: aodh.yml.aio
-        - name: cinder.yml.aio
-        - name: ceilometer.yml.aio
-        - name: designate.yml.aio
-        - name: glance.yml.aio
-        - name: gnocchi.yml.aio
-        - name: heat.yml.aio
-        - name: horizon.yml.aio
-        - name: keystone.yml.aio
-        - name: neutron.yml.aio
-        - name: nova.yml.aio
-        - name: swift.yml.aio
BREAKS HERE
-      job: "bash /opt/plexguide/menu/pgclone/cloneclean.sh"
BREAKS HERE
-- name: create bridges for interfaces that don't belong to any network
-  command: "brctl addbr {{ item.network }}"
-  ignore_errors: true
-  when: item.bridged|default(false)
-  with_items: "{{ topology_node.interfaces }}"
-
-- name: set bridges to hub mode
-  command: "brctl setageing {{ item.network }} 0"
-  when:
-    - item.bridged|default(false)
-    - item.hub|default(true)
-  with_items: "{{ topology_node.interfaces }}"
-
-- name: bring bridges up
-  command: "ip link set {{ item.network }} up"
-           --network {{ 'bridge' if (interface.bridged|default(False)) else 'network' }}:{{ prefix|default('') }}{{(prefix is defined|ternary('-',''))}}{{ interface.network }}
BREAKS HERE
-          - "(groups[('tag_' ~ env_type ~ '_' ~ guid ~ '_node') | replace('-', '_')] | length) == \
-    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_node') | replace('-', '_') }}:&tag_newnode_true"
-  hosts:
-    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_node') | replace('-', '_') }}:&tag_newnode_true"
-  hosts: "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_bastion') | replace('-', '_') }}"
-  hosts: "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_bastion') | replace('-', '_') }}"
-## Generate new /etc/ansible/hosts file 
-- name: Generate new-version of bastion /etc/ansible/hosts, move new_nodes hosts into nodes group
-    - name: generate ansible hosts file
-      template:
-        src: "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/files/hosts_template.j2" ## path is wrong
-        dest: "{{ ANSIBLE_REPO_PATH }}/workdir/hosts-{{ env_type }}-{{ guid }}"
-
-    - name: Ensure tags 'newnode' are set to false
-      with_items: "{{ groups[('tag_' ~ env_type ~ '_' ~ guid ~ '_node') | replace('-', '_')] | intersect(groups['tag_newnode_true']) }}"
-  hosts: "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_bastion') | replace('-', '_') }}"
-        src: "{{ ANSIBLE_REPO_PATH }}/workdir/hosts-{{ env_type }}-{{ guid }}"
BREAKS HERE
-    - mod-configuration-data
-    - mod-users-data 
-    - mod-users-bl-data
-    - mod-notes-data
BREAKS HERE
-  - name: Ensure Forever (NodeJS) is installed [Ubuntu 14]
-    npm:
-      name: forever
-      global: yes
-      executable: "{{ rocket_chat_original_npm }}"
-    when: (ansible_distribution == "Ubuntu")
-          and (ansible_distribution_major_version == "14")
-
-  - name: Check to see if the Rocket.Chat log file is present [Ubuntu 14]
-    stat: path=/var/log/rocketchat.log
-    register: rocket_chat_log_file_state
-    when: (ansible_distribution == "Ubuntu")
-          and (ansible_distribution_major_version == "14")
-
-  - name: Ensure the Rocket.Chat log file is present [Ubuntu 14]
-      state: touch
-      owner: "{{ rocket_chat_service_user }}"
-      group: "{{ rocket_chat_service_group }}"
-    when: (ansible_distribution == "Ubuntu")
-          and (ansible_distribution_major_version == "14")
-          and not rocket_chat_log_file_state.stat.exists
-
-  - name: Check to see if the Rocket.Chat pid file is present [Ubuntu 14]
-    stat: path=/var/run/rocketchat.pid
-    register: rocket_chat_pid_file_state
-    when: (ansible_distribution == "Ubuntu")
-          and (ansible_distribution_major_version == "14")
-  - name: Ensure the Rocket.Chat pid file is present [Ubuntu 14]
-    file:
-      path: /var/run/rocketchat.pid
-      state: touch
-      owner: "{{ rocket_chat_service_user }}"
-      group: "{{ rocket_chat_service_group }}"
-          and not rocket_chat_pid_file_state.stat.exists
BREAKS HERE
-    - { role: ansible-role-tripleo-cleanup-nfo }
BREAKS HERE
-    shell: echo /dev/$(lsblk -no pkname "{{ item }}")
-      if parted -s "{{ item }}" print | grep -sq boot; then
-        echo "Looks like {{ item }} has a boot partition,"
-      sgdisk -Z "{{ item }}"
-      dd if=/dev/zero of="{{ item }}" bs=1M count=200
BREAKS HERE
-  tags:
-    - keystone-install
-    - keystone-config
-  when:
-    - keystone_service_setup | bool
-  when:
-    - keystone_sp != {}
-  tags:
-    - keystone-install
-
-- include: keystone_token_cleanup.yml
-  when: keystone_apache_enabled | bool
-  when: not keystone_apache_enabled | bool
-  when: not keystone_mod_wsgi_enabled | bool
-  tags:
-    - keystone-install
-    - keystone-config
-
-- include: keystone_ldap_setup.yml
-  when:
-    - keystone_service_setup | bool
-- include: keystone_federation_sp_idp_setup.yml
-  when:
-    - keystone_idp != {}
BREAKS HERE
-      delay: 2
BREAKS HERE
-    shell: "oadm policy add-role-to-user -n default osbs-custom-build {{ osbs_koji_prod_username }} && touch /etc/origin/koji-custom-build-policy-added"
BREAKS HERE
-    - name: Trigger dnsmasq restart
-      command: /bin/true
-      changed_when:
-        - (lxc_net_manage_iptables is defined) and (lxc_net_manage_iptables | bool)
-        - (iptables_clear is defined) and (iptables_clear is changed)
-      notify:
-        - Restart dnsmasq
-
BREAKS HERE
-security_audit_DAC_chown: yes                     # V-38545
-security_audit_DAC_lchown: yes                    # V-38558
-security_audit_DAC_fchown: yes                    # V-38552
-security_audit_DAC_fchownat: yes                  # V-38554
-security_audit_DAC_fremovexattr: yes              # V-38556
-security_audit_DAC_lremovexattr: yes              # V-38559
-security_audit_DAC_fsetxattr: yes                 # V-38557
-security_audit_DAC_lsetxattr: yes                 # V-38561
-security_audit_DAC_setxattr: yes                  # V-38565
BREAKS HERE
-      name: ceph-rbd-mirror@admin.service
BREAKS HERE
-## System info
BREAKS HERE
-    # (see templates/secret/env_vars.yml.j2) given a group (the application,
-    # _e.g._ richie or edxapp) and the set of environment variables defined in
-    # the above mentionned credentials file.
BREAKS HERE
-   - { role: openqa/fixes, tags: ['openqa_fixes'] }
BREAKS HERE
-    expected_change: true
-    expected_change: false
-    expected_change: true
BREAKS HERE
-#  example task to compact blobstore :
-#  - name: compact-blobstore
-#    task_alert_email: alerts@example.org # optional
-#      blobstoreName: 'default' # all task attributes are stored as strings by nexus internally
BREAKS HERE
-      deployer_input: "~/tempest-deployer-input.conf"
BREAKS HERE
-        ANSIBLE_ACTION_PLUGINS: "${HOME}/ansible_venv/repositories/ansible-config_template/action"
-        ANSIBLE_CONNECTION_PLUGINS: "${HOME}/ansible_venv/repositories/openstack-ansible-plugins/connection"
-        ANSIBLE_ROLES_PATH: "${HOME}/ansible_venv/repositories/roles"
-        ANSIBLE_ACTION_PLUGINS: "${HOME}/ansible_venv/repositories/ansible-config_template/action"
-        ANSIBLE_CONNECTION_PLUGINS: "${HOME}/ansible_venv/repositories/openstack-ansible-plugins/connection"
-        ANSIBLE_ROLES_PATH: "${HOME}/ansible_venv/repositories/roles"
-        ANSIBLE_ACTION_PLUGINS: "${HOME}/ansible_venv/repositories/ansible-config_template/action"
-        ANSIBLE_CONNECTION_PLUGINS: "${HOME}/ansible_venv/repositories/openstack-ansible-plugins/connection"
-        ANSIBLE_ROLES_PATH: "${HOME}/ansible_venv/repositories/roles"
-        ANSIBLE_ACTION_PLUGINS: "${HOME}/ansible_venv/repositories/ansible-config_template/action"
-        ANSIBLE_CONNECTION_PLUGINS: "${HOME}/ansible_venv/repositories/openstack-ansible-plugins/connection"
-        ANSIBLE_ROLES_PATH: "${HOME}/ansible_venv/repositories/roles"
BREAKS HERE
-  command: "true"
-  changed_when: no
-  when: rhel_07_031000
-      - notimplemented
BREAKS HERE
-        httpd_bin: "/usr/sbin/apache2ctl"
BREAKS HERE
-## Apache SSL Settings
-
BREAKS HERE
-      - name: try to find unused nodes and add them into a new unused group
-        # Set empty list for Liberty version
-        with_items: "{{ ironic_node_list.stdout|default('')|from_csv }}"
-            host_id: "{{ item.Name or item.UUID }}"
-            - host_id not in openstack_servers|map(attribute='name')|list
-            - host_id in groups.all
-            name: "{{ host_id }}"
-        omit_hosts: "{{ hostvars[groups['undercloud'][0]].nodes_to_delete|default([])|difference(hostvars[groups['undercloud'][0]].nodes_added|default([])) }}"
BREAKS HERE
-  when: ansible_distribution_major_version|int != 8
BREAKS HERE
-  gather_facts: true
BREAKS HERE
-- name: Set venv path
-    - name: Set venv fact
-- name: Keystone DB Migrations
-    - name: Perform a Keystone DB sync
-- name: Glance DB Migrations
-    - name: Perform a Glance DB sync
-- name: Cinder DB Migrations
-    - name: Perform a cinder DB sync
-- name: Nova DB Migrations
-    - name: Run nova null uuid checks
-    - name: Perform a Nova DB sync
-    - name: Perform Nova online data migrations
-- name: Stop Neutron Server
-    - name: Stop Neutron server
-- name: Neutron DB Migrations
-    - name: Perform a Neutron DB Upgrade
-- name: Heat DB Migrations
-    - name: Perform a heat DB sync
-- name: Horizon DB drop
-    - name: Drop horizon DB - It will be recreated later
-- name: Ironic DB Migrations
-    - name: Update database schema
BREAKS HERE
-    - name: Check for Node Selector (env=app) on project openshift-template-service-broker
-      shell: oc describe project openshift-template-service-broker|grep node-selector|grep app
-      register: node_selector_present
-      ignore_errors: true
-      when: osrelease | version_compare('3.7', '>=')
-
-    - name: Remove Node Selector (env=app) from project openshift-template-service-broker if it is set
-      - node_selector_present.rc == 0
BREAKS HERE
-    register: output
-  - debug: var=output.stdout_lines
BREAKS HERE
-        - fstrim-root  # temporary, https://www.redhat.com/archives/dm-devel/2019-May/msg00082.html
BREAKS HERE
-- name: Group hosts by product version juno, icehouse, havana, 4.0, 5.0
-    - group_by: key={{ product.version }}
-      when: product is defined
BREAKS HERE
-- import_playbook: "/srv/web/infra/ansible/playbooks/include/virt-create.yml myhosts=ci_cc_rdu01.fedoraproject.org"
BREAKS HERE
-odoo_config_limit_memory_hard: 805306368
-odoo_config_limit_memory_soft: 671088640
BREAKS HERE
-    zuul_git_src_dir: "/home/zuul/src/git.openstack.org"
BREAKS HERE
-      test_oc_cmd: >-
-        oc
-        {% if openshift_connection_certificate_authority is defined %}
-        --certificate-authority={{ openshift_connection_certificate_authority | quote }}
-        {% endif %}
-        {% if openshift_connection_insecure_skip_tls_verify is defined %}
-        --insecure-skip-tls-verify={{ openshift_connection_insecure_skip_tls_verify | quote }}
-        {% endif %}
-        {% if openshift_connection_server is defined %}
-        --server={{ openshift_connection_server | quote }}
-        {% endif %}
-        {% if openshift_connection_token is defined %}
-        --token={{ openshift_connection_token | quote }}
-        {% endif %}
-      openshift_connection:
-        server: "{{ openshift_connection_server }}"
-        insecure_skip_tls_verify: "{{ openshift_connection_insecure_skip_tls_verify }}"
-        token: "{{ openshift_connection_token }}"
-  roles:
-  - role: openshift-provision
-
-  roles:
-  - role: openshift-provision
BREAKS HERE
-      command: sh docker-bench-security.sh -l /tmp/output.log
-        dest: ./{{ inventory_hostname }}-docker-report-{{ ansible_date_time.date }}.log
-        msg: "Report can be found at ./{{ inventory_hostname }}-docker-report-{{ ansible_date_time.date }}.log"
BREAKS HERE
-          src: '{{item}}.j2'
-          - grafana/grafana.ini
BREAKS HERE
-      - '/rest/scopes/d0324c4b-56c9-485b-b7dd-8f6218a4b5bb'
-      - '/rest/scopes/9422ce05-dc39-4358-b723-c8c825eae412'
-    - name: Add Scopes to the Ethernet Network
-
-    - name: Add one scope to the Ethernet Network
-      oneview_ethernet_network:
-        config: "{{ config }}"
-        state: present
-        data:
-          name: "{{ network_name }}"
-          scopeUris: []
-      delegate_to: localhost
-
-    - name: Update the Ethernet Network with two scopes
-      oneview_ethernet_network:
-        config: "{{ config }}"
-        state: present
-        data:
-          name: "{{ network_name }}"
-          scopeUris: "{{ network_scope_uris }}"
-      delegate_to: localhost
BREAKS HERE
-- hosts: database
-    - database
-  tags:
-    - database
-- hosts: message-broker
-    - message-broker
-  tags: message-broker
BREAKS HERE
-    - name: change ceph_stable to False
-        regexp: "ceph_stable:.*"
-        replace: "ceph_stable: False"
-        dest: "{{ change_dir }}/group_vars/all"
-      when: change_dir is defined
-
-    - name: set ceph_dev to True
-      lineinfile:
-        line: "ceph_dev: True"
BREAKS HERE
-    inspec_test_directory: "/tmp/molecule/inspec"
BREAKS HERE
-# requires --extra-vars="target='modernpaste01.phx2.fedoraprjoect.org' paste='WMk4~kSfeW1OUNsLjF8hlMnRi-rYnlYzizqToCmG3BY='"
BREAKS HERE
-  - name: fail if the hose is not already defined/existent
BREAKS HERE
-    state: present
-    state: present
-    state: present
BREAKS HERE
-    - ansible_pkg_mgr in ['dnf', 'yum']
-    - ansible_selinux.status is defined
BREAKS HERE
-        {% if item.1.volume_group is defined %}
-        if [ "$(pvdisplay | grep -B1 {{ item.1.volume_group }} | awk '/PV/ {print $3}')" ];then
-          for device in `pvdisplay | grep -B1 {{ item.1.volume_group }} | awk '/PV/ {print $3}'`
-        echo "{{ item.1 }} volume_group not defined"
-      with_items: cinder_backends|dictsort
-      when: >
-        cinder_backends is defined and
-        physical_host != container_name
BREAKS HERE
-  hosts: ios
-    - debug:
-        var: cli
-        verbosity: 2
-      tags: facts
-
-    - name: Gather facts
-      ios_facts:
-      tags: facts
-
BREAKS HERE
-    - role: "os_magnum"
BREAKS HERE
-    sslonly: true
BREAKS HERE
-    - switch_type_name: "Cisco Nexus 6xxx"  # set the name of an existent switch type to run this example
-        name: "{{ switch_type_name }}"
BREAKS HERE
-  when: "'{{ ansible_os_family }}' == 'Debian'"
-- include_tasks: yum.yml
-  when: "'{{ ansible_os_family }}' == 'RedHat'"
BREAKS HERE
-      postgresql_privs: database=$item role=${item}app type=database
-                        priv="SELECT,INSERT,UPDATE,DELETE"
BREAKS HERE
-      apt: 'name="{{ item }}" state=present'
-      with_items:
-        - build-essential
-        - apt-transport-https
-        - curl
BREAKS HERE
-            instance_id: "{{ resource_prefix }}constructed"
-        #- name: set connection information for all tasks
-        #  set_fact:
-        #    aws_connection_info: &aws_connection_info
-        #      aws_access_key: '{{ aws_access_key }}'
-        #      aws_secret_key: '{{ aws_secret_key }}'
-        #      security_token: '{{ security_token }}'
-        #      region: '{{ aws_region }}'
-        #  no_log: yes
-
-        # TODO: Uncomment once rds_instance has been added
-        #- name: create minimal mariadb instance in default VPC and default subnet group
-        #  rds_instance:
-        #    state: present
-        #    engine: mariadb
-        #    instance_class: db.t2.micro
-        #    storage: 20
-        #    instance_id: 'rds-mariadb-{{ resource_prefix }}'
-        #    master_username: 'ansible-test-user'
-        #    master_password: 'password-{{ resource_prefix }}'
-        #    <<: *aws_connection_info
-        #  register: setup_instance
-
-        - name: Use AWS CLI to create an RDS DB instance
-          command: "aws rds create-db-instance --db-instance-identifier '{{ instance_id }}' --engine 'mariadb' --db-instance-class 'db.t2.micro' --allocated-storage 20 --master-user-password '{{ resource_prefix }}' --master-username 'ansibletestuser' --tags Key='workload_type',Value='other'"
-          environment:
-            AWS_ACCESS_KEY_ID: "{{ aws_access_key }}"
-            AWS_SECRET_ACCESS_KEY: "{{ aws_secret_key }}"
-            AWS_SESSION_TOKEN: "{{ security_token }}"
-            AWS_DEFAULT_REGION: "{{ aws_region }}"
-              - groups.rds_parameter_group_default_mariadb10_0
-        - name: Use AWS CLI to delete the DB instance
-          command: "aws rds delete-db-instance --db-instance-identifier '{{ instance_id }}' --skip-final-snapshot"
-          ignore_errors: True
-          environment:
-            AWS_ACCESS_KEY_ID: "{{ aws_access_key }}"
-            AWS_SECRET_ACCESS_KEY: "{{ aws_secret_key }}"
-            AWS_SESSION_TOKEN: "{{ security_token }}"
-            AWS_DEFAULT_REGION: "{{ aws_region }}"
-
-        # TODO: Uncomment once rds_instance has been added
-        #- name: remove mariadb instance
-        #  rds_instance:
-        #    state: absent
-        #    engine: mariadb
-        #    skip_final_snapshot: yes
-        #    instance_id: ansible-rds-mariadb-example
-        #    <<: *aws_connection_info
-        #  ignore_errors: yes
-        #  when: setup_instance is defined
BREAKS HERE
-        when: product.repo_type in ['poodle']
BREAKS HERE
-        node_id: "58c42d2c-d9bb-4d88-9d9c-3d032ad455e2"
-            vc_id: "67dbce0d-973e-4b7d-813d-7ae5a91754c2"
-            - network-44
-            - network-44
-            - network-44
-            management_network_id: "network-44"
-            compute_id: "domain-c49"
-            storage_id: "datastore-43"
-        state: "absent"
BREAKS HERE
-    #- bugyou  # TODO -- we still have to write this role.
BREAKS HERE
-- name: Creates network
-- name: Tempest installation and configuration
BREAKS HERE
-          subdomain_base: "{{ ansible_hostname}}.{{subdomain_base}}"
BREAKS HERE
-    when: disable_repos == "yes"
BREAKS HERE
-  hosts: bodhi-backend01.stg.fedoraproject.org
BREAKS HERE
-  gather_facts: no
BREAKS HERE
-#
-# Enable these if you use ceph rbd for at least one component (glance, cinder, nova)
-# ceph cluster-id, overrule with correct uuid!
-#ceph_fsid: d4ab416b-490c-4ab9-87e8-2364b524e0f2
-#ceph_conf:
-#  global:
-#    fsid: '{{ ceph_fsid }}'
-#    mon_initial_members: 'mon1.example.local,mon2.example.local,mon3.example.local'
-#    mon_host: '10.16.5.40,10.16.5.41,10.16.5.42'
-#    auth_cluster_required: cephx
-#    auth_service_required: cephx
-#    auth_client_required: cephx
BREAKS HERE
-    - neutron-lbaas-agent
BREAKS HERE
-      when:
-        - hostvars[item]['server_vm'] | default(false) | bool
-      with_items: "{{ groups['pxe_servers'] }}"
BREAKS HERE
-  - command: pt_chown
-    path: /usr/libexec
-    stig_id: V-72181
-    arch_specific: no
-    distro: redhat
BREAKS HERE
-        source:
-          type: image
-          mode: pull
-          server: https://images.linuxcontainers.org
-          protocol: lxd
-          alias: ubuntu/xenial/amd64
-        profiles: ["default"]
BREAKS HERE
-      no_subscribe: "{{do_not_subscribe}}"
-    when: not hostvars['bastion']['no_subscribe']
-    when: not hostvars['bastion']['no_subscribe']
BREAKS HERE
-  - role: httpd/reverseproxy
-    website: admin.fedoraproject.org
-    destname: mailman
-    localpath: /mailman
-    remotepath: /mailman
-    proxyurl: http://collab03.fedoraproject.org
BREAKS HERE
-  - include_vars: "{{ item }}"
-    with_first_found:
-      - "{{ ansible_distribution }}_{{ ansible_distribution_major_version }}.yml"
-      - "{{ ansible_os_family }}_{{ ansible_distribution_major_version }}.yml"
-      - "{{ ansible_distribution }}.yml"
-      - "{{ ansible_os_family }}.yml"
BREAKS HERE
-      shell: "oc patch storageclass glusterfs-block -p '{\"metadata\": {\"annotations\": \
BREAKS HERE
-        dest: /root/.ssh/opentstack.pem
BREAKS HERE
-security_sysctl_tcp_syncookies: 1                 # V-38539
BREAKS HERE
-        - fstrim-root
BREAKS HERE
-            overcloud_deploy: "no"
BREAKS HERE
-  hosts: iscsi-gws
BREAKS HERE
-  become: yes
BREAKS HERE
-- name: Prepare MQ/DB services
-    - name: Configure MySQL user (nova)
-      include: common-tasks/mysql-db-user.yml
-      vars:
-        user_name: "{{ nova_galera_user }}"
-        password: "{{ nova_container_mysql_password }}"
-        login_host: "{{ nova_galera_address }}"
-        db_name: "{{ nova_galera_database }}"
-      run_once: yes
-
-    - name: Configure MySQL user (nova-api)
-      include: common-tasks/mysql-db-user.yml
-      vars:
-        user_name: "{{ nova_api_galera_user }}"
-        password: "{{ nova_api_container_mysql_password }}"
-        login_host: "{{ nova_api_galera_address }}"
-        db_name: "{{ nova_api_galera_database }}"
-        db_append_privs: "yes"
-      run_once: yes
-
-    - name: Configure MySQL user (nova-api cell0)
-      include: common-tasks/mysql-db-user.yml
-      vars:
-        user_name: "{{ nova_api_galera_user }}"
-        password: "{{ nova_api_container_mysql_password }}"
-        login_host: "{{ nova_api_galera_address }}"
-        db_name: "{{ nova_cell0_database }}"
-        db_append_privs: "yes"
-      run_once: yes
-
BREAKS HERE
-        - name: aodh.yml.aio
-        - name: ceilometer.yml.aio
-        - name: gnocchi.yml.aio
BREAKS HERE
-swift_oslomsg_notify_transport: rabbit
-swift_oslomsg_notify_servers: 127.0.0.1
-swift_oslomsg_notify_port: 5672
-swift_oslomsg_notify_use_ssl: False
BREAKS HERE
-          - when: install_student_user | bool
-              - name: Create .kube for {{ student_user }} user
-                  path: /home/{{ student_user }}/.kube
-                  owner: "{{ student_user }}"
-              - name: Copy /home/{{ remote_user }}/{{ cluster_name }}/auth/kubeconfig to ~{{ student_user }}
-                  dest: /home/{{ student_user }}/.kube/config
-                  owner: "{{ student_user }}"
BREAKS HERE
-   - { role: taskotron/imagefactory-client, tags: ['imagefactoryclient'], when: deployment_type in ['dev'] }
BREAKS HERE
-- name: Set up repos on subnodes for upgrading
-  hosts: overcloud
-  vars:
-    ib_repo_host: undercloud
-  roles:
-    - role: repo-setup
-      when: mixed_upgrade|default(false)|bool
-    - role: install-built-repo
-      when: hostvars['undercloud']['compressed_gating_repo'] is defined and mixed_upgrade|default(false)|bool
-- name:  Upgrade Tripleo
-  hosts: undercloud
-  tags:
-    - overcloud-upgrade
-  gather_facts: no
-  roles:
-    - role: overcloud-upgrade
-      when: containerized_overcloud_upgrade|bool
BREAKS HERE
-          shell echo '[local]' >> /tmp/inventory && echo '192.168.122.1' >> /tmp/inventory echo && '[bast]' >> /tmp/inventory && echo 'bastion' >> /tmp/inventory && echo '[nodes]' >> /tmp/inventory
BREAKS HERE
-      check_galaxy_data: true
-      check_tool_data: true
BREAKS HERE
-        containerized: "{{ 'docker exec ceph-mon-' + hostvars[groups[mon_host]]['ansible_hostname'] if containerized_deployment else None }}"
BREAKS HERE
-
-- name: setup fedmsg for MBS
-  import_tasks: "fedmsg.yml"
-  when: env == "staging"
BREAKS HERE
-# kubectl 客户端配置
-- hosts: 
-  - kube-master
-  - kube-node
-  - deploy
-  roles:
-  - kubectl
-
-# master 节点部署
-  - kube-node
BREAKS HERE
-      volumes: "/data"
-      volumes_from: "{{ groups.decepticons[0] }}"
BREAKS HERE
-- name: test become_root
BREAKS HERE
-        - /opt/appdata/pgblitz/keys/unprocessed
-        - /opt/appdata/pgblitz/keys/processed
-        - /opt/appdata/pgblitz/keys/badjson
BREAKS HERE
-    - name: run test script
-      win_shell: files/test_script.ps1
-      register: psoutput
BREAKS HERE
-    nova_compute_driver: nova_powervm.virt.powervm.driver.PowerVMDriver
BREAKS HERE
-          dest: "/etc/sysconfig/network/script/{{ item[0] }}-veth-{{ item[1].name | default('br-mgmt') }}-2-{{ item[1].veth_peer | default('eth1') }}"
BREAKS HERE
-        gather_subset: "network,hardware,virtual"
BREAKS HERE
-- hosts: all
BREAKS HERE
-    - {
-      role: ansible-ansible-openshift-ansible,
-        cluster_inventory_filename: "cluster-inventory-stg",
-        openshift_htpasswd_file: "/etc/origin/htpasswd",
-        openshift_master_public_api_url: "https://{{ osbs_url }}:8443",
-        openshift_release: "v3.9.0",
-        openshift_ansible_path: "/root/openshift-ansible",
-        openshift_ansible_pre_playbook: "playbooks/prerequisites.yml",
-        openshift_ansible_playbook: "playbooks/deploy_cluster.yml",
-        openshift_ansible_version: "openshift-ansible-3.9.30-1",
-        openshift_ansible_ssh_user: root,
-        openshift_ansible_install_examples: false,
-        openshift_ansible_containerized_deploy: false,
-        openshift_cluster_masters_group: "osbs-masters-stg",
-        openshift_cluster_nodes_group: "osbs-nodes-stg",
-        openshift_cluster_infra_group: "osbs-masters-stg",
-        openshift_auth_profile: "osbs",
-        openshift_cluster_url: "https://{{osbs_url}}",
-        openshift_master_ha: false,
-        openshift_debug_level: 2,
-        openshift_shared_infra: true,
-        openshift_deployment_type: "origin",
-        openshift_ansible_python_interpreter: "/usr/bin/python3",
-        openshift_ansible_use_crio: false,
-        openshift_ansible_crio_only: false,
-      when: env == 'staging',
-    }
-    - {
-      role: ansible-ansible-openshift-ansible,
-        cluster_inventory_filename: "cluster-inventory",
-        openshift_htpasswd_file: "/etc/origin/htpasswd",
-        openshift_master_public_api_url: "https://{{ osbs_url }}:8443",
-        openshift_release: "v3.9.0",
-        openshift_ansible_path: "/root/openshift-ansible",
-        openshift_ansible_pre_playbook: "playbooks/prerequisites.yml",
-        openshift_ansible_playbook: "playbooks/deploy_cluster.yml",
-        openshift_ansible_version: "openshift-ansible-3.9.30-1",
-        openshift_ansible_ssh_user: root,
-        openshift_ansible_install_examples: false,
-        openshift_ansible_containerized_deploy: false,
-        openshift_cluster_masters_group: "osbs-masters",
-        openshift_cluster_nodes_group: "osbs-nodes",
-        openshift_cluster_infra_group: "osbs-masters",
-        openshift_auth_profile: "osbs",
-        openshift_cluster_url: "{{osbs_url}}",
-        openshift_master_ha: false,
-        openshift_debug_level: 2,
-        openshift_shared_infra: true,
-        openshift_deployment_type: "origin",
-        openshift_ansible_python_interpreter: "/usr/bin/python3",
-        openshift_ansible_use_crio: false,
-        openshift_ansible_crio_only: false,
-      when: env == 'production',
-      tags: ['openshift-cluster','ansible-ansible-openshift-ansible']
-    }
-
BREAKS HERE
-    cache_index_item: "{{ lxc_cache_map.distro }};{{ lxc_cache_map.release }};{{ lxc_cache_map.arch }};{{ lxc_cache_map.variant }}"
BREAKS HERE
-    name=lobsters-cron
BREAKS HERE
-              msg: "Disabling linchpin tests until container is fixed. Refer Travis ci for unit tests and flake8 tests"
-              args:
-                chdir: ../../
BREAKS HERE
-    - include: create-grant-db.yml
-      db_name: "{{ neutron_galera_database }}"
-      db_password: "{{ neutron_container_mysql_password }}"
-
BREAKS HERE
-   - import_tasks: tasks/gather-facts.yml
-   - import_tasks: tasks/config-banner.yml
-   - import_tasks: tasks/config-loopback.yml
-   - import_tasks: tasks/config-common.yml
-   - import_tasks: tasks/config-routing.yml
-   - import_tasks: tasks/config-lan.yml
-   - import_tasks: tasks/config-wan.yml
-   - import_tasks: tasks/config-crypto.yml
-   - import_tasks: tasks/config-tunnel.yml
-   - import_tasks: tasks/config-eigrp.yml
-   - import_tasks: tasks/backup.yml
BREAKS HERE
-    - name: Ensure rabbitmq vhost
-      rabbitmq_vhost:
-        name: "{{ neutron_rabbitmq_vhost }}"
-        state: "present"
-      delegate_to: "{{ groups['rabbitmq_all'][0] }}"
-      when: inventory_hostname == groups['neutron_all'][0]
-      tags:
-        - neutron-rabbitmq
-        - neutron-rabbitmq-vhost
-    - name: Ensure rabbitmq user
-      rabbitmq_user:
-        user: "{{ neutron_rabbitmq_userid }}"
-        password: "{{ neutron_rabbitmq_password }}"
-        vhost: "{{ neutron_rabbitmq_vhost }}"
-        configure_priv: ".*"
-        read_priv: ".*"
-        write_priv: ".*"
-        state: "present"
-      delegate_to: "{{ groups['rabbitmq_all'][0] }}"
-      when: inventory_hostname == groups['neutron_all'][0]
-      tags:
-        - neutron-rabbitmq
-        - neutron-rabbitmq-user
-    - name: Create DB for service
-      mysql_db:
-        login_user: "root"
-        login_password: "secrete"
-        login_host: "localhost"
-        name: "{{ neutron_galera_database }}"
-        state: "present"
-      delegate_to: "{{ groups['galera_all'][0] }}"
-      when: inventory_hostname == groups['neutron_all'][0]
-      tags:
-        - mysql-db-setup
-    - name: Grant access to the DB for the service
-      mysql_user:
-        login_user: "root"
-        login_password: "secrete"
-        login_host: "localhost"
-        name: "{{ neutron_galera_database }}"
-        password: "{{ neutron_container_mysql_password }}"
-        host: "{{ item }}"
-        state: "present"
-        priv: "{{ neutron_galera_database }}.*:ALL"
-      with_items:
-        - "localhost"
-        - "%"
-      delegate_to: "{{ groups['galera_all'][0] }}"
-      when: inventory_hostname == groups['neutron_all'][0]
-      tags:
-        - mysql-db-setup
BREAKS HERE
-      command: "{{ tempest_venv_bin }}/pip show {{ item.name }}"
-      with_items: "{{ tempest_plugins }}"
-    - name: Run tempest
-        . {{ tempest_venv_bin }}/activate
-        {{ tempest_venv_bin | dirname }}/run_tempest.sh --no-virtual-env ${RUN_TEMPEST_OPTS} tempest.api.identity.v3
-      environment:
-        RUN_TEMPEST_OPTS: "--serial"
BREAKS HERE
-    target: http://fedoraplanet.org
BREAKS HERE
-      when: "forever_list.stdout.find('{{ node_apps_location }}/app/app.js') == -1"
BREAKS HERE
-#  content in the openstack-ansible-security role. Review the comments below
-#  as well as the main openstack-ansible-security documentation:
-#    http://docs.openstack.org/developer/openstack-ansible-security/
-# See the openstack-ansible-security documentation for more details.
BREAKS HERE
-    health_osd_check_retries: 10
-    health_osd_check_delay: 10
BREAKS HERE
-  hosts: controller
BREAKS HERE
-tags: explorer
BREAKS HERE
-  - name: Setup /etc/hosts
-    copy:
-      src: /etc/hosts
-      dest: /etc/hosts
-
-  - name: Assign hostname
-    hostname:
-      name: "{{ inventory_hostname }}"
-
-  - name: Copy setup script
-    copy:
-      src: setup_{{ ansible_os_family }}.sh
-      dest: /tmp/setup.sh
-      mode: 0755
-
-
-  - name: Run node setup
-    shell: /tmp/setup.sh
-
-  - name: Changing permissions of Docker socket to 666
-    file:
-      path: /run/docker.sock
-      mode: 0666
BREAKS HERE
-# keys, etc).
-working_dir: "/home/{{ ansible_user }}"
BREAKS HERE
-  yaml: {key: defaults.host_key_checking}
-  default: [host_list, script, yaml, ini]
-  - {key: inventory_enabled, section: defaults}
-  yaml: {key: inventory.enabled_plugins}
-  yaml: {key: inventory.ignore_extensions}
-  yaml: {key: inventory.ignore_patterns}
BREAKS HERE
-- name: install the undercloud packages and get the guest image
-  hosts: undercloud
-  tasks:
-    - name: get the guest-image
-      get_url: >
-        url="{{ distro.images[distro.name][distro.version].remote_file_server }}{{ distro.images[distro.name][distro.version].guest_image_name }}"
-        dest=/home/stack/{{ distro.images[distro.name][distro.version].guest_image_name }}
-        timeout=360
-
-    - name: install python-rdomanager-oscplugin
-      yum: name=python-rdomanager-oscplugin state=present
-      sudo: yes
-
BREAKS HERE
-    - name: Determine Python interpreter
BREAKS HERE
-      retries: 4
BREAKS HERE
-- include: common-playbooks/unbound-clients.yml
-  vars:
-    unbound_group: "{{ openstack_host_group|default('hosts') }}"
-  when:
-    - resolvconf_enabled | bool
-
BREAKS HERE
-    - beat-install
BREAKS HERE
-async: Run a task asyncronouslly if the C(action) supports this.
BREAKS HERE
-- name: Group hosts by provisioner
-  hosts: local
-  sudo: no
-    - group_by: key={{ provisioner.type }}
-    - group_by: key={{ provisioner.network.type }}
-    - group_by: key={{ provisioner.skip }}
-  tags:
-    - provision
-
-- name: Get nodes - OpenStack
-  hosts: local:&openstack:!skip_provision
-  sudo: no
-  gather_facts: False
-  roles:
-    - { role: get_nodes/openstack }
-  tags:
-    - provision
-
-- name: Get nodes - Rackspace
-  hosts: local:&rackspace:!skip_provision
-  sudo: no
-  gather_facts: False
-  roles:
-    - { role: get_nodes/rax }
-    - { role: get_nodes/add_hosts,
-              nodes_created: "{{ new_nodes.results }}",
-              floating_ips: [],
-              network_expected: rax }
-  tags:
-    - provision
-
-- name: Pre provision tasks prepare libvirt host
-  hosts: libvirt_host:&libvirt:!skip_provision
-  sudo: yes
-  roles:
-    - { role: libvirt/prepare_host }
-
-
-# Not even sure the firewall prepping is needed anymore on libvirt host
-# FIXME: something is wrong in this condition, iptables is started in rhel7
-#- name: Pre provision tasks Libvirt IPtables firewall
-#  hosts: libvirt_host:&libvirt:~(RedHat|CentOS)-6:!~(Fedora-20|(RedHat|CentOS)-7):!skip_provision
-#  sudo: yes
-#  roles:
-#    - { role: libvirt/firewall/iptables }
-# FIXME: yesterday this task worked perfectly, today is a mess of errors. Hurray.
-#- name: Pre provision tasks Libvirt Firewalld firewall
-#  hosts: libvirt_host:&libvirt:!~(RedHat|CentOS)-6:!skip_provision
-#  sudo: yes
-#  roles:
-#    - { role: libvirt/firewall/firewalld }
-
-- name: Get networks - Libvirt
-  hosts: libvirt_host:&libvirt:!skip_provision
-  sudo: no
-  gather_facts: False
-  roles:
-    - { role: get_networks/libvirt }
-  tags:
-    - provision
-
-- name: Get nodes - Libvirt
-  hosts: libvirt_host:&libvirt:!skip_provision
-  sudo: no
-  gather_facts: False
-  roles:
-    - { role: get_nodes/libvirt }
-  tags:
-    - provision
-
-- name: Assign floating IP and add hosts - Nova
-  hosts: local:&openstack:&nova:!skip_provision
-  sudo: no
-  gather_facts: False
-  roles:
-    - { role: get_nodes/fip-nova }
-    - { role: get_nodes/add_hosts,
-              nodes_created: "{{ new_nodes.results }}",
-              floating_ips: "{{ new_floating_ips.results }}",
-              network_expected: nova }
-  tags:
-    - provision
-
-- name: Assign floating IP and add hosts - Neutron
-  hosts: local:&openstack:&neutron:!skip_provision
-  sudo: no
-  gather_facts: False
-  roles:
-    - { role: get_nodes/fip-neutron }
-    - { role: get_nodes/add_hosts,
-              nodes_created: "{{ new_nodes.results }}",
-              floating_ips: "{{ new_floating_ips.results }}",
-              network_expected: neutron }
-  tags:
-    - provision
-
-- name: Post provision tasks
-  hosts: local:!manual:!skip_provision:!libvirt
-  sudo: no
-  gather_facts: False
-  roles:
-    - { role: get_nodes/post }
-  tags:
-    - provision
-
-- name: Set facts for hosts
-  hosts: openstack_nodes:!manually-provisioned:!skip_provision
-  gather_facts: False
-  roles:
-    - { role: set_facts }
-    - { role: wait_for_hosts }
-  tags:
-    - provision
BREAKS HERE
-    name: gridengine-master
BREAKS HERE
-      shell: utilities/service_discovery.sh {{ users }} {{ slaves }}
BREAKS HERE
-      - name: check for subscription
-        command: subscription-manager identity
-        ignore_errors: yes
-        register: cdn_status
-        changed_when: false
-        when: not install.cdn|default(False)
-
-        tags:
-            - upgrade
-            - upgrade_repos
-        when:
-           - not install.cdn|default(False)
-           - cdn_status.rc != 0
-
-      - role: cdn_registery
-        cdn_args_file: "{{ install.cdn|default('') }}"
-        become: True
-        when:
-            - "install.cdn|default(False) or cdn_status.rc == 0"
-            - "install.version|openstack_distribution == 'OSP'"
-
BREAKS HERE
-- name: stop ceph cluster
-  hosts:
-    - mons
-    - osds
-    - mdss
-    - rgws
-    - nfss
-  become: yes
-    osd_group_name: osds
-    mon_group_name: mons
-    rgw_group_name: rgws
-    mds_group_name: mdss
-    nfs_group_name: nfss
-# When set to true both groups of packages are purged.
-# This can cause problem with qemu-kvm
-    purge_all_packages: true
-# When set to true and raw _multi_journal is used then block devices are also zapped
-    zap_block_devs: true
-    ceph_packages:
-      - ceph
-      - ceph-common
-      - ceph-fs-common
-      - ceph-fuse
-      - ceph-mds
-      - ceph-release
-      - ceph-radosgw
-    ceph_remaining_packages:
-      - libcephfs1
-      - librados2
-      - libradosstriper1
-      - librbd1
-      - python-cephfs
-      - python-rados
-      - python-rbd
-    cluster: ceph # name of the cluster
-    monitor_name: "{{ ansible_hostname }}"
-    mds_name: "{{ ansible_hostname }}"
-    osd_auto_discovery: false
-  handlers:
-  - name: restart machine
-    shell: sleep 2 && shutdown -r now "Ansible updates triggered"
-    async: 1
-    poll: 0
-    ignore_errors: true
-  - name: wait for server to boot
-    become: false
-    local_action: wait_for port=22 host={{ inventory_hostname }} state=started delay=10 timeout=500
-  - name: remove data
-    file:
-     path: /var/lib/ceph
-     state: absent
-  - name: check for a device list
-    fail:
-      msg: "OSD automatic discovery was detected, purge cluster does not support this scenario. If you want to purge the cluster, manually provide the list of devices in group_vars/osds using the devices variable."
-    when:
-      osd_group_name in group_names and
-      devices is not defined and
-      osd_auto_discovery
-
-  - name: get osd numbers
-    shell: "if [ -d /var/lib/ceph/osd ] ; then ls /var/lib/ceph/osd | cut -d '-' -f 2 ; fi"
-    register: osd_ids
-    changed_when: false
-
-  - name: are we using systemd
-    shell: "if [ -d /usr/lib/systemd ] ; then find /usr/lib/systemd/system -name 'ceph*' | wc -l ; else echo 0 ; fi"
-    register: systemd_unit_files
-
-# after Hammer release
-  - name: stop ceph-osd with systemd
-      name: ceph-osd@{{item}}
-    with_items: "{{ osd_ids.stdout_lines }}"
-      systemd_unit_files.stdout != "0" and
-      osd_group_name in group_names
-  - name: stop ceph mons with systemd
-    service:
-      name: ceph-mon@{{ ansible_hostname }}
-      state: stopped
-      enabled: no
-      ansible_os_family == 'RedHat' and
-      systemd_unit_files.stdout != "0" and
-      mon_group_name in group_names
-  - name: stop ceph mdss with systemd
-      name: ceph-mds@{{ ansible_hostname }}
-      systemd_unit_files.stdout != "0" and
-      mds_group_name in group_names
-      systemd_unit_files.stdout != "0" and
-      rgw_group_name in group_names
-  - name: stop ceph nfss with systemd
-      name: nfs-ganesha
-      systemd_unit_files.stdout != "0" and
-      nfs_group_name in group_names
-      systemd_unit_files.stdout != "0" and
-      rbdmirror_group_name in group_names
-
-# before infernalis release, using sysvinit scripts
-# we use this test so we do not have to know which RPM contains the boot script
-# or where it is placed.
-  - name: stop ceph osds
-    shell: "service ceph status osd ; if [ $? == 0 ] ; then service ceph stop osd ; else echo ; fi"
-      ansible_os_family == 'RedHat' and
-      osd_group_name in group_names
-  - name: stop ceph mons
-    shell: "service ceph status mon ; if [ $? == 0 ] ; then service ceph stop mon ; else echo ; fi"
-    when:
-      ansible_os_family == 'RedHat' and
-      mon_group_name in group_names
-  - name: stop ceph mdss
-    shell: "service ceph status mds ; if [ $? == 0 ] ; then service ceph stop mds ; else echo ; fi"
-      mds_group_name in group_names
-  - name: stop ceph rgws
-    shell: "service ceph-radosgw status ; if [ $? == 0 ] ; then service ceph-radosgw stop ; else echo ; fi"
-      rgw_group_name in group_names
-      ansible_os_family == 'RedHat' and
-      nfs_group_name in group_names
-  - name: stop ceph osds on ubuntu
-    shell: |
-      for id in $(ls /var/lib/ceph/osd/ |grep -oh '[0-9]*'); do
-        initctl stop ceph-osd cluster={{ cluster }} id=$id
-      done
-      ansible_distribution == 'Ubuntu' and
-      osd_group_name in group_names
-    with_items: "{{ osd_ids.stdout_lines }}"
-  - name: stop ceph mons on ubuntu
-    command: initctl stop ceph-mon cluster={{ cluster }} id={{ monitor_name }}
-      ansible_distribution == 'Ubuntu' and
-      mon_group_name in group_names
-  - name: stop ceph mdss on ubuntu
-    command: initctl stop ceph-mds cluster={{ cluster }} id={{ mds_name }}
-    failed_when: false
-      ansible_distribution == 'Ubuntu' and
-      mds_group_name in group_names
-  - name: stop ceph rgws on ubuntu
-    command: initctl stop radosgw cluster={{ cluster }} id={{ ansible_hostname }}
-    failed_when: false
-      ansible_distribution == 'Ubuntu' and
-      rgw_group_name in group_names
-  - name: stop ceph nfss on ubuntu
-    command: initctl stop nfs-ganesha
-    failed_when: false
-      ansible_distribution == 'Ubuntu' and
-      nfs_group_name in group_names
-  - name: stop ceph rbd mirror on ubuntu
-    command: initctl stop ceph-rbd-mirorr cluster={{ cluster }} id=admin
-      ansible_distribution == 'Ubuntu' and
-      rbdmirror_group_name in group_names
-
-  - name: check for anything running ceph
-    shell: "ps awux | grep -- /usr/bin/[c]eph-"
-    register: check_for_running_ceph
-    failed_when: check_for_running_ceph.rc == 0
-    when:
-      osd_group_name in group_names
-    when:
-      osd_group_name in group_names
-    when:
-     osd_group_name in group_names
-    when:
-      osd_group_name in group_names
-
-  - name: remove monitor store and bootstrap keys
-    file:
-      path: /var/lib/ceph/
-      state: absent
-    when:
-      mon_group_name in group_names
-      osd_group_name in group_names and
-     osd_group_name in group_names and
-      osd_group_name in group_names and
BREAKS HERE
-  - name: Enable firewall master lb external zone services
-  - name: Setup firewall master lb external zone sources
BREAKS HERE
-      join_repospanner_node: repospanner01.ansible.fedoraproject.org
-      join_repospanner_node: repospanner01.ansible.fedoraproject.org
-      join_repospanner_node: centos01.rpms.fedoraproject.org
-      join_repospanner_node: centos01.rpms.fedoraproject.org
BREAKS HERE
-- hosts: "{{ groups['mons'][0] }}"
-  gather_facts: False
-  become: True
-
-  roles:
-    - role: ceph-defaults
-    - role: ceph-fetch-keys
-
-  post_tasks:
-    - name: set_fact docker_exec_cmd if containerized_deployment
-      set_fact:
-        docker_exec_cmd: "docker exec ceph-mon-{{ hostvars[groups[mon_group_name][0]]['ansible_hostname'] }}"
-      when: containerized_deployment
-
BREAKS HERE
-    - name: check_hostname
-      shell: hostname 
-      register: src_hostname
-    - name: copy_id_rsa_pub
-      copy: src={{ ansible_env.HOME }}/.ssh/id_rsa.pub dest={{ key_dir }}/id_rsa.pub.{{ ansible_env.SSH_CLIENT.split(' ')[0] }}
-      shell: cat {{ key_dir }}/id_rsa.pub.{{ ansible_env.SSH_CLIENT.split(' ')[0] }}
-      lineinfile: dest={{ ansible_env.HOME }}/.ssh/authorized_keys line="{{ key_string.stdout }}"
BREAKS HERE
-  hosts: localhost
BREAKS HERE
-
BREAKS HERE
-  command: >
-    virtualenv-tools --update-path=auto --reinitialize {{ neutron_bin | dirname }}
BREAKS HERE
-        name: "{{ item }}"
-      with_items:
-        - openssh-server
BREAKS HERE
-        when: create_user|changed
-        when: create_user|changed
-        - hostvars[item].create_user|changed
BREAKS HERE
-            - install.version|openstack_release == 13
BREAKS HERE
-      shell: "pip2 install ansible-tower-cli"
-      shell: "pip2 install ansible-lint"
-      get_url: 
-        url: https://github.com/mglantz/ansible-roadshow/blob/master/content/tower-backup.tar.gz
-        dest: /opt/tower/tower-backup.tar.gz
-      shell: "{{tower_installer_path.stdout}} -r /opt/tower/tower-backup.tar.gz"
BREAKS HERE
-      with_items:
BREAKS HERE
-- name: Elasticsearch Xpack tests
BREAKS HERE
-  local_action: fedmsg
-                cert_prefix="shell"
-                topic="playbook.start"
-                msg="just a test that we are starting"
-  local_action: fedmsg
-                cert_prefix="shell"
-                topic="playbook.complete"
-                msg="just a test that we have completed"
BREAKS HERE
-  - name: Start pacemaker service
-    service: name=pacemaker state=started
-
BREAKS HERE
-  authorized_key: user=root key="{{ item }}"
-  with_file:
-  - /srv/web/infra/ansible/files/twisted/ssh-pub-key
-  tags:
-  - config
-  - sshkeys
BREAKS HERE
-  hosts: all
-  - { role: docker_setup, when: not no_docker_storage_setup, device: '/dev/vdb'}
BREAKS HERE
-    server_aliases:
BREAKS HERE
-  when: rhel_07_032010
BREAKS HERE
-    - name: Ensure the fc-network 'fc-network-1' to be added to the scope resource assignments is present
-      oneview_fc_network:
-        config: "{{ config }}"
-        state: present
-        data:
-          name: 'fc-network-1'
-          fabricType: FabricAttach
-          autoLoginRedistribution: True
-          linkStabilityTime: 30
-      delegate_to: localhost
-
-    - set_fact: fc_network_1="{{ fc_network }}"
-
-    - name: Ensure the fc-network 'fc-network-2' to be added to the scope resource assignments is present
-      oneview_fc_network:
-        config: "{{ config }}"
-        state: present
-        data:
-          name: 'fc-network-2'
-          fabricType: FabricAttach
-          autoLoginRedistribution: True
-          linkStabilityTime: 30
-      delegate_to: localhost
-
-    - set_fact: fc_network_2="{{ fc_network }}"
-
-    - name: Ensure the fc-network 'fc-network-3' to be added to the scope resource assignments is present
-      oneview_fc_network:
-        config: "{{ config }}"
-        state: present
-        data:
-          name: 'fc-network-3'
-          fabricType: FabricAttach
-          autoLoginRedistribution: True
-          linkStabilityTime: 30
-      delegate_to: localhost
-
-    - set_fact: fc_network_3="{{ fc_network }}"
-
-              - '{{ fc_network_1.uri }}'
-              - '{{ fc_network_2.uri }}'
-              - '{{ fc_network_3.uri }}'
-              - '{{ fc_network_1.uri }}'
BREAKS HERE
-- name: Fail if incompatible configuration detected
-  fail:
-    msg: "keystone_apache_enabled must be True when keystone_mod_wsgi_enabled."
-  when:
-    - not keystone_apache_enabled | bool
-    - keystone_mod_wsgi_enabled | bool
-  tags:
-    - always
-
-- include: "keystone_{{ (keystone_apache_enabled | bool) | ternary('apache', 'nginx') }}.yml"
-  static: no
-  when: not keystone_mod_wsgi_enabled | bool
BREAKS HERE
-public_hostname: 'nexus.vm'
BREAKS HERE
-        - data_migrations | succeeded
BREAKS HERE
-    - name: Generate /etc/ansible/hosts file with lab hosts template =< 3.9 release: {{ osrelease }}
BREAKS HERE
-- lineinfile:
-  path: /etc/postfix/main.cf
-  state: absent
-  regexp: '^relayhost = bastion'
BREAKS HERE
-- name: Azure | Manage Router Public IP
-  azure_rm_publicipaddress:
-    resource_group: "{{ rg }}"
-    state: "{{ state }}"
-    name: "{{ router_lb_public_ip }}"
-    allocation_method: Static
-  register: router_lb_ip
-- set_fact:
-    router_lb_ip: "{{ router_lb_ip.state.ip_address }}"
-
BREAKS HERE
-                templates/undercloud.conf
BREAKS HERE
-      goss_file: goss.yml
-      goss_addtl_dirs: [goss]
-      goss_env_vars:
-        distro: "{{ ansible_distribution | lower }}"
-        should: "pass"
-        boolean: true
-        list:
-          - one
-          - two
-          - three
-        dict:
-          a: b
-          c: d
-- name: fail
-  hosts: all
-  become: true
-  any_errors_fatal: true
-  roles:
-    - role: default
-      degoss_clean: false
-      degoss_debug: true
-      goss_env_vars:
-        distro: "{{ ansible_distribution | lower }}"
-        should: "fail"
-        boolean: true
-        list:
-          - one
-          - two
-          - three
-        dict:
-          a: b
-          c: d
-      when: fail is defined
BREAKS HERE
-    provision_clusterresourcequota_def: "{{ lookup('template', 'resources/' ~ provision_clusterresourcequota) | from_yaml }}"
-    quota_pods: 6
-    quota_secrets: 2
-    openshift_clusters:
-    - cluster_resources:
-      got_clusterresourcequota: "{{ get_clusterresourcequota.stdout | from_json | combine(ignore_differences, recursive=True) }}"
-      cmp_clusterresourcequota: "{{ provision_clusterresourcequota_def | combine(ignore_differences, recursive=True) }}"
-    provision_clusterresourcequota_def: "{{ lookup('template', 'resources/' ~ provision_clusterresourcequota) | from_yaml }}"
-    quota_pods: 8
-    quota_secrets: 3
-    openshift_clusters:
-    - cluster_resources:
-    vars:
-      cmp_clusterresourcequota: "{{ provision_clusterresourcequota_def | combine(ignore_differences, recursive=True) }}"
-
-      got_clusterresourcequota: "{{ get_clusterresourcequota.stdout | from_json | combine(ignore_differences, recursive=True) }}"
-      cmp_clusterresourcequota: "{{ provision_clusterresourcequota_def | combine(ignore_differences, recursive=True) }}"
BREAKS HERE
-  gather_facts: no
-           opts=vers="{{ nfsversion }}"
BREAKS HERE
-  - { role: inventory, host_ip: hostvars['localhost']['ansible_default_ipv4']['address']}
BREAKS HERE
-  - name: Create en external network
-      name: external
-      provider_network_type: flat
-      provider_physical_network: datacenter
BREAKS HERE
-    - include: included.yml
BREAKS HERE
-      command: echo "{{ host_group_vars_example_group_one_molecule_yml }} {{ host_group_vars_example_group_two_molecule_yml }}"
BREAKS HERE
-    - demo
BREAKS HERE
-- name: "MEDIUM | RHEL-07-030370 | PATCH | All uses of the chown command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030370
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030370
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030380 | PATCH | All uses of the fchown command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030380
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030380
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030390 | PATCH | All uses of the lchown command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030390
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030390
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030400 | PATCH | All uses of the fchownat command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030400
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030400
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030410 | PATCH | All uses of the chmod command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030410
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030410
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030420 | PATCH | All uses of the fchmod command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030420
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030420
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030430 | PATCH | All uses of the fchmodat command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030430
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030430
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030440 | PATCH | All uses of the setxattr command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030440
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030440
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030450 | PATCH | All uses of the fsetxattr command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030450
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030450
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030460 | PATCH | All uses of the lsetxattr command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030460
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030460
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030470 | PATCH | All uses of the removexattr command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030470
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030470
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030480 | PATCH | All uses of the fremovexattr command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030480
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030480
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030490 | PATCH | All uses of the lremovexattr command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030490
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030490
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030500 | PATCH | All uses of the creat command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030500
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030500
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030510 | PATCH | All uses of the open command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030510
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030510
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030520 | PATCH | All uses of the openat command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030520
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030520
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030530 | PATCH | All uses of the open_by_handle_at command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030530
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030530
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030540 | PATCH | All uses of the truncate command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030540
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030540
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030550 | PATCH | All uses of the ftruncate command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030550
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030550
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030560 | PATCH | All uses of the semanage command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030560
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030560
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030570 | PATCH | All uses of the setsebool command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030570
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030570
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030580 | PATCH | All uses of the chcon command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030580
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030580
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030590 | PATCH | All uses of the restorecon command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030590
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030590
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030600 | PATCH | The operating system must generate audit records for all successful/unsuccessful account access count events."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030600
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030600
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030610 | PATCH | The operating system must generate audit records for all unsuccessful account access events."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030610
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030610
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030620 | PATCH | The operating system must generate audit records for all successful account access events."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030620
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030620
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030630 | PATCH | All uses of the passwd command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030630
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030630
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030640 | PATCH | All uses of the unix_chkpwd command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030640
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030640
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030650 | PATCH | All uses of the gpasswd command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030650
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030650
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030660 | PATCH | All uses of the chage command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030660
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030660
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030670 | PATCH | All uses of the userhelper command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030670
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030670
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030680 | PATCH | All uses of the su command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030680
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030680
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030690 | PATCH | All uses of the sudo command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030690
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030690
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030700 | PATCH | All uses of the sudo command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030700
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030700
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030710 | PATCH | All uses of the newgrp command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030710
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030710
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030720 | PATCH | All uses of the chsh command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030720
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030720
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030730 | PATCH | All uses of the sudoedit command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030730
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030730
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030740 | PATCH | All uses of the mount command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030740
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030740
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030750 | PATCH | All uses of the umount command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030750
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030750
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030760 | PATCH | All uses of the postdrop command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030760
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030760
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030770 | PATCH | All uses of the postqueue command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030770
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030770
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030780 | PATCH | All uses of the ssh-keysign command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030780
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030780
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030800 | PATCH | All uses of the crontab command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030800
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030800
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030810 | PATCH | All uses of the pam_timestamp_check command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030810
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030810
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030820 | PATCH | All uses of the init_module command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030820
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030820
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030830 | PATCH | All uses of the delete_module command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030830
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030830
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030840 | PATCH | All uses of the insmod command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030840
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030840
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030850 | PATCH | All uses of the rmmod command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030850
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030850
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030860 | PATCH | All uses of the modprobe command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030860
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030860
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030870 | PATCH | The operating system must generate audit records for all account creations, modifications, disabling, and termination events that affect /etc/passwd."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030870
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030870
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030871 | PATCH | The operating system must generate audit records for all account creations, modifications, disabling, and termination events that affect /etc/group."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030871
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030871
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030872 | PATCH | The operating system must generate audit records for all account creations, modifications, disabling, and termination events that affect /etc/gshadow."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030872
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030872
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030873 | PATCH | The operating system must generate audit records for all account creations, modifications, disabling, and termination events that affect /etc/shadow."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030873
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030873
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030874 | PATCH | The operating system must generate audit records for all account creations, modifications, disabling, and termination events that affect /etc/opasswd."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030874
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030874
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030880 | PATCH | All uses of the rename command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030880
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030880
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030890 | PATCH | All uses of the renameat command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030890
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030890
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030900 | PATCH | All uses of the rmdir command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030900
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030900
-      - notimplemented
-
-- name: "MEDIUM | RHEL-07-030910 | PATCH | All uses of the unlink command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030910
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030910
-      - notimplemented
-- name: "MEDIUM | RHEL-07-030920 | PATCH | All uses of the unlinkat command must be audited."
-  command: "true"
-  changed_when: no
-  when: rhel_07_030920
-  tags:
-      - cat2
-      - medium
-      - patch
-      - RHEL-07-030920
-      - notimplemented
BREAKS HERE
-- include: /srv/web/infra/ansible/playbooks/include/proxies-reverseproxy.yml
BREAKS HERE
-  command: "true"
-  changed_when: no
-  when: rhel_07_031010
-      - notimplemented
BREAKS HERE
-          env: "{{director_env|combine(containers_env, director_env_extra|default({}))}}"
BREAKS HERE
-- name: Remove the blacklisted packages
-  package:
-    name: "{{ openstack_hosts_package_list | selectattr('state','equalto','absent') | map(attribute='name') | list }}"
-    state: absent
-
BREAKS HERE
-#     - name: get show interface output
BREAKS HERE
-        - inventory_hostname in groups[mgr_group_name]
-          or groups[mgr_group_name] | length == 0
BREAKS HERE
-    - name: make sure we are not running with TESTWORKSHOP as the name so no overlap
BREAKS HERE
-    remotepath: /
BREAKS HERE
-        name: ../../galaxy/openshift-ansible/roles/openshift_openstack
-        tasks_from: container-storage-setup.yml
BREAKS HERE
-- name: Update apt sources
-    cache_valid_time: 600
-  register: apt_update
-  until: apt_update|success
-  retries: 5
-  delay: 2
BREAKS HERE
-    when: webcontext.stdout.find('httpd_sys_content_t') == -1
BREAKS HERE
-- hosts: localhost
-  name:  Create empty links file
-  tags:  [ links ]
-  tasks:
-  - file: path={{links|default('links.txt')}} state=absent
-  - file: path={{links|default('links.txt')}} state=touch
-
-  serial: 1
-  - snmp_facts:
-      host: "{{ip}}"
-    lineinfile: 
-      dest: "{{links|default('links.txt')}}"
-      line: "{{inventory_hostname}} {{item.description}}"
-    with_items: "{{ansible_interfaces.values()}}"
-    delegate_to: localhost
-    when: |
-      {% if item.description.find('to ') >= 0 %}
-        {% set host = item.description.partition('to ')[2] %}
-        {{
-          host and
-          (host > inventory_hostname or (not host in hostvars))
-        }}
-      {% else %} False {% endif %}
-    tags: [ links ]
BREAKS HERE
-      delegate_to: "{{ delegate_node }}"
BREAKS HERE
-  - name: Clean subscription from hosts
-    hosts: all
-    gather_facts: False
BREAKS HERE
-  ansible_become: yes
BREAKS HERE
-      - name: copy our auth key to the hypervisor
-        copy:
-            src: "{{ ansible_ssh_private_key_file | expanduser | realpath }}"
-            dest: "~/backup_server_auth_key"
-            mode: "0400"
BREAKS HERE
-    tags: ssh-config
BREAKS HERE
-  - name: Open iptables port 80
-    iptables:
-      action: insert
-      protocol: tcp
-      destination_port: "80"
-      state: present
-      chain: INPUT
-      jump: ACCEPT
-    when:
-    - qe_quay_ssl_lets_encrypt_certs|d(False)|bool
BREAKS HERE
-   - copr/frontend
BREAKS HERE
-    shell: rm -rf /etc/systemd/system/ceph*
BREAKS HERE
-- include: pre_sanity.yml
-- include: reset_helm.yml
-- include: remove_pvs.yml
-- include: reset_nfs.yml
-- include: reset_drain.yml
-- include: reset.yml
-- include: reset-weave.yml
-- include: prerequisites.yml
-- include: common.yml
-- include: master.yml
-- include: network.yml
-- include: node.yml
-- include: post_cluster_deploy.yml
-- include: helm.yml
-- include: rook_clean.yml
-- include: rook.yml
-- include: sanity.yml
BREAKS HERE
-        - osrelease | version_compare('3.7', '<')
-        - osrelease | version_compare('3.7', '>=')
-        - osrelease | version_compare('3.10', '<')
-        - osrelease | version_compare('3.10', '>=')
-      when: osrelease version_compare('3.7', '>=')
-      when: osrelease | version_compare('3.7', '>=')
-      until: octag_result|succeeded
-      until: pullr|succeeded
-      - osrelease | version_compare('3.7', '>=')
-      - osrelease | version_compare('3.7', '>=')
-        - osrelease | version_compare('3.9.0', '>=')
-        - osrelease | version_compare('3.9.25', '<=')
-        - osrelease | version_compare('3.9.0', '>=')
-        - osrelease | version_compare('3.9.25', '<=')
-      when:
-        - install_aws_broker|bool
BREAKS HERE
-    - shell: '~/{{ tester.dir }}/runner.sh {{ tester.testset }} /root/nosetests.xml | tee "~/{{ tester.dir }}.log"'
BREAKS HERE
-           url: "https://dl.fedoraproject.org/pub/fedora/linux/releases/29/Cloud/x86_64/images/Fedora-Cloud-Base-28-1.1.x86_64.qcow2" }
BREAKS HERE
-    # TODO(logan): Remove the following line once
-    # upstream issue https://github.com/ceph/ceph-ansible/issues/2111 is fixed
-    radosgw_ssl: no
BREAKS HERE
-  
-    
-    
-    - { role: "set-repositories", when: 'repo_method is defined' }
-    - { role: "common", when: 'install_common' }
-    - { role: "set_env_authorized_key", when: 'set_env_authorized_key' }
-  
-    
-    
-    -  { role: "bastion", when: 'install_bastion' }
-  
-    
-    
BREAKS HERE
-      - name: check for existing nodes
-            - oc_nodes.stdout|from_json
-            - oc_nodes.stdout|from_json
BREAKS HERE
-    - name: Remove Tiller namespace.
-        state: present
BREAKS HERE
-        url: https://git.openstack.org/cgit/openstack/openstack-ansible/plain/ansible-role-requirements.yml
BREAKS HERE
-  - name: Install createrepo
-    yum: pkg=createrepo state=present
BREAKS HERE
-  include: prereq.yml
BREAKS HERE
-ANSIBLE_SSH_PIPELINING:
-  # TODO: move to ssh plugin
-  default: False
-  description:
-    - Pipelining reduces the number of SSH operations required to execute a module on the remote server,
-      by executing many Ansible modules without actual file transfer.
-    - This can result in a very significant performance improvement when enabled.
-    - "However this conflicts with privilege escalation (become). For example, when using 'sudo:' operations you must first disable 'requiretty' in /etc/sudoers on all managed hosts, which is why it is disabled by default."
-  env: [{name: ANSIBLE_SSH_PIPELINING}]
-  ini:
-  - {key: pipelining, section: ssh_connection}
-  type: boolean
-  yaml: {key: ssh_connection.pipelining}
BREAKS HERE
-    - ceph_osd_container_stat.get('rc') == 0
BREAKS HERE
-# Execute tempest against the overcloud deployment
BREAKS HERE
-  hosts: haproxy_all
-      when: groups['haproxy_all'] | length > 1
BREAKS HERE
-  - { role: docker_setup, when: no_docker_storage_setup == false, device: '/dev/vdb'}
BREAKS HERE
-    src: vault_logrotate.j2
-    src: vault_listener.hcl.j2
-    src: vault_bsdinit.j2
-    src: vault_sysvinit.j2
-    src: vault_debian.init.j2
-    src: vault_systemd.service.j2
BREAKS HERE
-  - name: Create log directory for node
-    file:
-      state: directory
-      path: /tmp/{{ inventory_hostname }}
-    become: false
BREAKS HERE
-- name: Remove the reminent neutron agent containers
-    - name: Search for old containers not yet deleted
-      shell: >-
-        while read line; do lxc-destroy -fn $line; done < /etc/openstack_deploy/leapfrog_remove_remaining_old_containers
-      failed_when: false
-      args:
-        executable: /bin/bash
BREAKS HERE
-  gather_facts: True
BREAKS HERE
-  gather_facts: false
BREAKS HERE
-    command: subscription-manager register --force --username={{ hostvars['bastion']['user'] }} --password={{ hostvars['bastion']['pwd'] }}
-    command: "subscription-manager attach --pool={{ hostvars['bastion']['poolid'] }}"
BREAKS HERE
-  when: ansible_distribution_major_version|int >=7 and nmclitest|success and ( not ansible_ifcfg_blacklist) and not nm_controlled_resolv
-  when: ansible_distribution_major_version|int >=7 and nmclitest|success and ( not ansible_ifcfg_blacklist )
-  when: (virthost is not defined) and (item.startswith(('eth','br','enc'))) and (hostvars[inventory_hostname]['ansible_' + item.replace('-','_')]['type'] == 'ether') and (ansible_distribution_major_version|int >=7) and hostvars[inventory_hostname]['ansible_' + item.replace('-','_')]['active'] and nmclitest|success and ( not ansible_ifcfg_blacklist ) and ( ansible_ifcfg_whitelist is not defined or item in ansible_ifcfg_whitelist )
BREAKS HERE
-- name: Determine latest openSUSE container build information
-  uri:
-    url: "{{ _lxc_hosts_container_image_url_base }}/Dockerfile"
-    return_content: true
-  register: _lxc_opensuse_image_build_info
-- name: Set fact for openSUSE container build information
-  set_fact:
-    opensuse_image_build_info: "{{ _lxc_opensuse_image_build_info.content|regex_search('Version: (.*)', '\\1')|join(' ') }}"
BREAKS HERE
-nova_spicehtml5_git_repo: https://gitlab.freedesktop.org/spice/spice-html5
BREAKS HERE
-    - name: set selinux to permissive for ospd-8 (workaround bug bz 1284133)
-      selinux: policy=targeted state=permissive
-      sudo: yes
-      when: (workarounds['rhbz1280101']['enabled'] is defined and workarounds['rhbz1280101']['enabled'] | bool)
-
BREAKS HERE
-  when:
-    - "neutron_services['neutron-server']['group'] in group_names"
-  when:
-    - "neutron_services['neutron-server']['group'] in group_names"
BREAKS HERE
-  become: true
-             - install.undercloud.packages is defined
BREAKS HERE
-  when: is_rhel_compatible
-    policy=targeted
-    state=permissive
-  when: is_rhel_compatible
-    policy=targeted
-    state=enforcing
-  when: 'is_rhel_compatible'
BREAKS HERE
-     action: authorized_key user=root key='$PIPE(${auth_keys_from_fas} @sysadmin-main ${root_auth_users})'
BREAKS HERE
-          Exiting shrink-osd playbook, no OSD()s was/were removed.
-        osd_hosts: "{{ (item.stdout | from_json).crush_location.host }}"
BREAKS HERE
-  sudo: yes
-  sudo_user: '{{ app_user }}'
BREAKS HERE
-# Once the swift cluster has been setup DO NOT change these hash values!
-swift_hash_path_suffix:
-swift_hash_path_prefix:
BREAKS HERE
-  yaml: {key: plugins.connection.path}
BREAKS HERE
-- include_tasks: mq_setup.yml
-  with_items:
-    - oslomsg_setup_host: "{{ nova_oslomsg_rpc_setup_host }}"
-      oslomsg_userid: "{{ nova_oslomsg_rpc_userid }}"
-      oslomsg_password: "{{ nova_oslomsg_rpc_password }}"
-      oslomsg_vhost: "{{ nova_oslomsg_rpc_vhost }}"
-      oslomsg_transport: "{{ nova_oslomsg_rpc_transport }}"
-    - oslomsg_setup_host: "{{ nova_oslomsg_notify_setup_host }}"
-      oslomsg_userid: "{{ nova_oslomsg_notify_userid }}"
-      oslomsg_password: "{{ nova_oslomsg_notify_password }}"
-      oslomsg_vhost: "{{ nova_oslomsg_notify_vhost }}"
-      oslomsg_transport: "{{ nova_oslomsg_notify_transport }}"
-  no_log: true
-  tags:
BREAKS HERE
-         - "spark_defaults['sparn_executor_memory'] : {{ hostvars['localhost']['spark_defaults']['spark_yarn_executor_memory'] }}"
BREAKS HERE
-      shell: PATH=/home/isucon/.local/perl/bin:$PATH carton install
-    # TODO: - carton install (perl)
BREAKS HERE
-    SSLCertificateChainFile: fedoracommunity.org.intermediate.crt
BREAKS HERE
-  - name: debug checkout ec2_facts
-    debug:
-      var: ec2_facts
-
BREAKS HERE
-  tags: ocp-pre
-  tags: logging-metrics
-  tags: logging-metrics
-    admin_user: "{{ openshift_master_htpasswd_users.keys() }}"
BREAKS HERE
-       path: './aws-private.pem'
BREAKS HERE
-    restart_app: "{{ ansible_location != 'docker' }}"
BREAKS HERE
-    with_items: vmlist.list_vms
BREAKS HERE
-        aws_region: "{{ _region.region }}"
-        host_suffix: "{{ _region.host_suffix }}"
BREAKS HERE
-          {{ host }} ansible_host={{ hostvars[host]['ansible_host'] }} ansible_become=true ansible_user={{ hostvars[host]['ansible_user'] }}
-        src: "{{ zuul.executor.work_root }}/{{ zuul.project.src_dir }}/ansible/inventory/all-in-one"
-    - name: generate global.yml file
-        api_interface_name: "{{ hostvars['primary']['ansible_default_ipv4'].alias }}"
-        api_interface_address: "{{ hostvars['primary']['nodepool']['private_ipv4'] }}"
-        src: "{{ zuul.executor.work_root }}/{{ zuul.project.src_dir }}/tests/templates/globals-default.j2"
-        src: "{{ zuul.executor.work_root }}/{{ zuul.project.src_dir }}/tests/templates/nova-compute-overrides.j2"
-        src: "{{ zuul.executor.work_root }}/{{ zuul.project.src_dir }}/etc/kolla/passwords.yml"
-        requirements: "{{ ansible_env.HOME }}/{{ zuul.project.src_dir }}/requirements.txt"
-      shell: "{{ zuul.project.src_dir }}/tools/generate_passwords.py"
-        chdir: "{{ zuul.project.src_dir }}"
BREAKS HERE
-    apps_dns: "{{app_dns | default(default_apps_dns)}}"
-    master_dns: "{{ master_dns | default(default_master_dns)}}"
BREAKS HERE
-  - /var/lib/lxc
BREAKS HERE
-- name: Install Quay Enterprise
-  hosts: quay_enterprise
-    - name: Include Quay Role
-        name: config-quay-enterprise
BREAKS HERE
-      with_items: "{{ running_osds.stdout_lines }}"
BREAKS HERE
-  become: True
-  gather_facts: True
-    - name: Install pip requirements
-      pip:
-        name: "{{ item }}"
-        state: "{{ ironic_pip_package_state }}"
-      with_items:
-        - python-neutronclient
-        - shade
-      register: install_packages
-      until: install_packages|success
-      retries: 5
-      delay: 2
-  vars_files:
-    - common/test-vars.yml
BREAKS HERE
-- name: update known_hosts file
-    src: etc/ssh/ssh_known_hosts.j2
-    dest: /etc/ssh/ssh_known_hosts
BREAKS HERE
-- hosts: localhost
-- hosts: localhost
-- hosts: localhost
BREAKS HERE
-- name: push packages out
-  hosts: pdc-backend;pdc-backend-stg;pdc-web;pdc-web-stg
-  tasks:
-    yum: name="pdc-*" state=latest
-
-- name: verify the backend, and stop it
-  hosts: pdc-backend;pdc-backend-stg
-  user: root
-  vars_files: 
-   - /srv/web/infra/ansible/vars/global.yml
-   - "/srv/private/ansible/vars.yml"
-   - /srv/web/infra/ansible/vars/{{ ansible_distribution }}.yml
-  handlers:
-  - include: "{{ handlers }}/restart_services.yml"
-    when: inventory_shortname == 'pdc-web01'
BREAKS HERE
-  - name: Static network for VOIP network
-    copy: src='{{files_folder}}/hostname.em1' dest="/etc/hostname.em1" owner="root" group="wheel" mode="0640"
-
-  - name: Define bridge
-    copy: src='{{files_folder}}/hostname.bridge0' dest='/etc/hostname.bridge0' owner='root' group='wheel' mode='0640'
BREAKS HERE
-   - { role: memcached, tags: [ 'memcached' ] }
BREAKS HERE
-      lxc_container_release: trusty
-      lxc_container_backing_store: dir
-      global_environment_variables:
-        PATH: "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
BREAKS HERE
-  description: Colon separated list of Ansible inventory sources
BREAKS HERE
-  hosts: testdays
BREAKS HERE
-nova_linuxnet_interface_driver: nova.network.linux_net.NeutronLinuxBridgeInterfaceDriver
-
-
BREAKS HERE
-    - datacheck
BREAKS HERE
-  failed_access: yes                              # V-38566
BREAKS HERE
-      command: "oc patch daemonset dockergc --patch='\"spec\": { \"template\": { \"spec\": { \"containers\": [ { \"command\": [ \"/usr/bin/oc\" ], \"name\": \"dockergc\" } ] } } }' -n default"
-      command: "oc delete pod $(oc get pods -n default|grep dockergc|awk -c '{print $1}') -n default"
BREAKS HERE
-      env: "MON_IP={{ hostvars[inventory_hostname]['ansible_' + monitor_interface]['ipv4']['address'] }},CEPH_DAEMON=MON,CEPH_PUBLIC_NETWORK={{ ceph_mon_docker_subnet }},CEPH_FSID={{ ceph_fsid.stdout }},{{ ceph_mon_docker_extra_env }}"
BREAKS HERE
-# file: primogen/tests/vagrant.yml
BREAKS HERE
-# pull dump from prod to batcave
-#
-  - fetch:
-       src: /var/tmp/fas2.dump.xz
-       dest: /var/tmp/fas2.dump.xz
-    delegate_to: db-fas01.phx2.fedoraproject.org
-#
BREAKS HERE
-security_admin_space_left_action: SUSPEND                  # V-54381
BREAKS HERE
-    roles: ["web", "database", "cache", "dbcache"]
BREAKS HERE
-  hosts: utility_all
-    # This sets the tempest group to the utility group
-    tempest_main_group: utility_all
BREAKS HERE
-          - docker
-          - ansible
-          - docker
-          - ansible
BREAKS HERE
-    keys_tmp: "{{ keys_tmp|default([]) + [ { 'key': item.key, 'name': item.name, 'caps': { 'mon': item.mon_cap|quote, 'osd': item.osd_cap|default('')|quote, 'mds': item.mds_cap|default('')|quote, 'mgr': item.mgr_cap|default('')|quote } , 'mode': item.mode } ] }}"
BREAKS HERE
-        echo dash > /usr/lib/x86_64-linux-gnu/guestfs/supermin.d/zz-dash-packages
-        rm -rf /var/tmp/.guestfs*
-        - "{{ (_libguestfs_version.stdout is version('1.38.1', '<')) or
-              ((_libguestfs_version.stdout is version('1.39.0', '>=')) and
-               (_libguestfs_version.stdout is version('1.39.1', '<'))) }}"
BREAKS HERE
-    - name: ensure correct key_file permission
-      file: path={{ provisioner.key_file }} mode=0600
-
BREAKS HERE
-    localpath: /bodhi2
BREAKS HERE
-    mode: "{{ item.item.mode }}"
BREAKS HERE
-        when: rhel_07_020270_patch | changed
-        when: item | changed
-            - item | changed
-            - item | changed
-            - item | changed
-            - item | changed
-      - result | failed
-      - rhel7stig_auditd_space_left_action_result | failed
-      - rhel7stig_auditd_action_mail_acct_result | failed
-      - result | failed
-      - result | failed
-      - not ansible_distribution_version | version_compare('7.4', '>=') or
-      - result | failed
BREAKS HERE
-  hosts: copr-front-stg
-  hosts: copr-front-stg
-  gather_facts: false
BREAKS HERE
-              - debug: var=ansible_user
BREAKS HERE
-          username: "{{ provision.user }}"
-          password: "{{ provision.password }}"
BREAKS HERE
-      tags: ['php','httpd']
BREAKS HERE
-    - {{ nomad_data_dir }}
-    - {{ nomad_config_dir }}
-    - {{ nomad_log_dir }}
BREAKS HERE
-# DEPLOY ALL THE THINGS!  Depending on the currently selected set of
-# tags, this will deploy the undercloud, deploy the overcloud, and
-# perform some validation tests on the overcloud.
BREAKS HERE
-    command: semanage fcontext -a -t httpd_sys_content_t "/var/www/html/(.*)"
BREAKS HERE
-    # TODO: if release != master use rdo/rhos release rpm/tool
BREAKS HERE
-      connection: local
-      connection: local
BREAKS HERE
-
-  vars:
-    aodh_rabbitmq_userid: aodh
-    aodh_rabbitmq_vhost: /aodh
-    aodh_rabbitmq_servers: "{{ rabbitmq_servers }}"
-    aodh_rabbitmq_port: "{{ rabbitmq_port }}"
-    aodh_rabbitmq_use_ssl: "{{ rabbitmq_use_ssl }}"
BREAKS HERE
-- name: "Create and start container"
-  vars:
-    docker_cmd: "docker run -d --name=plex -p {{plex.port}}:{{plex.port}}/tcp -e PLEX_UID={{uid.stdout}} -e PLEX_GID={{gid.stdout}} -e CHANGE_CONFIG_DIR_OWNERSHIP=false -v /etc/localtime:/etc/localtime:ro -v /opt/plex:/config -v '{{plex.transcodes}}:/transcode' -v /mnt/unionfs/Media:/data --restart=always --network=host plexinc/pms-docker"
-  command: "{{docker_cmd}}"
BREAKS HERE
-      port: "{{ item.nodePort }}"
-      port: "{{ item.nodePort }}"
BREAKS HERE
-    when: "forever_list.stdout.find('{{ node_apps_location}}/app/app.js') == -1"
BREAKS HERE
-- include_tasks: mq_setup.yml
-  with_items:
-    - oslomsg_setup_host: "{{ neutron_oslomsg_rpc_setup_host }}"
-      oslomsg_userid: "{{ neutron_oslomsg_rpc_userid }}"
-      oslomsg_password: "{{ neutron_oslomsg_rpc_password }}"
-      oslomsg_vhost: "{{ neutron_oslomsg_rpc_vhost }}"
-      oslomsg_transport: "{{ neutron_oslomsg_rpc_transport }}"
-    - oslomsg_setup_host: "{{ neutron_oslomsg_notify_setup_host }}"
-      oslomsg_userid: "{{ neutron_oslomsg_notify_userid }}"
-      oslomsg_password: "{{ neutron_oslomsg_notify_password }}"
-      oslomsg_vhost: "{{ neutron_oslomsg_notify_vhost }}"
-      oslomsg_transport: "{{ neutron_oslomsg_notify_transport }}"
-  no_log: true
BREAKS HERE
-    - {id: "030560", path: "/usr/sbin/semanage", extra_fields: "-F perm=x", key: "privileged-priv_change", create: "{{ rhel_07_030560 }}"}
-    - {id: "030570", path: "/usr/sbin/setsebool", extra_fields: "-F perm=x", key: "privileged-priv_change", create: "{{ rhel_07_030570 }}"}
-    - {id: "030580", path: "/usr/bin/chcon", extra_fields: "-F perm=x", key: "privileged-priv_change", create: "{{ rhel_07_030580 }}"}
-    - {id: "030590", path: "/usr/sbin/setfiles", extra_fields: "-F perm=x", key: "privileged-priv_change", create: "{{ rhel_07_030590 }}"}
-    - {id: "030630", path: "/usr/bin/passwd", extra_fields: "-F perm=x", key: "privileged-passwd", create: "{{ rhel_07_030630 }}"}
-    - {id: "030640", path: "/usr/sbin/unix_chkpwd", extra_fields: "-F perm=x", key: "privileged-passwd", create: "{{ rhel_07_030640 }}"}
-    - {id: "030650", path: "/usr/bin/gpasswd", extra_fields: "-F perm=x", key: "privileged-passwd", create: "{{ rhel_07_030650 }}"}
-    - {id: "030660", path: "/usr/bin/chage", extra_fields: "-F perm=x", key: "privileged-passwd", create: "{{ rhel_07_030660 }}"}
-    - {id: "030670", path: "/usr/sbin/userhelper", extra_fields: "-F perm=x", key: "privileged-passwd", create: "{{ rhel_07_030670 }}"}
-    - {id: "030680", path: "/usr/bin/su", extra_fields: "-F perm=x", key: "privileged-priv_change", create: "{{ rhel_07_030680 }}"}
-    - {id: "030690", path: "/usr/bin/sudo", extra_fields: "-F perm=x", key: "privileged-priv_change", create: "{{ rhel_07_030690 }}"}
-    - {id: "030710", path: "/usr/bin/newgrp", extra_fields: "-F perm=x", key: "privileged-priv_change", create: "{{ rhel_07_030710 }}"}
-    - {id: "030720", path: "/usr/bin/chsh", extra_fields: "-F perm=x", key: "privileged-priv_change", create: "{{ rhel_07_030720 }}"}
-    - {id: "030730", path: "/bin/sudoedit", extra_fields: "-F perm=x", key: "privileged-priv_change", create: "{{ rhel_07_030730 }}"}
-    - {id: "030740", path: "/bin/mount", key: "privileged-mount", create: "{{ rhel_07_030740 }}"}
-    - {id: "030740", path: "/usr/bin/mount", key: "privileged-mount", create: "{{ rhel_07_030740 }}"}
-    - {id: "030750", path: "/bin/umount", extra_fields: "-F perm=x", key: "privileged-mount", create: "{{ rhel_07_030750 }}"}
-    - {id: "030760", path: "/usr/sbin/postdrop", extra_fields: "-F perm=x", key: "privileged-postfix", create: "{{ rhel_07_030760 }}"}
-    - {id: "030770", path: "/usr/sbin/postqueue", extra_fields: "-F perm=x", key: "privileged-postfix", create: "{{ rhel_07_030770 }}"}
-    - {id: "030780", path: "/usr/libexec/openssh/ssh-keysign", extra_fields: "-F perm=x", key: "privileged-ssh", create: "{{ rhel_07_030780 }}"}
-    - {id: "030800", path: "/usr/bin/crontab", extra_fields: "-F perm=x", key: "privileged-cron", create: "{{ rhel_07_030800 }}"}
-    - {id: "030810", path: "/sbin/pam_timestamp_check", extra_fields: "-F perm=x", key: "privileged-pam", create: "{{ rhel_07_030810 }}"}
-    - {id: "030840", path: "/sbin/insmod", key: "module_change", create: "{{ rhel_07_030840 }}"}
-    - {id: "030850", path: "/sbin/rmmod", key: "module_change", create: "{{ rhel_07_030850 }}"}
-    - {id: "030860", path: "/sbin/modprobe", key: "module_change", create: "{{ rhel_07_030860 }}"}
BREAKS HERE
-            ironic node-update {{ item }} add properties/root_device='{"size": {{ hostvars[item].disk  }} }'
BREAKS HERE
-    - name: stop and disable rbd-target-api daemon
-        name: rbd-target-api
-        state: stopped
-        enabled: no
-      when: igw_purge_type == 'all'
-
-    - name: stop and disable rbd-target-gw daemon
-      service:
-        name: rbd-target-gw
BREAKS HERE
-- name: "Perform control machine setup"
-  hosts: localhost
-      when: "log"
-- name: "Collect network information and perform device-specific tests"
-  hosts: ospf_routers
-  vars_files:
-    - "login_creds.vault"
-  tasks:
-    - name: "SYS >> Ensure login_creds is defined with non-empty 'host' field"
-      assert:
-        that:
-          - "login_creds is defined and login_creds.keys() | length"
-          - "login_creds.host is defined and login_creds.host"
-        msg: |-
-          login_creds is malformed. This could signify a problem with the
-          vault or variable inclusion. For an insecure workaround, you can
-          define the 'login_creds' dictionary at the play variables level
-          with plain-text passwords.
-    - name: "INCLUDE >> Use correct commands for {{ device_type }} device"
-      include_tasks: "devices/{{ device_type }}/main.yml"
-
-- name: "Perform network-wide validation tests"
-  hosts: localhost
-  tasks:
-    - name: "SYS >> Store empty OSPF RID list for future comparison"
-      set_fact:
-        OSPF_RIDS: []
-    - name: "SYS >> Build OSPF RID list by iterating through hostvars"
-      set_fact:
-        OSPF_RIDS: "{{ OSPF_RIDS + [item.value.OSPF_BASIC.process.rid] }}"
-      when: >-
-        item.key in groups.ospf_routers and
-        item.value.OSPF_BASIC is defined
-      with_dict: "{{ hostvars }}"
-      loop_control:
-        label: "host:{{ item.key }}"
-    - name: "SYS >> Assert that there are no duplicate OSPF RIDs"
-      assert:
-        that: "OSPF_RIDS | unique | length == OSPF_RIDS | length"
-        msg: |-
-          OSPF_RIDS contained duplicates. Check the logs to find the bad nodes.
-          {{ OSPF_RIDS | to_nice_json }}
BREAKS HERE
-    - name: Patch instack-virt-setup to ensure dhcp.leases is not used to determine ip (workaround https://review.openstack.org/#/c/232584)
-      sudo: yes
-      lineinfile:
-        dest=/usr/bin/instack-virt-setup
-        regexp="/var/lib/libvirt/dnsmasq/default.leases"
-        line="    IP=$(ip n | grep $(tripleo get-vm-mac $UNDERCLOUD_VM_NAME) | awk '{print $1;}')"
-      when: workarounds.enabled is defined and workarounds.enabled|bool
-
BREAKS HERE
-  connection: network_cli
BREAKS HERE
-    - playbooks/test-vars.yml
BREAKS HERE
-nova_oslomsg_rpc_transport: rabbit
-nova_oslomsg_rpc_servers: 127.0.0.1
-nova_oslomsg_rpc_port: 5672
-nova_oslomsg_rpc_use_ssl: False
-nova_oslomsg_notify_transport: rabbit
-nova_oslomsg_notify_servers: 127.0.0.1
-nova_oslomsg_notify_port: 5672
-nova_oslomsg_notify_use_ssl: False
BREAKS HERE
-    mailman_postfix_mydestination: "lists.fedoraproject.org, lists.stg.fedoraproject.org"
BREAKS HERE
-    force: no
-  shell: creates=/usr/bin/etcd tar vxzf /tmp/etcd-{{ etcd_version }}-linux-amd64.tar.gz && mv etcd-{{ etcd_version }}-linux-amd64/etcd* /usr/bin
BREAKS HERE
-- hosts: all
BREAKS HERE
-        when: install.version|openstack_distribution == 'RDO'
-            state: "{{ item.name | default(omit) }}"
BREAKS HERE
-        msg: "{{ core_apps }}"
-        msg: "{{ acme_templates | to_nice_yaml  }}"
-        msg: "{{ rb_template | to_nice_yaml }}"
-        msg: "{{ role_template | to_nice_yaml }}"
-        msg: "{{ base_acme_templates | to_nice_yaml }}"
BREAKS HERE
-    - { role: helios_prep, when: deploy_helios == True}
-    - { role: helios_broker_setup, when: (not broker_ci) and (deploy_helios == True)}
BREAKS HERE
-nova_compute_init_overrides:
-  Unit:
-    ? libvirtd.service
-    ? syslog.target
-    ? network.target
BREAKS HERE
-            key: "{{ item }}"
-        with_file:
-            - "{{ inventory_dir }}/id_rsa.pub"
BREAKS HERE
-nova_db_max_pool_size: 5
BREAKS HERE
-        src: "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/files/prometheus_alerts_labs.yml"
-        dest: /root/prometheus_alerts_labs.yml
BREAKS HERE
-        for F in $(ls -d1 /var/log/rpm.list {{ job.archive|join(' ') }}); do
BREAKS HERE
-        shell: "python maven-chain-builder.py opendaylight-chain.ini {{ ansible_ssh_user }}"
BREAKS HERE
-  template: src={{ item }} dest=/etc/sysconfig/iptables mode=600 backup=yes
-  - iptables/iptables.{{ ansible_fqdn }}
-  - iptables/iptables.{{ host_group }}
-  - iptables/iptables.{{ env }}
-  - iptables/iptables
BREAKS HERE
-        - item.min_size | default(osd_pool_default_min_size) != ceph_osd_pool_default_min_size
BREAKS HERE
-# This defines the user that deploys the overcloud from the undercloud
-undercloud_user: "stack"
BREAKS HERE
-    - "magnum"
BREAKS HERE
-        rule_id: "{{ rule.json.id }}"
-        rule_id: "{{ rule.json.id }}"
-    #
-    # - name: Delete stream
-    #   graylog_streams:
-    #     action: delete
-    #     endpoint: "{{ endpoint }}"
-    #     graylog_user: "{{ graylog_user }}"
-    #     graylog_password: "{{ graylog_password }}"
-    #     stream_id: "{{ stream.json.id }}"
-    - name: Update pipeline
-        description: "test description update"
-      register: pipeline
BREAKS HERE
-      when: docker_install|default(false)
-      when: docker_install|default(false)
-        clair_ssl_trust_src_file: "{{ hostvars[groups['quay_enterprise'][0]]['quay_ssl_cert_file'] if hostvars[groups['quay_enterprise'][0]]['quay_ssl_enable'] is defined and hostvars[groups['quay_enterprise'][0]]['quay_ssl_enable']|bool else '' }}"
-        quay_builder_ssl_trust_src_file: "{{ hostvars[groups['quay_enterprise'][0]]['quay_ssl_cert_file'] if hostvars[groups['quay_enterprise'][0]]['quay_ssl_enable'] is defined and hostvars[groups['quay_enterprise'][0]]['quay_ssl_enable']|bool else '' }}"
BREAKS HERE
-- include: neutron_check.yml
-- include: neutron_ml2_ovs_powervm.yml
-  static: no
-- include: neutron_pre_install.yml
-- include: neutron_install.yml
-- include: "{{ item }}"
-- include: neutron_post_install.yml
-- include: "neutron_init_{{ ansible_service_mgr }}.yml"
-- include: neutron_db_setup.yml
-  static: no
-- include: neutron_service_setup.yml
-  static: no
-- include: neutron_l3_ha.yml
-  static: no
BREAKS HERE
-    - { role: cleanup_nodes/openstack/fip-nova,
-              nodes: '{{ cleanup_nodes }}' }
-    - { role: cleanup_nodes/openstack/fip-neutron,
-              nodes: '{{ cleanup_nodes }}' }
-    - { role: cleanup_nodes/rax, nodes: '{{ cleanup_nodes }}' }
BREAKS HERE
-  - 0.north-america.pool.ntp.org
-  - 1.north-america.pool.ntp.org
-  - 2.north-america.pool.ntp.org
-  - 3.north-america.pool.ntp.org
BREAKS HERE
-  sudo: true
BREAKS HERE
-          path: "{{ provision.key_file }}"
-      when: provision.key_file is defined
BREAKS HERE
-    state: latest
BREAKS HERE
-          content: "export VAULT_ADDR='{{lookup('env','VAULT_ADDR')}}'\nexport VAULT_TOKEN='{{vault_init['root_token']}}'\nexport VAULT_KEYS='{{vault_init['keys'] | join(' ')}}'\n"
BREAKS HERE
-  when: copr_results_dir_st.stat.pw_name != copr
BREAKS HERE
-      when: deployment_type == "prod", tags: ['openvpn_client'] }
BREAKS HERE
-    - name: create site-enabled directory
-      file:
-        name: /etc/httpd/conf/sites-enabled
-        state: directory
-
BREAKS HERE
-                ansible_python_interpreter: "/tmp/venv_shade/bin/python"
-            path_venv: "/tmp/venv_shade"
BREAKS HERE
-        delay: 5
BREAKS HERE
-- hosts: memcached
-    - container_extra_setup
-    - common
-    - container_common
-    - memcached
-  vars_files:
-    - vars/config_vars/container_config_memcached.yml
-    - vars/repo_packages/memcached.yml
BREAKS HERE
-    src: /dev/sdb
-    fstype: ext4
-    state: present
BREAKS HERE
-  when: >
-    inventory_hostname == groups['designate_all'][0]
-  when: >
-    inventory_hostname == groups['designate_all'][0]
-
BREAKS HERE
-  with_items: "{{ lxc_packages }}"
BREAKS HERE
-                  flv_min_disk_size: 19
-                  --disk {{ [(item.disk | int) - 1, flv_min_disk_size] | max }}
BREAKS HERE
-    - name: "MEDIUM | RHEL-07-030360 | PATCH | All privileged function executions must be audited. (find suid/sgid programs)"
-      command: find "{{ item.mount }}" -type f ( -perm -4000 -o -perm -2000 ) -xdev
-      changed_when: false
-      register: rhel_07_030360_audit
-      when: item['device'].startswith('/dev') and not 'bind' in item['options']
-      with_items:
-        - "{{ ansible_mounts }}"
-    - name: "MEDIUM | RHEL-07-030360 | PATCH | All privileged function executions must be audited. (add programs to audit file)"
-      include_tasks: audit_suid_sgid_command.yml
-      when:
-        - item.stdout_lines is defined
-        - item.stdout_lines | length > 0
-      with_items: "{{ rhel_07_030360_audit.results }}"
-      loop_control:
-        loop_var: outer_item
-      
BREAKS HERE
-  url: "https://bitbucket.org/pypy/pypy/downloads/pypy2-v5.3.1-linux64.tar.bz2"
-  sha256: "6d0e8b14875b76b1e77f06a2ee3f1fb5015a645a951ba7a7586289344d4d9c22"
BREAKS HERE
-        baseurl: "{{ repo_url }}/TOOLS/x86_64/os/"
BREAKS HERE
-- import_tasks: mq_setup.yml
-  when:
-    - "designate_services['designate-api']['group'] in group_names"
-    - "inventory_hostname == ((groups[designate_services['designate-api']['group']]| intersect(ansible_play_hosts)) | list)[0]"
-  vars:
-    _oslomsg_rpc_setup_host: "{{ designate_oslomsg_rpc_setup_host }}"
-    _oslomsg_rpc_userid: "{{ designate_oslomsg_rpc_userid }}"
-    _oslomsg_rpc_password: "{{ designate_oslomsg_rpc_password }}"
-    _oslomsg_rpc_vhost: "{{ designate_oslomsg_rpc_vhost }}"
-    _oslomsg_rpc_transport: "{{ designate_oslomsg_rpc_transport }}"
-    _oslomsg_notify_setup_host: "{{ designate_oslomsg_notify_setup_host }}"
-    _oslomsg_notify_userid: "{{ designate_oslomsg_notify_userid }}"
-    _oslomsg_notify_password: "{{ designate_oslomsg_notify_password }}"
-    _oslomsg_notify_vhost: "{{ designate_oslomsg_notify_vhost }}"
-    _oslomsg_notify_transport: "{{ designate_oslomsg_notify_transport }}"
-    _oslomsg_configure_notify: "{{ designate_ceilometer_enabled | bool }}"
-  tags:
-    - common-mq
-    - designate-config
-
-- include_tasks: designate_db_setup.yml
BREAKS HERE
-- name: Drop lxc-openstack app armor profile
-  template:
-    src: "lxc-openstack.apparmor.j2"
-    dest: "/etc/apparmor.d/lxc/lxc-openstack"
-    owner: "root"
-    group: "root"
-    mode: "0644"
-  notify:
-    - Start apparmor
-    - Reload apparmor
-  tags:
-    - lxc-files
-    - lxc-apparmor
-    - lxc_hosts-config
BREAKS HERE
-      dest: "{{ item.dest }}"
-      regexp: "{{ item.regexp}}"
-      line: "{{ item.line }}"
-      insertbefore: BOF
-  with_items:
-      -   dest: '/etc/ssh/sshd_config'
-          regexp: '^#?PrintLastLog'
-          line: 'PrintLastLog yes'
-      -   dest: '/etc/pam.d/sshd'
-          regexp: '^#?session required pam_lastlog.so'
-          line: 'session required pam_lastlog.so showfailed'
BREAKS HERE
-  with_items:
-    - "{{ neutron_requires_pip_packages }}"
-  with_items:
-    - "{{ neutron_pip_packages }}"
-  with_items:
-    - "{{ neutron_pip_packages }}"
BREAKS HERE
-          env:
-            PGID: 999
BREAKS HERE
-    - setup-host
BREAKS HERE
-
BREAKS HERE
-- name: Add the worker/orchestrator labels to the nodes
-    - name: Add the worker label
-      command: "oc -n {{ osbs_worker_namespace }} label nodes {{ item }} worker=true --overwrite"
-
-    - name: Add the orchestrator labels to the nodes
-      command: "oc -n {{ osbs_namespace }} label nodes {{ item }} orchestrator=true --overwrite"
-      loop: "{{ groups['osbs-nodes'] }}"
BREAKS HERE
-swift_service_publicuri_proto: "{{ swift_service_proto }}"
-swift_service_adminuri_proto: "{{ swift_service_proto }}"
-swift_service_internaluri_proto: "{{ swift_service_proto }}"
BREAKS HERE
-    security_enable_virus_scanner: yes
BREAKS HERE
-          _r_acmesh_host_cert_root: "{{dirs.certificates}}"
-      tags: ['never', 'ssl', 'system']
-      tags: ['never', 'nginx', 'system']
-          clickhouse_listen_host_default: ['::1', '127.0.0.1', "{{if_inner}}"]
-          clickhouse_networks_default: ['::1', '127.0.0.1', "{{docker_net}}"]
BREAKS HERE
-    command: /usr/bin/fcomm-index-packages --index-db-dest=/var/cache/fedoracommunity/packages/xapian --icons-dest /var/cache/fedoracommunity/packages/icons --mdapi-url=https://apps.stg.fedoraproject.org/mdapi --icons-url=https://dl.fedoraproject.org/pub/alt/screenshots --tagger-url=https://apps.stg.fedoraproject.org/tagger creates=/var/cache/fedoracommunity/packages/xapian/search/termlist.glass
-    command: /usr/bin/fcomm-index-packages --index-db-dest=/var/cache/fedoracommunity/packages/xapian --icons-dest /var/cache/fedoracommunity/packages/icons --mdapi-url=https://apps.fedoraproject.org/mdapi --icons-url=https://dl.fedoraproject.org/pub/alt/screenshots --tagger-url=https://apps.fedoraproject.org/tagger creates=/var/cache/fedoracommunity/packages/xapian/search/termlist.glass
-  - name: Make sure the perms are straight 
-    file: path=/var/cache/fedoracommunity/packages/ state=directory owner=apache group=fedmsg mode="g+rw" recursive=yes
BREAKS HERE
-    - name: with_fileglob loop using *.txt
BREAKS HERE
-    - include: common/ensure-oslomsg.yml
-      rpc_vhost: "{{ designate_oslomsg_rpc_vhost }}"
-      rpc_user: "{{ designate_oslomsg_rpc_userid }}"
-      rpc_password: "{{ designate_oslomsg_rpc_password }}"
-      notify_vhost: "{{ designate_oslomsg_notify_vhost }}"
-      notify_user: "{{ designate_oslomsg_notify_userid }}"
-      notify_password: "{{ designate_oslomsg_notify_password }}"
-
-  vars:
-    named_config_file: "{{ (ansible_pkg_mgr == 'apt') | ternary('/etc/bind/named.conf.options','/etc/named.conf') }}"
-    bind_package_name: "{{ (ansible_pkg_mgr == 'apt') | ternary('bind9','bind') }}"
-    bind_service_name: "{{ (ansible_pkg_mgr == 'apt') | ternary('bind9','named') }}"
-    bind_service_state: "{{ (ansible_pkg_mgr == 'apt') | ternary('restarted','started') }}"
-  vars_files:
-    - common/test-vars.yml
BREAKS HERE
-    dest: /etc/homebox/dns-entries.d//40-dovecot.bind
BREAKS HERE
-  command: "{{ splunk.exec }} init shcluster-config -auth 'admin:{{ splunk.password }}' -mgmt_uri 'https://{{ ansible_hostname }}:{{ splunk.svc_port }}' -replication_port {{ splunk.shc.replication_port }} -replication_factor {{ shc_replication_factor }} -conf_deploy_fetch_url 'https://{{ groups['splunk_deployer'][0] }}:{{ splunk.svc_port }}' -secret '{{ splunk.shc.secret }}' -shcluster_label '{{ splunk.shc.label }}'"
-    user: admin
-  command: "{{ splunk.exec }} bootstrap shcluster-captain -servers_list '{% for host in groups['splunk_search_head'] %}https://{{ host }}:{{ splunk.svc_port }}{% if not loop.last %},{% endif %}{% endfor %}' -auth 'admin:{{ splunk.password }}'"
BREAKS HERE
-#  AMI Choice
-#---------------------------------------------------
-- name: Define AMIs to use, based on region, for us-east-1
-  set_fact:
-    nodes:
-      tower_rhel:
-        ami_id: "ami-0394fe9914b475c53" # RHEL-7.5_HVM-20180813-x86_64-0-Hourly2-GP2
-        instance_type: "t2.large"
-      rhel:
-        ami_id: "ami-0394fe9914b475c53" # RHEL-7.5_HVM-20180813-x86_64-0-Hourly2-GP2
-        instance_type: "t2.small"
-      win:
-        ami_id: "ami-01945499792201081" # Windows_Server-2016-English-Full-Base-2018.09.15
-        instance_type: "t2.large"
-  when: region == "us-east-1"
-
-- name: Define AMIs to use, based on region, for us-east-2
-  set_fact:
-    nodes:
-      tower_rhel:
-        ami_id: "ami-04268981d7c33264d" # RHEL-7.5_HVM-20180813-x86_64-0-Access2-GP2
-        instance_type: "t2.large"
-      rhel:
-        ami_id: "ami-04268981d7c33264d" # RHEL-7.5_HVM-20180813-x86_64-0-Access2-GP2
-        instance_type: "t2.small"
-      win:
-        ami_id: "ami-0ca3e3965ada31684" # Windows_Server-2016-English-Full-Base-2018.09.15
-        instance_type: "t2.large"
-  when: region == "us-east-2"
-
-#---------------------------------------------------
-    name: "dp-{{ workshop_prefix }}-key"
-    name: "dp-{{ workshop_prefix }}-vpc"
-      Name: "dp-{{ workshop_prefix }}-igw"
-    name: "dp-{{ workshop_prefix }}-rhel-sg"
-      rule_desc: "allow all on port 443 (SSH)"
-      rhel: "dp-{{ workshop_prefix }}"
-    name: "dp-{{ workshop_prefix }}-win-sg"
-      win: "dp-{{ workshop_prefix }}"
-      Name: "dp-{{ workshop_prefix }}-subnet"
-      Name: "dp-{{ workshop_prefix }}-route"
-    key_name: "dp-{{ workshop_prefix }}-key"
-    group: "dp-{{ workshop_prefix }}-rhel-sg"
-    instance_type: "{{ nodes.rhel.instance_type }}"
-    image: "{{ nodes.rhel.ami_id }}"
-      Name: "dp-{{ workshop_prefix }}-rhel"
-      rhel: "dp-{{ workshop_prefix }}"
-    key_name: "dp-{{ workshop_prefix }}-key"
-    group: "dp-{{ workshop_prefix }}-rhel-sg"
-    instance_type: "{{ nodes.tower_rhel.instance_type }}"
-    image: "{{ nodes.tower_rhel.ami_id }}"
-      Name: "dp-{{ workshop_prefix }}-tower_rhel"
-      tower_rhel: "dp-{{ workshop_prefix }}"
-    key_name: "dp-{{ workshop_prefix }}-key"
-    group: "dp-{{ workshop_prefix }}-win-sg"
-    instance_type: "{{ nodes.win.instance_type }}"
-    image: "{{ nodes.win.ami_id }}"
-      Name: "dp-{{ workshop_prefix }}-win"
-      win: "dp-{{ workshop_prefix }}"
-    name: dp-{{ workshop_prefix}}-rhel-{{ item.id }}
-    record: "rhel7.{{ item.ami_launch_index }}.{{ workshop_prefix }}.{{ domain_name }}"
-    record: "tower.{{ item.ami_launch_index }}.{{ workshop_prefix }}.{{ domain_name }}"
-    name: dp-{{ workshop_prefix}}-win-{{ item.id }}
-    record: "win2016.{{ item.ami_launch_index }}.{{ workshop_prefix }}.{{ domain_name }}"
BREAKS HERE
-    georep: action=stop
BREAKS HERE
-      inventory_hostname == groups['ironic_conductor'][0]
-  - include: ironic_conductor_post_install.yml
-      inventory_hostname == groups['ironic_conductor'][0]
BREAKS HERE
-    - playbooks/test-vars.yml
BREAKS HERE
-## Nova virtualization Type Autodetect
-# Set to True if nova_virt_type is not defined, then it will auto
-# choose kvm or qemu for nova_virt_type according to /proc/cpuinfo
-nova_virt_autodetect: True
-
-# Once nova_virt_type is defined, then nova_virt_autodetect won't work any more
-nova_virt_type: kvm
BREAKS HERE
-  user: root
BREAKS HERE
-  when: backup_ip != None
BREAKS HERE
-      command: echo "{{ host_group_vars_host_molecule_yml }}"
-      changed_when: false
-      command: echo "{{ host_group_vars_host_vars_dir }}"
-      changed_when: false
-      command: |-
-        echo "{{ host_group_vars_example_group_one_molecule_yml }} {{ host_group_vars_example_group_two_molecule_yml }}"
-      changed_when: false
-      command: echo "{{ host_group_vars_group_vars_dir }}"
-      changed_when: false
-      command: echo "{{ host_group_vars_example_1_child_group_molecule_yml }}"
-      changed_when: false
-      command: echo "{{ hostvars['extra_host']['host_group_vars_extra_host_molecule_yml'] }}"
-      changed_when: false
-      command: /bin/true
-      changed_when: false
-      command: /bin/true
-      changed_when: false
BREAKS HERE
-    command: "command -v ceph-volume"
BREAKS HERE
-  hosts: all
-      connection: local
BREAKS HERE
-      - { role: carlosbuenosvinos.ansistrano-deploy }
BREAKS HERE
-          options: 'rw,no_root_squash,sync'
BREAKS HERE
-        when: image_files is defined and install.images.cleanup == 'yes'
BREAKS HERE
-    - name: Create ciruclar symbolic link
-        path: subdir/subdir1/circles
BREAKS HERE
-    path: "{{ lxc_container_cache_path }}/{{ lxc_cache_map.distro }}/{{ lxc_cache_map.release }}/{{ lxc_cache_map.arch }}/default/rootfs.tar.xz"
BREAKS HERE
-- name: purge ceph nfs cluster
-
-  hosts: "{{ nfs_group_name|default('nfss') }}"
-
-  become: true
-
-  tasks:
-
-  - name: disable ceph nfs service
-    service:
-      name: "ceph-nfs@{{ ansible_hostname }}"
-      state: stopped
-      enabled: no
-    ignore_errors: true
-
-  - name: remove ceph nfs container
-    docker_container:
-      image: "{{ ceph_docker_registry }}/{{ ceph_docker_image }}:{{ ceph_docker_image_tag }}"
-      name: "ceph-nfs-{{ ansible_hostname }}"
-      state: absent
-    ignore_errors: true
-
-  - name: remove ceph nfs service
-    file:
-      path: /etc/systemd/system/ceph-nfs@.service
-      state: absent
-
-  - name: remove ceph nfs directories for "{{ ansible_hostname }}"
-    file:
-      path: "{{ item }}"
-      state: absent
-    with_items:
-      - /etc/ganesha
-      - /var/lib/nfs/ganesha
-      - /var/run/ganesha
-
-  - name: remove ceph nfs image
-    docker_image:
-      state: absent
-      repository: "{{ ceph_docker_registry }}"
-      name: "{{ ceph_docker_image }}"
-      tag: "{{ ceph_docker_image_tag }}"
-      force: yes
-    tags: remove_img
-
-
BREAKS HERE
-  vars:
-    teardown: false
-  tags: control_node
-  roles:
-    - { role: tower_request, when: student_total > 9 }
BREAKS HERE
-  serial: 1
BREAKS HERE
-    - geerlingguy.pip
-    - geerlingguy.nodejs
BREAKS HERE
-      when: ovirt_auth is undefined
BREAKS HERE
-#    script: {{ scripts }}/needs-updates --host {{ ansible_fqdn  }}
-    script: needs-updates --host {{ ansible_fqdn }}
-    local_action: shell ls -d -1 {{datadir_prfx_path}}/{{ansible_fqdn}}-* 2>/dev/null | sort -r | head -1
-    local_action: file path=/{{datadir_prfx_path}}/{{ansible_fqdn}}-{{timestamp.stdout}} state=directory
-    fetch: src={{item}} dest=/{{datadir_prfx_path}}/{{ansible_fqdn}}-{{timestamp.stdout}}/ flat=true
-    local_action: shell for file in {{datadir_prfx_path}}/{{ansible_fqdn}}-{{timestamp.stdout}}/*; do filename=$(basename $file); diff {{datadir_prfx_path}}/{{ansible_fqdn}}-{{timestamp.stdout}}/$filename {{last_dir.stdout.strip(':')}}/$filename; done
BREAKS HERE
-  when: not "true" in nodns
-    when: not "true" in nonagios
-    when: not "true" in nonagios
BREAKS HERE
-        - remove_baseline
BREAKS HERE
-  when: not neutron_get_venv | success or neutron_developer_mode | bool
-    - neutron_get_venv | changed or neutron_venv_dir | changed
-    - neutron_get_venv | changed or neutron_venv_dir | changed
BREAKS HERE
-       - name: Install pre-defined Undercloud packages
-         #(skatlapa): Check if the below are needed against any_errors_fatal
-         #register: yum_result
-         #failed_when: "'Nothing to do' in yum_result.stderr"
BREAKS HERE
-    - name: Gather facts about all SAS Logical JBOD Attachment
-
-
-
BREAKS HERE
-                {%- elif 'hypervisor' in groups.all -%}
BREAKS HERE
-        
BREAKS HERE
-      zonecert: pagure_push,
BREAKS HERE
-  command: "true"
-  changed_when: no
-  when: rhel_07_030300
-      - notimplemented
-      - id: "030740"
-        path: "/usr/bin/mount"
-        key: "privileged-mount"
BREAKS HERE
-    file:
-      path: "{{user_ssh_dir}}/known_hosts"
-
BREAKS HERE
-        - osrelease | version_compare('3.9', '>=')
-        - osrelease | version_compare('3.9', '>=')
BREAKS HERE
-    mailman_mm_db_pass: "{{ mailman_mm_db_pass }}"
-    mailman_hk_admin_db_pass: "{{ mailman_hk_admin_db_pass }}"
-    mailman_hk_db_pass: "{{ mailman_hk_db_pass }}"
-    mailman_ks_admin_db_pass: "{{ mailman_ks_admin_db_pass }}"
-    mailman_ks_db_pass: "{{ mailman_ks_db_pass }}"
BREAKS HERE
-      with_items: vms.list_vms
-      with_items: vms.list_vms
-        user_id=`cat /etc/passwd | grep stack | awk -F ':' '{ print $3 }'`;
-        rm -Rf /run/user/$user_id/libvirt
-      with_flattened:
-        - director_rpms.stdout_lines
-        - rhos_rpms.stdout_lines
-        - delorean_rpms.stdout_lines
BREAKS HERE
-      shell: systemctl list-units | grep -E "loaded * active" | grep -oE "ceph-osd@([0-9]{1,}|[a-z]+).service"
BREAKS HERE
-  role:
BREAKS HERE
-          state: asbsent
-      tags: ['never', 'logspout', 'logger', 'monitoring', 'docker', 'pservice', 'docker-container', 'full_setup']
BREAKS HERE
-  when: not swift_force_change_hashes | bool
-    - swift_do_setup | bool
-    - inventory_hostname in groups['swift_all']
-    - swift_do_setup | bool
-    - inventory_hostname in groups['swift_all']
-    - swift_do_setup | bool
-    - inventory_hostname in groups['swift_all']
-    - inventory_hostname in groups['swift_hosts']
-    - inventory_hostname in groups['swift_hosts']
-    - swift_do_setup | bool
-    - inventory_hostname in groups['swift_proxy']
-    - swift_do_setup | bool
-- include: swift_init_common.yml
-    - inventory_hostname == groups['swift_all'][0]
-    - swift_do_setup | bool
-    - swift_do_sync | bool
-    - need_sync is defined
-    - swift_do_sync | bool
-    - need_sync is defined
-
-- include: swift_sync_post_install.yml
-  static: no
-  when:
-   - swift_do_sync | bool
-   - not swift_do_setup | bool
-  tags:
-    - swift-config
BREAKS HERE
-      when:
-        - ansible_pkg_mgr == 'apt'
-      with_items: "{{ utility_apt_packages | default([]) }}"
-      when:
-        - ansible_pkg_mgr == 'apt'
-      with_items: "{{ utility_yum_packages | default([]) }}"
-      when:
-        - ansible_pkg_mgr == 'yum'
-      when:
-        - utility_ssh_private_key is defined
-    utility_apt_packages:
-      - git
-    utility_yum_packages:
-      - git
BREAKS HERE
-      when: "'{{ cluster }}' == 'openshift'"
-      when: "'{{ cluster }}' == 'kubernetes'"
BREAKS HERE
-- name: Cleanup nodes from nova
-  hosts: local
-    - { role: cleanup_nodes, nodes: '{{ cleanup_nodes }}' }
BREAKS HERE
-  
-    - "./{{ env_type }}_vars.yml"
-    - "./{{ env_type }}_secret_vars.yml"
-
BREAKS HERE
-          version: 8.0.192-zulu
BREAKS HERE
--  yum: name=apache2 state=installed
--  service: name=httpd state=started enabled=yes
BREAKS HERE
-        register: result
-        failed_when: "result.rc != 0 and result.stderr.find('Flavor with name baremetal already exists') != -1"
-            openstack flavor set --property 'cpu_arch'='x86_64' --property 'capabilities:boot_option'='local' baremetal
-      - name: read instackenv file for bm deployment
-              failed_when: "result.rc != 0 and result.stderr.find('Flavor with name {{ item.name }} already exists') != -1"
BREAKS HERE
-  ansible_ssh_user: cloud-user
-  become: true
-  - name: Copy etc hosts file to all hosts
BREAKS HERE
-  hosts: testhost
-       #  tags:
-       #   - any_errors_fatal_includes
BREAKS HERE
-nova_program_name: nova-api-os-compute
-nova_spice_program_name: nova-spicehtml5proxy
-nova_novncproxy_program_name: nova-novncproxy
-nova_metadata_program_name: nova-api-metadata
-
-## Nova cert
-nova_cert_program_name: nova-cert
-nova_compute_program_name: nova-compute
-## Nova conductor
-nova_conductor_program_name: nova-conductor
-
-nova_consoleauth_program_name: nova-consoleauth
-nova_scheduler_program_name: nova-scheduler
-## Service Names
-nova_service_names:
-  - "{{ nova_metadata_program_name }}"
-  - "{{ nova_cert_program_name }}"
-  - "{{ nova_conductor_program_name }}"
-  - "{{ nova_program_name }}"
-  - "{{ nova_scheduler_program_name }}"
-  - "{{ nova_compute_program_name }}"
-  - "{{ nova_spice_program_name }}"
-  - "{{ nova_consoleauth_program_name }}"
-  - "{{ nova_novncproxy_program_name }}"
BREAKS HERE
-- name: output
-  local_action: debug msg="Looking for host at ${inst_rs.instances[0].public_ip} internal ${inst_rs.instances[0].private_ip}"
-
BREAKS HERE
-- name: dump puppet apply logs into /var/log for collection
BREAKS HERE
-      http_port: {{ repoSpanner_rpms_http }},
-      http_port: {{ repoSpanner_rpms_http }},
-      http_port: {{ repoSpanner_rpms_http }},
BREAKS HERE
-- name: copy run-in-venv.sh script
-  shell: cp /mnt/data/run-in-venv.sh /usr/local/bin
-  command: "run-in-venv.sh python3 manage.py migrate"
-  command: "run-in-venv.sh python3 manage.py collectstatic --noinput"
-# otherwise Apache cannot execute psyco shared lib in virtualenv with httpd_sys_content_t type
BREAKS HERE
-    net: host
BREAKS HERE
-          - 'ansible_mounts|default("UNDEF_HW") != "UNDEF_HW"'
BREAKS HERE
-- name: update the system
-  gather_facts: True
BREAKS HERE
-          memory: "100m"
BREAKS HERE
-    action: copy src="{{ files }}/download/cron-hourly-sync.sh"  dest=/etc/cron.hourly/sync-mirror.sh owner=root group=root mode=755
BREAKS HERE
-      - name: check if host supports virtualization
-        include: tasks/validate.yml
-
-      - name: setup the hypervisor
-        include: tasks/setup.yml
BREAKS HERE
-     with_items: filelist
-     with_items: filelist
BREAKS HERE
-  shell: az network public-ip show -n bastionExternalIP -g jhorn-test --query dnsSettings.fqdn
BREAKS HERE
-    - name: Run test script to confirm Memcached is reachable via PHP.
BREAKS HERE
-    - name: Get current boot time
-      command: who -b
-      register: before_boot_time
-    - name: Get current boot time
-      command: who -b
-      register: after_boot_time
-    - name: Ensure system was actually rebooted
-      assert:
-        that:
-          - reboot_result is changed
-          - reboot_result.elapsed > 10
-          - before_boot_time.stdout != after_boot_time.stdout
-      ignore_errors: yes
BREAKS HERE
-    - "/home/{{ odoo_user }}/odoo/server/{{ (odoo_version | int) < 9 and 'openerp' or 'odoo' }}/addons"
BREAKS HERE
-  when: httpd_setup_enable
-      when: nexus_blob_split
BREAKS HERE
-               path: "{{ working_dir }}/hiera_override.yaml"
-               line: "{{ install.hieradata.config.split('=')[0] }}: {{ install.hieradata.config.split('=')[1] }}"
-            when: install.hieradata.config != ''
BREAKS HERE
-      raw: apt-get -y install python
-      when:
-        - ansible_pkg_mgr == 'apt'
BREAKS HERE
-  - fedmsg/base
BREAKS HERE
-security_audit_apparmor_changes: yes              # V-38541
BREAKS HERE
-    - name: Remove default travis databases
-      package:
-        name:
-          - postgresql*
-          # - mysql*
-        state: absent
-
-    - name: pre-install the db backend
-      package:
-        name: "{{ db_packages_name[nextcloud_db_backend] }}"
-        state: present
-      when: nextcloud_db_backend == 'pgsql'
-    - name: pre-activate the db backend
-      service:
-        name: "{{ db_packages_name[nextcloud_db_backend] }}"
-        state: started
-    db_packages_name:
-      pgsql: postgresql
-      mariadb: mariadb-server
-      mysql: mysql-server
-     
BREAKS HERE
-      name: 'keygen-persistent'
-      name: 'fedmsg-relay-persistent'
BREAKS HERE
-    - {id: "030640", path: "/usr/bin/unix_chkpwd", extra_fields: "-F perm=x", key: "privileged-passwd", create: "{{ rhel_07_030640 }}"}
BREAKS HERE
-        - "user.info: SSH password: {{ student_password | d(hostvars[bastions[0]].student_password) }}"
-        - student_password is defined or hostvars[bastions[0]].student_password is defined
BREAKS HERE
-
-    test_java_version_update: 121
-    test_java_version_build: 13
BREAKS HERE
-  include_vars: "vars/topology/nodes/{{ node.key }}.yml"
BREAKS HERE
-  hosts: bugzilla2fedmsg-stg
-  hosts: bugzilla2fedmsg-stg
BREAKS HERE
-- name: Ensure the imap server FQDN resolves to localhost
-  lineinfile:
-    path: /etc/hosts
-    line: '127.0.0.1    {{ network.imap }}'
-    
-# - name: Create indexes and control directory
-#   file:
-#     path: "/var/vmail/{{ dir }}"
-#     state: directory
-#     owner: dovecot
-#     group: users
-#     mode: 0775
-#   with_items:
-#     - indexes
-#     - control
-#   loop_control:
-#     loop_var: dir
-    
-# - name: Restart dovecot
-#   when: config.changed
-#   service:
-#     name: dovecot
-#     state: restarted
BREAKS HERE
-      yum: pkg=libselinux-python state=installed
BREAKS HERE
-- name: Workaround for BZ#1543914 - set SELinux to permissive for OSP 13
BREAKS HERE
-- name: "MEDIUM | RHEL-07-040500 | PATCH | The operating system must, for networked systems, synchronize clocks with a server that is synchronized to one of the redundant United States Naval Observatory (USNO) time servers, a time server designated for the appropriate DoD network (NIPRNet/SIPRNet), and/or the Global Positioning System (GPS)."
-  lineinfile:
-      create: yes
-      dest: "{{ rhel7stig_time_service_configs[rhel7stig_time_service].conf }}"
-      regexp: "{{ item.regexp }}"
-      line: "{{ item.line }}"
-  notify: restart {{ rhel7stig_time_service }}
-  with_items: "{{ rhel7stig_time_service_configs[rhel7stig_time_service].lines }}"
-- name: "MEDIUM | RHEL-07-040500 | PATCH | The operating system must, for networked systems, synchronize clocks with a server that is synchronized to one of the redundant United States Naval Observatory (USNO) time servers, a time server designated for the appropriate DoD network (NIPRNet/SIPRNet), and/or the Global Positioning System (GPS)."
-  replace:
-      dest: "{{ rhel7stig_time_service_configs[rhel7stig_time_service].conf }}"
-      regexp: '^server \S+( \w+)?$'
-  notify: restart {{ rhel7stig_time_service }}
BREAKS HERE
-    - role: "{{ rolename | basename }}"
-          - "'Pin: release o=TestRelease' in preference_contents"
BREAKS HERE
-    - { role: "{{ ANSIBLE_REPO_PATH }}/roles/bastion", when: 'install_bastion' }
BREAKS HERE
-      haproxy_state: disabled
-      haproxy_state: enabled
BREAKS HERE
-    unattended_upgrades_enabled: true
-    unattended_upgrades_notifications: true
BREAKS HERE
-              startDate: '2016-07-01T14:29:42.000Z'
-              endDate: '2018-07-01T03:29:42.000Z'
BREAKS HERE
-      shell: "aws route53 list-hosted-zones-by-name --region={{aws_region}} --dns-name={{env_type}}.{{guid}}.internal. --output text --query='HostedZones[*].Id' | awk -F'/' '{print $3}'"
BREAKS HERE
-    - name: Set docker storage disk path
-        container_runtime_docker_storage_setup_device: "/dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_2"
BREAKS HERE
-   - { role: taskotron/testdays, tags: ['testdays'] }
BREAKS HERE
-  hosts: bugzilla2fedmsg;bugzilla2fedmsg-stg
-  hosts: bugzilla2fedmsg;bugzilla2fedmsg-stg
-#  - rkhunter
-  - denyhosts
-  #- collectd/base
-  #- fedmsg/base
BREAKS HERE
-      command: umount "{{ item.0 }}"1
-        - "{{ devices }}"
BREAKS HERE
-- name: add tor repository
-  apt_repository:
-    repo: "deb http://deb.torproject.org/torproject.org {{ ansible_distribution_release }} main"
-
-- name: download and import tor signing key
-    keyserver: keys.gnupg.net
-    id: A3C4F0F979CAA22CDBA8F512EE8CBC9E886DDD89
-- name: install tor package
-    state: present
-    update_cache: yes
-  register: tor_is_installed
-  with_dict: "{{ hidden_services }}"
-  with_dict: "{{ hidden_services }}"
-  with_dict:  "{{ hidden_services }}"
-  with_dict: "{{ hidden_services }}"
BREAKS HERE
-  # - packages::proxy::bugz
BREAKS HERE
-  hosts: keystone_all
BREAKS HERE
-  hosts: "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_bastion') | replace('-', '_') }}"
BREAKS HERE
-            name: http_debug
-            image: "{{images.http_debug}}"
-        _state: "{{ (http_debug_bind|default('') == 'absent')|ternary('absent', 'started') }}"
-        _bind_default: "{{if_inner}}:{{ports.http_debug.0}}:{{ports.http_debug.1}}"
-        _bind_dirty: "{{http_debug_bind|default(_bind_default)}}"
-      tags: ['never', 'http_debug']
BREAKS HERE
-    - include: ensure-rabbitmq.yml
-      vhost_name: "{{ neutron_rabbitmq_vhost }}"
-      user_name: "{{ neutron_rabbitmq_userid }}"
-      user_password: "{{ neutron_rabbitmq_password }}"
-      when:
-        - "'rabbitmq_all' in groups"
-        - "groups['rabbitmq_all'] | length > 0"
-        - "'oslomsg_rpc_all' not in groups"
-
-    - include: ensure-oslomsg.yml
-      rpc_vhost: "{{ neutron_oslomsg_rpc_vhost }}"
-      rpc_user: "{{ neutron_oslomsg_rpc_userid }}"
-      rpc_password: "{{ neutron_oslomsg_rpc_password }}"
-      notify_vhost: "{{ neutron_oslomsg_notify_vhost }}"
-      notify_user: "{{ neutron_oslomsg_notify_userid }}"
-      notify_password: "{{ neutron_oslomsg_notify_password }}"
-      when:
-        - "'oslomsg_rpc_all' in groups"
-        - "groups['oslomsg_rpc_all'] | length > 0"
-
-  roles:
-    - role: "os_neutron"
BREAKS HERE
-    localpath: /nagios
-    remotepath: /nagios/
BREAKS HERE
-- hosts: localhost
-  gather_facts: true
-  tasks:
-
-  - name: Install Common Packages (Takes Awhile)
BREAKS HERE
-          global_configuration: "{{ global_configuration }}"
BREAKS HERE
-      http_port: {{repoSpanner_ansible_http}},
BREAKS HERE
-- import_tasks: mq_setup.yml
-  when:
-    - "ironic_services['ironic-api']['group'] in group_names"
-    - "inventory_hostname == ((groups[ironic_services['ironic-api']['group']]| intersect(ansible_play_hosts)) | list)[0]"
-  vars:
-    _oslomsg_rpc_setup_host: "{{ ironic_oslomsg_rpc_setup_host }}"
-    _oslomsg_rpc_userid: "{{ ironic_oslomsg_rpc_userid }}"
-    _oslomsg_rpc_password: "{{ ironic_oslomsg_rpc_password }}"
-    _oslomsg_rpc_vhost: "{{ ironic_oslomsg_rpc_vhost }}"
-    _oslomsg_rpc_transport: "{{ ironic_oslomsg_rpc_transport }}"
-    _oslomsg_notify_setup_host: "{{ ironic_oslomsg_notify_setup_host }}"
-    _oslomsg_notify_userid: "{{ ironic_oslomsg_notify_userid }}"
-    _oslomsg_notify_password: "{{ ironic_oslomsg_notify_password }}"
-    _oslomsg_notify_vhost: "{{ ironic_oslomsg_notify_vhost }}"
-    _oslomsg_notify_transport: "{{ ironic_oslomsg_notify_transport }}"
-  tags:
-    - common-mq
-    - ironic-config
-
BREAKS HERE
-    - role: solr
BREAKS HERE
-nova_cpu_mode: host-model
BREAKS HERE
-        - osrelease is version('3.9', '<')
-        - osrelease is version('3.9', '>=')
-        - osrelease is version('3.9', '<')
-        - osrelease is version('3.9', '<')
-        - osrelease is version('3.9', '<')
-        - osrelease is version('3.9', '<')
-        - osrelease is version('3.9', '>=')
-        - osrelease is version('3.9', '>=')
-        - osrelease is version('3.9', '>=')
-        - osrelease is version('3.9', '>=')
-        - osrelease is version('3.9', '>=')
-        - osrelease is version('3.9', '>=')
-        - osrelease is version('3.9', '>=')
-        - osrelease is version('3.9', '>=')
BREAKS HERE
-- hosts: localhost
-        definition: "{{ lookup('template', '../files/templates/MetalLB/metallb_config.yml.j2') | from_yaml_all | list }}"
BREAKS HERE
-      rsyslog_client_log_dir: "/var/log/mysql"
BREAKS HERE
-      - networker
-        command: yum localinstall -y {{ product.rpmrepo[ansible_distribution] }}/rhos-release-latest.noarch.rpm
BREAKS HERE
-
BREAKS HERE
-            selinux_problems_list: []
-            selinux_problems_list: "{{ selinux_problems_list + (hostvars[item]['selinux_problems_found'] | default([])) }}"
-            selinux_problems: "{{ selinux_problems_list | length }}"
BREAKS HERE
-- name: Check if BZ1210411 packstack/floating_ip_pool is enabled
-  hosts: controller
-  gather_facts: no
-  sudo: no
-  tasks:
-      - group_by: key=workaround_bz1210411
-        when: workarounds.bz1210411 is defined
-
-- name: "Workaround BZ1210411: Update floating_ip_pool in nova.conf"
-  hosts: workaround_bz1210411
-  gather_facts: yes
-  sudo: yes
-  tasks:
-    - name: Get packstack's floating ip pool value
-      shell: crudini --get packstack_config.txt general CONFIG_NOVA_NETWORK_DEFAULTFLOATINGPOOL
-      args:
-        chdir: /root
-      register: packstack_floating_ip_pool
-
-    - name: Apply packstack's floating ip pool value to nova.conf
-      shell: crudini --set /etc/nova/nova.conf DEFAULT default_floating_pool {{ packstack_floating_ip_pool.stdout }}
-
-    - name: Restart nova services to apply changes in nova.conf
-      shell: openstack-service restart nova
-
-    - name: Restart neutron service to apply changes in floating ip pool
-      shell: openstack-service restart neutron
-
BREAKS HERE
-  copy: src="access_log.conf" dest=/etc/lighttpd/conf.d/access_log.conf owner=root group=root mode=0644
BREAKS HERE
-## Swift Auth
-swift_service_admin_tenant_name: "service"
-swift_service_admin_username: "swift"
-
BREAKS HERE
-
BREAKS HERE
-  command: "{{ docker_exec_client_cmd }} --cluster {{ cluster }} osd pool create {{ item.name }} {{ item.pgs }}"
-    - ceph_conf_overrides.get('global', {}).get('osd_pool_default_pg_num', False) != False
BREAKS HERE
-
BREAKS HERE
-    restapi_group_name:   restapis
-    - include_vars: roles/ceph-common/defaults/main.yml
-    - include_vars: roles/ceph-mon/defaults/main.yml
-    - include_vars: roles/ceph-restapi/defaults/main.yml
-    - include_vars: group_vars/all
-      failed_when: false
-    - include_vars: group_vars/{{ mon_group_name }}
-      failed_when: false
-    - include_vars: group_vars/{{ restapi_group_name }}
-      failed_when: false
-
-    - ceph-common
-    - include_vars: roles/ceph-common/defaults/main.yml
-    - include_vars: roles/ceph-mon/defaults/main.yml
-    - include_vars: roles/ceph-restapi/defaults/main.yml
-    - include_vars: group_vars/all
-      failed_when: false
-    - include_vars: group_vars/{{ mon_group_name }}
-      failed_when: false
-    - include_vars: group_vars/{{ restapi_group_name }}
-      failed_when: false
-
-    - name: select a running monitor
-      when: item != inventory_hostname
-    - include_vars: roles/ceph-common/defaults/main.yml
-    - include_vars: roles/ceph-osd/defaults/main.yml
-    - include_vars: group_vars/all
-      failed_when: false
-    - include_vars: group_vars/{{ osd_group_name }}
-      failed_when: false
-
-    - name: stop ceph osds (upstart)
-    - name: stop ceph osds (sysvinit)
-    - name: stop ceph osds (systemd)
-      when: is_systemd
-    - ceph-common
-    - include_vars: roles/ceph-common/defaults/main.yml
-    - include_vars: roles/ceph-osd/defaults/main.yml
-    - include_vars: group_vars/all
-      failed_when: false
-    - include_vars: group_vars/{{ osd_group_name }}
-      failed_when: false
-
-    - name: start ceph osds (upstart)
-    - name: start ceph osds (sysvinit)
-    - name: start ceph osds (systemd)
-      when: is_systemd
-    - include_vars: roles/ceph-common/defaults/main.yml
-    - include_vars: roles/ceph-mds/defaults/main.yml
-    - include_vars: group_vars/all
-      failed_when: false
-    - include_vars: group_vars/{{ mds_group_name }}
-      failed_when: false
-
-    - ceph-common
-    - include_vars: roles/ceph-common/defaults/main.yml
-    - include_vars: roles/ceph-mds/defaults/main.yml
-    - include_vars: group_vars/all
-      failed_when: false
-    - include_vars: group_vars/{{ mds_group_name }}
-      failed_when: false
-
-    - include_vars: roles/ceph-common/defaults/main.yml
-    - include_vars: roles/ceph-rgw/defaults/main.yml
-    - include_vars: group_vars/all
-      failed_when: false
-    - include_vars: group_vars/{{ rgw_group_name }}
-      failed_when: false
-
-    - name: stop ceph rgws with systemd
-        name: ceph-radosgw@rgw.{{ ansible_hostname }}
-        enabled: yes
-      when: is_systemd
-    - name: stop ceph rgws with upstart
-        name: ceph-radosgw
-      when: is_upstart.stat.exists == True
-    - ceph-common
-    - include_vars: roles/ceph-common/defaults/main.yml
-    - include_vars: roles/ceph-rgw/defaults/main.yml
-    - include_vars: group_vars/all
-      failed_when: false
-    - include_vars: group_vars/{{ rgw_group_name }}
-      failed_when: false
-
-    - name: start ceph rgws with systemd
-        name: ceph-radosgw@rgw.{{ ansible_hostname }}
-        enabled: yes
-      when: is_systemd
-    - name: start ceph rgws with upstart
-        name: ceph-radosgw
-      when: is_upstart.stat.exists == True
BREAKS HERE
-    - keystone_get_venv | changed or keystone_venv_dir | changed or install_packages | changed
BREAKS HERE
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      vars:
-        user_name: "{{ gnocchi_galera_user }}"
-        password: "{{ gnocchi_container_mysql_password }}"
-        login_host: "{{ gnocchi_galera_address }}"
-        db_name: "{{ gnocchi_galera_database }}"
-      when: inventory_hostname == groups['gnocchi_all'][0]
-
-  vars:
-    gnocchi_galera_user: gnocchi
-    gnocchi_galera_database: gnocchi
-    gnocchi_galera_address: "{{ galera_address }}"
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - gnocchi
BREAKS HERE
-   - { role: openqa_worker, tags: ['openqa_worker'] }
BREAKS HERE
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      vars:
-        user_name: "{{ trove_galera_user }}"
-        password: "{{ trove_galera_password }}"
-        login_host: "{{ trove_galera_address }}"
-        db_name: "{{ trove_galera_database_name }}"
-      when: inventory_hostname == groups['trove_all'][0]
-
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - trove
BREAKS HERE
-    #roles:
-      #- azure_infra
-- name: install Grafana
-  import_playbook: /usr/share/ansible/openshift-ansible/playbooks/openshift-grafana/config.yml
-        dest: "{{ local_home }}/.kube/"
BREAKS HERE
-    mysql_hardening_enabled: yes
BREAKS HERE
-        that: "'keystone-{{ repo_build_release_tag }}.tgz' in venv_folder_content.stdout"
-        that: "'tempest-{{ repo_build_release_tag }}.tgz' in venv_folder_content.stdout"
-        that: "'nova-{{ repo_build_release_tag }}.tgz' not in venv_folder_content.stdout"
BREAKS HERE
-        openshift_ansible_version: "openshift-ansible-3.7.32-1",
BREAKS HERE
-    update_cache: yes
-    cache_valid_time: 600
BREAKS HERE
-                    key="{{ lookup('file', '{{ private }}/files/releng/sshkeys/primary-s390x-sshfs' + '-staging.pub' if env == 'staging' else '.pub') }}"
BREAKS HERE
-# test code for the copy module and action plugin
-# (c) 2014, Michael DeHaan <michael.dehaan@gmail.com>
-# (c) 2017, Ansible Project
-#
-# GNU General Public License v3 or later (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt )
-#
-
-- set_fact:
-    output_dir_expanded: '{{ output_dir | expanduser }}'
-
-- name: record the output directory
-  set_fact: output_file={{output_dir}}/foo.txt
-
-- name: locate sha1sum/shasum
-  shell: which sha1sum || which shasum
-  register: sha1sum
-
-- name: initiate a basic copy, and also test the mode
-  copy: src=foo.txt dest={{output_file}} mode=0444
-  register: copy_result
-
-- name: check the mode of the output file
-  file: name={{output_file}} state=file
-  register: file_result_check
-
-- name: assert the mode is correct
-  assert:
-    that:
-      - "file_result_check.mode == '0444'"
-
-#- debug:
-#    var: copy_result
-
-- name: assert basic copy worked
-  assert:
-    that:
-      - "'changed' in copy_result"
-      - "'dest' in copy_result"
-      - "'group' in copy_result"
-      - "'gid' in copy_result"
-      - "'md5sum' in copy_result"
-      - "'checksum' in copy_result"
-      - "'owner' in copy_result"
-      - "'size' in copy_result"
-      - "'src' in copy_result"
-      - "'state' in copy_result"
-      - "'uid' in copy_result"
-
-- name: verify that the file was marked as changed
-  assert:
-    that:
-      - "copy_result.changed == true"
-
-- name: verify that the file checksums are correct
-  assert:
-    that:
-      - "copy_result.checksum == 'c79a6506c1c948be0d456ab5104d5e753ab2f3e6'"
-
-- name: verify that the legacy md5sum is correct
-  assert:
-    that:
-      - "copy_result.md5sum == 'c47397529fe81ab62ba3f85e9f4c71f2'"
-  when: ansible_fips|bool != True
-
-- name: check the stat results of the file
-  stat: path={{output_file}}
-  register: stat_results
-
-#- debug: var=stat_results
-
-- name: assert the stat results are correct
-  assert:
-    that:
-      - "stat_results.stat.exists == true"
-      - "stat_results.stat.isblk == false"
-      - "stat_results.stat.isfifo == false"
-      - "stat_results.stat.isreg == true"
-      - "stat_results.stat.issock == false"
-      - "stat_results.stat.checksum == 'c79a6506c1c948be0d456ab5104d5e753ab2f3e6'"
-
-- name: verify that the legacy md5sum is correct
-  assert:
-    that:
-      - "stat_results.stat.md5 == 'c47397529fe81ab62ba3f85e9f4c71f2'"
-  when: ansible_fips|bool != True
-
-- name: overwrite the file via same means
-  copy: src=foo.txt dest={{output_file}}
-  register: copy_result2
-
-- name: assert that the file was not changed
-  assert:
-    that:
-      - "not copy_result2|changed"
-
-- name: overwrite the file using the content system
-  copy: content="modified" dest={{output_file}}
-  register: copy_result3
-
-- name: check the stat results of the file
-  stat: path={{output_file}}
-  register: stat_results
-
-#- debug: var=stat_results
-
-- name: assert that the file has changed
-  assert:
-     that:
-       - "copy_result3|changed"
-       - "'content' not in copy_result3"
-       - "stat_results.stat.checksum == '99db324742823c55d975b605e1fc22f4253a9b7d'"
-       - "stat_results.stat.mode != '0700'"
-
-- name: overwrite the file again using the content system, also passing along file params
-  copy: content="modified" dest={{output_file}} mode=0700
-  register: copy_result4
-
-- name: check the stat results of the file
-  stat: path={{output_file}}
-  register: stat_results
-
-#- debug: var=stat_results
-
-- name: assert that the file has changed
-  assert:
-     that:
-       - "copy_result3|changed"
-       - "'content' not in copy_result3"
-       - "stat_results.stat.checksum == '99db324742823c55d975b605e1fc22f4253a9b7d'"
-       - "stat_results.stat.mode == '0700'"
-
-- name: try invalid copy input location fails
-  copy: src=invalid_file_location_does_not_exist dest={{output_dir}}/file.txt
-  ignore_errors: True
-  register: failed_copy
-
-- name: assert that invalid source failed
-  assert:
-    that:
-      - "failed_copy.failed"
-      - "'invalid_file_location_does_not_exist' in failed_copy.msg"
-
-- name: Clean up
-  file:
-    path: "{{ output_file }}"
-    state: absent
-
-- name: Copy source file to destination directory with mode
-  copy:
-    src: foo.txt
-    dest: "{{ output_dir }}"
-    mode: 0500
-  register: copy_results
-
-- name: check the stat results of the file
-  stat:
-    path: '{{ output_file }}'
-  register: stat_results
-
-#- debug: var=stat_results
-
-- name: assert that the file has changed
-  assert:
-     that:
-       - "copy_results|changed"
-       - "stat_results.stat.checksum == 'c79a6506c1c948be0d456ab5104d5e753ab2f3e6'"
-       - "stat_results.stat.mode == '0500'"
-
-# Test copy with mode=preserve
-- name: Set file perms to an odd value
-  file:
-    path: '{{ output_file }}'
-    mode: 0547
-
-- name: Copy with mode=preserve
-  copy:
-    src: '{{ output_file }}'
-    dest: '{{ output_dir }}/copy-foo.txt'
-    mode: preserve
-  register: copy_results
-
-- name: check the stat results of the file
-  stat:
-    path: '{{ output_dir }}/copy-foo.txt'
-  register: stat_results
-
-- name: assert that the file has changed and has correct mode
-  assert:
-     that:
-       - "copy_results|changed"
-       - "copy_results.mode == '0547'"
-       - "stat_results.stat.checksum == 'c79a6506c1c948be0d456ab5104d5e753ab2f3e6'"
-       - "stat_results.stat.mode == '0547'"
-
-#
-# test recursive copy local_follow=False, no trailing slash
-#
-
-- name: Create empty directory in the role we're copying from (git can't store empty dirs)
-  file:
-    path: '{{ role_path }}/files/subdir/subdira'
-    state: directory
-
-- name: set the output subdirectory
-  set_fact: output_subdir={{output_dir}}/sub
-
-- name: make an output subdirectory
-  file: name={{output_subdir}} state=directory
-
-- name: setup link target for absolute link
-  copy: dest=/tmp/ansible-test-abs-link content=target
-
-- name: setup link target dir for absolute link
-  file: dest=/tmp/ansible-test-abs-link-dir state=directory
-
-- name: test recursive copy to directory no trailing slash, local_follow=False
-  copy: src=subdir dest={{output_subdir}} directory_mode=0700 local_follow=False
-  register: recursive_copy_result
-
-#- debug: var=recursive_copy_result
-- name: assert that the recursive copy did something
-  assert:
-    that:
-      - "recursive_copy_result|changed"
-
-- name: check that a file in a directory was transferred
-  stat: path={{output_dir}}/sub/subdir/bar.txt
-  register: stat_bar
-
-- name: check that a file in a deeper directory was transferred
-  stat: path={{output_dir}}/sub/subdir/subdir2/baz.txt
-  register: stat_bar2
-
-- name: check that a file in a directory whose parent contains a directory alone was transferred
-  stat: path={{output_dir}}/sub/subdir/subdir2/subdir3/subdir4/qux.txt
-  register: stat_bar3
-
-- name: assert recursive copy files
-  assert:
-    that:
-      - "stat_bar.stat.exists"
-      - "stat_bar2.stat.exists"
-      - "stat_bar3.stat.exists"
-
-- name: check symlink to absolute path
-  stat:
-    path: '{{ output_dir }}/sub/subdir/subdir1/ansible-test-abs-link'
-  register: stat_abs_link
-
-- name: check symlink to relative path
-  stat:
-    path: '{{ output_dir }}/sub/subdir/subdir1/bar.txt'
-  register: stat_relative_link
-
-- name: check symlink to self
-  stat:
-    path: '{{ output_dir }}/sub/subdir/subdir1/invalid'
-  register: stat_self_link
-
-- name: check symlink to nonexistent file
-  stat:
-    path: '{{ output_dir }}/sub/subdir/subdir1/invalid2'
-  register: stat_invalid_link
-
-- name: check symlink to directory in copy
-  stat:
-   path: '{{ output_dir }}/sub/subdir/subdir1/subdir3'
-  register: stat_dir_in_copy_link
-
-- name: check symlink to directory outside of copy
-  stat:
-    path: '{{ output_dir }}/sub/subdir/subdir1/ansible-test-abs-link-dir'
-  register: stat_dir_outside_copy_link
-
-- name: assert recursive copy symlinks local_follow=False
-  assert:
-    that:
-      - "stat_abs_link.stat.exists"
-      - "stat_abs_link.stat.islnk"
-      - "'/tmp/ansible-test-abs-link' == stat_abs_link.stat.lnk_target"
-      - "stat_relative_link.stat.exists"
-      - "stat_relative_link.stat.islnk"
-      - "'../bar.txt' == stat_relative_link.stat.lnk_target"
-      - "stat_self_link.stat.exists"
-      - "stat_self_link.stat.islnk"
-      - "'invalid' in stat_self_link.stat.lnk_target"
-      - "stat_invalid_link.stat.exists"
-      - "stat_invalid_link.stat.islnk"
-      - "'../invalid' in stat_invalid_link.stat.lnk_target"
-      - "stat_dir_in_copy_link.stat.exists"
-      - "stat_dir_in_copy_link.stat.islnk"
-      - "'../subdir2/subdir3' in stat_dir_in_copy_link.stat.lnk_target"
-      - "stat_dir_outside_copy_link.stat.exists"
-      - "stat_dir_outside_copy_link.stat.islnk"
-      - "'/tmp/ansible-test-abs-link-dir' == stat_dir_outside_copy_link.stat.lnk_target"
-
-- name: stat the recursively copied directories
-  stat: path={{output_dir}}/sub/{{item}}
-  register: dir_stats
-  with_items:
-    - "subdir"
-    - "subdir/subdira"
-    - "subdir/subdir1"
-    - "subdir/subdir2"
-    - "subdir/subdir2/subdir3"
-    - "subdir/subdir2/subdir3/subdir4"
-
-#- debug: var=dir_stats
-- name: assert recursive copied directories mode
-  assert:
-    that:
-      - "item.stat.mode == '0700'"
-  with_items: "{{dir_stats.results}}"
-
-- name: test recursive copy to directory no trailing slash, local_follow=False second time
-  copy: src=subdir dest={{output_subdir}} directory_mode=0700 local_follow=False
-  register: recursive_copy_result
-
-- name: assert that the second copy did not change anything
-  assert:
-    that:
-      - "not recursive_copy_result|changed"
-
-- name: cleanup the recursive copy subdir
-  file: name={{output_subdir}} state=absent
-
-#
-# Recursive copy with local_follow=False, trailing slash
-#
-
-- name: set the output subdirectory
-  set_fact: output_subdir={{output_dir}}/sub
-
-- name: make an output subdirectory
-  file: name={{output_subdir}} state=directory
-
-- name: setup link target for absolute link
-  copy: dest=/tmp/ansible-test-abs-link content=target
-
-- name: setup link target dir for absolute link
-  file: dest=/tmp/ansible-test-abs-link-dir state=directory
-
-- name: test recursive copy to directory trailing slash, local_follow=False
-  copy: src=subdir/ dest={{output_subdir}} directory_mode=0700 local_follow=False
-  register: recursive_copy_result
-
-#- debug: var=recursive_copy_result
-- name: assert that the recursive copy did something
-  assert:
-    that:
-      - "recursive_copy_result|changed"
-
-- name: check that a file in a directory was transferred
-  stat: path={{output_dir}}/sub/bar.txt
-  register: stat_bar
-
-- name: check that a file in a deeper directory was transferred
-  stat: path={{output_dir}}/sub/subdir2/baz.txt
-  register: stat_bar2
-
-- name: check that a file in a directory whose parent contains a directory alone was transferred
-  stat: path={{output_dir}}/sub/subdir2/subdir3/subdir4/qux.txt
-  register: stat_bar3
-
-- name: assert recursive copy files
-  assert:
-    that:
-      - "stat_bar.stat.exists"
-      - "stat_bar2.stat.exists"
-      - "stat_bar3.stat.exists"
-
-- name: check symlink to absolute path
-  stat:
-    path: '{{ output_dir }}/sub/subdir1/ansible-test-abs-link'
-  register: stat_abs_link
-
-- name: check symlink to relative path
-  stat:
-    path: '{{ output_dir }}/sub/subdir1/bar.txt'
-  register: stat_relative_link
-
-- name: check symlink to self
-  stat:
-    path: '{{ output_dir }}/sub/subdir1/invalid'
-  register: stat_self_link
-
-- name: check symlink to nonexistent file
-  stat:
-    path: '{{ output_dir }}/sub/subdir1/invalid2'
-  register: stat_invalid_link
-
-- name: check symlink to directory in copy
-  stat:
-    path: '{{ output_dir }}/sub/subdir1/subdir3'
-  register: stat_dir_in_copy_link
-
-- name: check symlink to directory outside of copy
-  stat:
-    path: '{{ output_dir }}/sub/subdir1/ansible-test-abs-link-dir'
-  register: stat_dir_outside_copy_link
-
-- name: assert recursive copy symlinks local_follow=False trailing slash
-  assert:
-    that:
-      - "stat_abs_link.stat.exists"
-      - "stat_abs_link.stat.islnk"
-      - "'/tmp/ansible-test-abs-link' == stat_abs_link.stat.lnk_target"
-      - "stat_relative_link.stat.exists"
-      - "stat_relative_link.stat.islnk"
-      - "'../bar.txt' == stat_relative_link.stat.lnk_target"
-      - "stat_self_link.stat.exists"
-      - "stat_self_link.stat.islnk"
-      - "'invalid' in stat_self_link.stat.lnk_target"
-      - "stat_invalid_link.stat.exists"
-      - "stat_invalid_link.stat.islnk"
-      - "'../invalid' in stat_invalid_link.stat.lnk_target"
-      - "stat_dir_in_copy_link.stat.exists"
-      - "stat_dir_in_copy_link.stat.islnk"
-      - "'../subdir2/subdir3' in stat_dir_in_copy_link.stat.lnk_target"
-      - "stat_dir_outside_copy_link.stat.exists"
-      - "stat_dir_outside_copy_link.stat.islnk"
-      - "'/tmp/ansible-test-abs-link-dir' == stat_dir_outside_copy_link.stat.lnk_target"
-
-- name: stat the recursively copied directories
-  stat: path={{output_dir}}/sub/{{item}}
-  register: dir_stats
-  with_items:
-    - "subdira"
-    - "subdir1"
-    - "subdir2"
-    - "subdir2/subdir3"
-    - "subdir2/subdir3/subdir4"
-
-#- debug: var=dir_stats
-- name: assert recursive copied directories mode
-  assert:
-    that:
-      - "item.stat.mode == '0700'"
-  with_items: "{{dir_stats.results}}"
-
-- name: test recursive copy to directory trailing slash, local_follow=False second time
-  copy: src=subdir/ dest={{output_subdir}} directory_mode=0700 local_follow=False
-  register: recursive_copy_result
-
-- name: assert that the second copy did not change anything
-  assert:
-    that:
-      - "not recursive_copy_result|changed"
-
-- name: cleanup the recursive copy subdir
-  file: name={{output_subdir}} state=absent
-
-#
-# test recursive copy local_follow=True, no trailing slash
-#
-
-- name: set the output subdirectory
-  set_fact: output_subdir={{output_dir}}/sub
-
-- name: make an output subdirectory
-  file: name={{output_subdir}} state=directory
-
-- name: setup link target for absolute link
-  copy: dest=/tmp/ansible-test-abs-link content=target
-
-- name: setup link target dir for absolute link
-  file: dest=/tmp/ansible-test-abs-link-dir state=directory
-
-- name: test recursive copy to directory no trailing slash, local_follow=True
-  copy: src=subdir dest={{output_subdir}} directory_mode=0700 local_follow=True
-  register: recursive_copy_result
-
-#- debug: var=recursive_copy_result
-- name: assert that the recursive copy did something
-  assert:
-    that:
-      - "recursive_copy_result|changed"
-
-- name: check that a file in a directory was transferred
-  stat: path={{output_dir}}/sub/subdir/bar.txt
-  register: stat_bar
-
-- name: check that a file in a deeper directory was transferred
-  stat: path={{output_dir}}/sub/subdir/subdir2/baz.txt
-  register: stat_bar2
-
-- name: check that a file in a directory whose parent contains a directory alone was transferred
-  stat: path={{output_dir}}/sub/subdir/subdir2/subdir3/subdir4/qux.txt
-  register: stat_bar3
-
-- name: check that a file in a directory whose parent is a symlink was transferred
-  stat: path={{output_dir}}/sub/subdir/subdir1/subdir3/subdir4/qux.txt
-  register: stat_bar4
-
-- name: assert recursive copy files
-  assert:
-    that:
-      - "stat_bar.stat.exists"
-      - "stat_bar2.stat.exists"
-      - "stat_bar3.stat.exists"
-      - "stat_bar4.stat.exists"
-
-- name: check symlink to absolute path
-  stat:
-    path: '{{ output_dir }}/sub/subdir/subdir1/ansible-test-abs-link'
-  register: stat_abs_link
-
-- name: check symlink to relative path
-  stat:
-    path: '{{ output_dir }}/sub/subdir/subdir1/bar.txt'
-  register: stat_relative_link
-
-- name: check symlink to self
-  stat:
-    path: '{{ output_dir }}/sub/subdir/subdir1/invalid'
-  register: stat_self_link
-
-- name: check symlink to nonexistent file
-  stat:
-    path: '{{ output_dir }}/sub/subdir/subdir1/invalid2'
-  register: stat_invalid_link
-
-- name: check symlink to directory in copy
-  stat:
-    path: '{{ output_dir }}/sub/subdir/subdir1/subdir3'
-  register: stat_dir_in_copy_link
-
-- name: check symlink to directory outside of copy
-  stat:
-    path: '{{ output_dir }}/sub/subdir/subdir1/ansible-test-abs-link-dir'
-  register: stat_dir_outside_copy_link
-
-- name: assert recursive copy symlinks local_follow=True
-  assert:
-    that:
-      - "stat_abs_link.stat.exists"
-      - "not stat_abs_link.stat.islnk"
-      - "stat_abs_link.stat.checksum == '0e8a3ad980ec179856012b7eecf4327e99cd44cd'"
-      - "stat_relative_link.stat.exists"
-      - "not stat_relative_link.stat.islnk"
-      - "stat_relative_link.stat.checksum == '6eadeac2dade6347e87c0d24fd455feffa7069f0'"
-      - "stat_self_link.stat.exists"
-      - "stat_self_link.stat.islnk"
-      - "'invalid' in stat_self_link.stat.lnk_target"
-      - "stat_invalid_link.stat.exists"
-      - "stat_invalid_link.stat.islnk"
-      - "'../invalid' in stat_invalid_link.stat.lnk_target"
-      - "stat_dir_in_copy_link.stat.exists"
-      - "not stat_dir_in_copy_link.stat.islnk"
-      - "stat_dir_in_copy_link.stat.isdir"
-      -
-      - "stat_dir_outside_copy_link.stat.exists"
-      - "not stat_dir_outside_copy_link.stat.islnk"
-      - "stat_dir_outside_copy_link.stat.isdir"
-
-- name: stat the recursively copied directories
-  stat: path={{output_dir}}/sub/{{item}}
-  register: dir_stats
-  with_items:
-    - "subdir"
-    - "subdir/subdira"
-    - "subdir/subdir1"
-    - "subdir/subdir1/subdir3"
-    - "subdir/subdir1/subdir3/subdir4"
-    - "subdir/subdir2"
-    - "subdir/subdir2/subdir3"
-    - "subdir/subdir2/subdir3/subdir4"
-
-#- debug: var=dir_stats
-- name: assert recursive copied directories mode
-  assert:
-    that:
-      - "item.stat.mode == '0700'"
-  with_items: "{{dir_stats.results}}"
-
-- name: test recursive copy to directory no trailing slash, local_follow=True second time
-  copy: src=subdir dest={{output_subdir}} directory_mode=0700 local_follow=True
-  register: recursive_copy_result
-
-- name: assert that the second copy did not change anything
-  assert:
-    that:
-      - "not recursive_copy_result|changed"
-
-- name: cleanup the recursive copy subdir
-  file: name={{output_subdir}} state=absent
-
-#
-# Recursive copy of tricky symlinks
-#
-- name: Create a directory to copy from
-  file:
-    path: '{{ output_dir }}/source1'
-    state: directory
-
-- name: Create a directory outside of the tree
-  file:
-    path: '{{ output_dir }}/source2'
-    state: directory
-
-- name: Create a symlink to a directory outside of the tree
-  file:
-    path: '{{ output_dir }}/source1/link'
-    src: '{{ output_dir }}/source2'
-    state: link
-
-- name: Create a circular link back to the tree
-  file:
-    path: '{{ output_dir }}/source2/circle'
-    src: '../source1'
-    state: link
-
-- name: Create output directory
-  file:
-    path: '{{ output_dir }}/dest1'
-    state: directory
-
-- name: Recursive copy the source
-  copy:
-    src: '{{ output_dir }}/source1'
-    dest: '{{ output_dir }}/dest1'
-    local_follow: True
-  register: copy_result
-
-- name: Check that the tree link is now a directory
-  stat:
-    path: '{{ output_dir }}/dest1/source1/link'
-  register: link_result
-
-- name: Check that the out of tree link is still a link
-  stat:
-    path: '{{ output_dir }}/dest1/source1/link/circle'
-  register: circle_result
-
-- name: Verify that the recursive copy worked
-  assert:
-    that:
-      - 'copy_result.changed'
-      - 'link_result.stat.isdir'
-      - 'not link_result.stat.islnk'
-      - 'circle_result.stat.islnk'
-      - '"../source1" == circle_result.stat.lnk_target'
-
-- name: Recursive copy the source a second time
-  copy:
-    src: '{{ output_dir }}/source1'
-    dest: '{{ output_dir }}/dest1'
-    local_follow: True
-  register: copy_result
-
-- name: Verify that the recursive copy made no changes
-  assert:
-    that:
-      - 'not copy_result.changed'
-
-#
-# Recursive copy with absolute paths (#27439)
-#
-- name: Test that output_dir is appropriate for this test (absolute path)
-  assert:
-    that:
-      - '{{ output_dir_expanded[0] == "/" }}'
-
-- name: create a directory to copy
-  file:
-    path: '{{ output_dir_expanded }}/source_recursive'
-    state: directory
-
-- name: create a file inside of the directory
-  copy:
-    content: "testing"
-    dest: '{{ output_dir_expanded }}/source_recursive/file'
-
-- name: Create a directory to place the test output in
-  file:
-    path: '{{ output_dir_expanded }}/destination'
-    state: directory
-
-- name: Copy the directory and files within (no trailing slash)
-  copy:
-    src: '{{ output_dir_expanded }}/source_recursive'
-    dest: '{{ output_dir_expanded }}/destination'
-
-- name: stat the recursively copied directory
-  stat: path={{output_dir}}/destination/{{item}}
-  register: copied_stat
-  with_items:
-    - "source_recursive"
-    - "source_recursive/file"
-    - "file"
-
-#- debug: var=copied_stat
-- name: assert with no trailing slash, directory and file is copied
-  assert:
-    that:
-      - "copied_stat.results[0].stat.exists"
-      - "copied_stat.results[1].stat.exists"
-      - "not copied_stat.results[2].stat.exists"
-
-- name: Cleanup
-  file:
-    path: '{{ output_dir_expanded }}/destination'
-    state: absent
-
-# Try again with no trailing slash
-
-- name: Create a directory to place the test output in
-  file:
-    path: '{{ output_dir_expanded }}/destination'
-    state: directory
-
-- name: Copy just the files inside of the directory
-  copy:
-    src: '{{ output_dir_expanded }}/source_recursive/'
-    dest: '{{ output_dir_expanded }}/destination'
-
-- name: stat the recursively copied directory
-  stat: path={{output_dir_expanded}}/destination/{{item}}
-  register: copied_stat
-  with_items:
-    - "source_recursive"
-    - "source_recursive/file"
-    - "file"
-
-#- debug: var=copied_stat
-- name: assert with trailing slash, only the file is copied
-  assert:
-    that:
-      - "not copied_stat.results[0].stat.exists"
-      - "not copied_stat.results[1].stat.exists"
-      - "copied_stat.results[2].stat.exists"
-
-#
-# issue 8394
-#
-
-- name: create a file with content and a literal multiline block
-  copy: |
-    content='this is the first line
-    this is the second line
-
-    this line is after an empty line
-    this line is the last line
-    '
-    dest={{output_dir}}/multiline.txt
-  register: copy_result6
-
-#- debug: var=copy_result6
-
-- name: assert the multiline file was created correctly
-  assert:
-    that:
-      - "copy_result6.changed"
-      - "copy_result6.dest == '{{output_dir_expanded}}/multiline.txt'"
-      - "copy_result6.checksum == '9cd0697c6a9ff6689f0afb9136fa62e0b3fee903'"
-
-# test overwriting a file as an unprivileged user (pull request #8624)
-# this can't be relative to {{output_dir}} as ~root usually has mode 700
-
-- name: create world writable directory
-  file: dest=/tmp/worldwritable state=directory mode=0777
-
-- name: create world writable file
-  copy: dest=/tmp/worldwritable/file.txt content="bar" mode=0666
-
-- name: overwrite the file as user nobody
-  copy: dest=/tmp/worldwritable/file.txt content="baz"
-  become: yes
-  become_user: nobody
-  register: copy_result7
-
-- name: assert the file was overwritten
-  assert:
-    that:
-      - "copy_result7.changed"
-      - "copy_result7.dest == '/tmp/worldwritable/file.txt'"
-      - "copy_result7.checksum == 'bbe960a25ea311d21d40669e93df2003ba9b90a2'"
-
-- name: clean up
-  file: dest=/tmp/worldwritable state=absent
-
-# test overwriting a link using "follow=yes" so that the link
-# is preserved and the link target is updated
-
-- name: create a test file to symlink to
-  copy: dest={{output_dir}}/follow_test content="this is the follow test file\n"
-
-- name: create a symlink to the test file
-  file: path={{output_dir}}/follow_link src='./follow_test' state=link
-
-- name: update the test file using follow=True to preserve the link
-  copy: dest={{output_dir}}/follow_link content="this is the new content\n" follow=yes
-  register: replace_follow_result
-
-- name: stat the link path
-  stat: path={{output_dir}}/follow_link
-  register: stat_link_result
-
-- name: assert that the link is still a link
-  assert:
-    that:
-    - stat_link_result.stat.islnk
-
-- name: get the checksum of the link target
-  shell: "{{ sha1sum.stdout }} {{output_dir}}/follow_test | cut -f1 -sd ' '"
-  register: target_file_result
-
-- name: assert that the link target was updated
-  assert:
-    that:
-    - replace_follow_result.checksum == target_file_result.stdout
-
-- name: update the test file using follow=False to overwrite the link
-  copy:
-    dest: '{{ output_dir }}/follow_link'
-    content: 'modified'
-    follow: False
-  register: copy_results
-
-- name: check the stat results of the file
-  stat:
-    path: '{{output_dir}}/follow_link'
-  register: stat_results
-
-#- debug: var=stat_results
-
-- name: assert that the file has changed and is not a link
-  assert:
-     that:
-       - "copy_results|changed"
-       - "'content' not in copy_results"
-       - "stat_results.stat.checksum == '99db324742823c55d975b605e1fc22f4253a9b7d'"
-       - "not stat_results.stat.islnk"
-
-#
-# I believe the below section is now covered in the recursive copying section.
-# Hold on for now as an original test case but delete once confirmed that
-# everything is passing
-
-#
-# Recursive copying with symlinks tests
-#
-- name: create a test dir to copy
-  file:
-    path: '{{ output_dir }}/top_dir'
-    state: directory
-
-- name: create a test dir to symlink to
-  file:
-    path: '{{ output_dir }}/linked_dir'
-    state: directory
-
-- name: create a file in the test dir
-  copy:
-    dest: '{{ output_dir }}/linked_dir/file1'
-    content: 'hello world'
-
-- name: create a link to the test dir
-  file:
-    path: '{{ output_dir }}/top_dir/follow_link_dir'
-    src: '{{ output_dir }}/linked_dir'
-    state: link
-
-- name: create a circular subdir
-  file:
-    path: '{{ output_dir }}/top_dir/subdir'
-    state: directory
-
-### FIXME:  Also add a test for a relative symlink
-- name: create a circular symlink
-  file:
-    path: '{{ output_dir }}/top_dir/subdir/circle'
-    src: '{{ output_dir }}/top_dir/'
-    state: link
-
-- name: copy the directory's link
-  copy:
-    src: '{{ output_dir }}/top_dir'
-    dest: '{{ output_dir }}/new_dir'
-    local_follow: True
-
-- name: stat the copied path
-  stat:
-    path: '{{ output_dir }}/new_dir/top_dir/follow_link_dir'
-  register: stat_dir_result
-
-- name: stat the copied file
-  stat:
-    path: '{{ output_dir }}/new_dir/top_dir/follow_link_dir/file1'
-  register: stat_file_in_dir_result
-
-- name: stat the circular symlink
-  stat:
-    path: '{{ output_dir }}/top_dir/subdir/circle'
-  register: stat_circular_symlink_result
-
-- name: assert that the directory exists
-  assert:
-    that:
-    - stat_dir_result.stat.exists
-    - stat_dir_result.stat.isdir
-    - stat_file_in_dir_result.stat.exists
-    - stat_file_in_dir_result.stat.isreg
-    - stat_circular_symlink_result.stat.exists
-    - stat_circular_symlink_result.stat.islnk
BREAKS HERE
-  tags:
-    - openshift_host_templates
-    - bastion_preparation
-  tags:
-    - openshift_host_templates
-    - openshift_install_idm_cert
BREAKS HERE
-  - debug: msg=Karate Chop!
BREAKS HERE
-    reverse_ip: '{{ external_ip | regex_replace(ip_regex, "\3.\2.\1") }}'
-  when: bind.reverse_ip == "auto"
-- name: Link DNS entries in /var/cache/bin
-  when: keys_dir.changed
-  file:
-    state: link
BREAKS HERE
-        command: "postconf -n smtpd_client_restrictions"
-        command: "postconf -e 'smtpd_client_restrictions=permit_mynetworks, reject'"
BREAKS HERE
-  hosts: undercloud
BREAKS HERE
-nexus_backup_dir: '/tmp/nexus-backup'
BREAKS HERE
-# https://bugzilla.redhat.com/show_bug.cgi?id=1535689
-- name: Allow execmem for Apache
-  seboolean:
-    name: httpd_execmem
-    state: yes
-    persistent: yes
-
BREAKS HERE
-      include_tasks: "/opt/plexguide/containers/_core.yml"
BREAKS HERE
-# fedora-messaging user/queue setup
-
-# relvalconsumer and autocloudreporter aren't particularly related
-# to openQA in any way, we just put those role on these boxes. There's
-# nowhere more obviously correct for rvc and acr should be on an
-# Autocloud box but I don't know if they're authed for RDB.
-  roles:
-   - { role: openqa/server, tags: ['openqa_server'] }
-   - { role: openqa/dispatcher, tags: ['openqa_dispatcher'] }
-   - { role: check-compose, tags: ['check-compose'] }
-   - { role: fedmsg/base, tags: ['fedmsg_base', 'fedmsg'] }
-   - { role: fedmsg/hub, tags: ['fedmsg_hub', 'fedmsg'] }
-   - { role: relvalconsumer, tags: ['relvalconsumer'] }
-   - { role: autocloudreporter, tags: ['autocloudreporter'] }
-
-  handlers:
-   - import_tasks: "{{ handlers_path }}/restart_services.yml"
-
-- name: set up openQA server data NFS mounts (staging)
-  hosts: openqa_stg
-
-  vars_files:
-   - /srv/web/infra/ansible/vars/global.yml
-   - "/srv/private/ansible/vars.yml"
-   - /srv/web/infra/ansible/vars/{{ ansible_distribution }}.yml
-
-  roles:
BREAKS HERE
-    - neutron_lbaas | bool
BREAKS HERE
----
-- name: Converge
-  hosts: all
-  become: true
-  pre_tasks:
-    - name: Check if rhel
-      command: which subscription-manager
-      ignore_errors: true
-      changed_when: false
-      register: sm
-
-    - name: Register RHEL
-      redhat_subscription:
-        state: present
-        password: "{{ lookup('env', 'RHN_PASSWORD') }}"
-        username: "{{ lookup('env', 'RHN_USERNAME') }}"
-        auto_attach: true
-      when: not sm is failed
-
-  roles:
-    - role: RHEL7-STIG
-      rhel7stig_using_password_auth: false
-
-  post_tasks:
-    - name: Check if rhel
-      command: which subscription-manager
-      ignore_errors: true
-      changed_when: false
-      register: sm
-
-    - name: Deregister from RH
-      redhat_subscription:
-        state: absent
-      when: not sm is failed
BREAKS HERE
-  - include: $roles/badges-backend
BREAKS HERE
-nova_novncproxy_base_url: "{{ nova_novncproxy_base_uri }}/vnc_auto.html"
-nova_novncproxy_git_repo: https://github.com/kanaka/novnc
BREAKS HERE
-    yum: name="python-fmn*" state=latest
BREAKS HERE
-- name: Update packages on the host
-  hosts: undercloud
-  vars:
-    - ansible_ssh_user: root
-    - name: repolist
-      command: yum -d 7 repolist
-
-    - name: update all packages
-      yum: name=* state=latest
-
-- name: Create the stack user on the undercloud
-  hosts: undercloud
-  vars:
-    - ansible_ssh_user: root
-  tasks:
-    - name: delete user (workaround for BZ 1284717)
-      user: name="{{ provisioner.remote_user }}" state=absent remove=yes force=yes
-      tags: workaround
-
-    - name: inspect user removal
-      shell: >
-        ls /home | grep "{{ provisioner.remote_user }}";
-        cat /etc/passwd | grep "{{ provisioner.remote_user }}";
-        cat /etc/group | grep "{{ provisioner.remote_user }}";
-        cat /etc/shadow | grep "{{ provisioner.remote_user }}";
-        ls /var/spool/mail | grep "{{ provisioner.remote_user }}";
-      register: result
-      failed_when: result.rc != 1
-
-    - name: create user
-      user: name="{{ provisioner.remote_user }}" state=present password=stack
-
-    - name: copy the .bash_profile file
-      command:  cp /root/.bash_profile /home/{{ provisioner.remote_user }}/
-
-    - name: create .ssh dir
-      file: path=/home/{{ provisioner.remote_user }}/.ssh mode=0700 owner=stack group=stack state=directory
-
-    - name: copy the authorized_keys file
-      command:  cp /root/.ssh/authorized_keys /home/{{ provisioner.remote_user }}/.ssh/
-
-    - name: set file permissions on authorized_hosts
-      file: path=/home/{{ provisioner.remote_user }}/.ssh/authorized_keys mode=0600 owner=stack group=stack
-
-    - name: copy ssh keys
-      command:  cp /root/.ssh/id_rsa /home/{{ provisioner.remote_user }}/.ssh/
-      when: hw_env.env_type == 'ovb_host_cloud'
-
-    - name: copy ssh pub keys
-      command:  cp /root/.ssh/id_rsa.pub /home/{{ provisioner.remote_user }}/.ssh/
-      when: hw_env.env_type == 'ovb_host_cloud'
-
-    - name: set permission on keys
-      file: path=/home/{{ provisioner.remote_user }}/.ssh/id_rsa mode=0600 owner=stack group=stack
-      when: hw_env.env_type == 'ovb_host_cloud'
-
-    - name: set permission on pub keys
-      file: path=/home/{{ provisioner.remote_user }}/.ssh/id_rsa.pub mode=0644 owner=stack group=stack
-      when: hw_env.env_type == 'ovb_host_cloud'
-
-    - name: add user to sudoers
-      lineinfile: dest=/etc/sudoers line="stack ALL=(root) NOPASSWD:ALL"
-
-    - name: set fact for the stack user home
-      set_fact: instack_user_home=/home/{{ provisioner.remote_user }}
-
-    - name: enabling ip forwarding
-      lineinfile: dest=/etc/sysctl.conf line='net.ipv4.ip_forward = 1' insertafter=EOF state=present
-      when: hw_env.ip_forwarding is defined and hw_env.ip_forwarding == 'true'
-
-    - name: check ip forwarding
-      shell: sysctl -p /etc/sysctl.conf
-      when: hw_env.ip_forwarding is defined and hw_env.ip_forwarding == 'true'
-
-- include: repo-{{ product.name }}.yml repo_host=undercloud
-
-- name: Configure the baremetal undercloud
-  hosts: undercloud
-  tasks:
-    - name: check if instackenv.json exists in root
-      stat: path="/root/instackenv.json"
-      register: instackenv_json_root
-      sudo_user: root
-      sudo: yes
-
-    - name: copy instackenv.json from root if it exists there
-      shell: cp /root/instackenv.json {{ instack_user_home }}/instackenv.json
-      when: instackenv_json_root.stat.exists == True
-      sudo_user: root
-      sudo: yes
-
-    - name: get instackenv.json
-      synchronize: src={{base_dir}}/khaleesi-settings/hardware_environments/{{hw_env.env_type}}/instackenv.json dest={{ instack_user_home }}/instackenv.json
-      when: instackenv_json_root.stat.exists == False
-
-    - name: chown instackenv.json
-      file: path={{ instack_user_home }}/instackenv.json owner=stack group=stack
-      sudo_user: root
-      sudo: yes
-
-    - name: install ipmitool
-      yum: name={{ item }} state=latest
-      with_items:
-        - OpenIPMI
-        - OpenIPMI-tools
-      sudo_user: root
-      sudo: yes
-
-    - name: install sshpass - DRACS
-      yum: name=sshpass state=latest
-      sudo_user: root
-      sudo: yes
-      when: hw_env.remote_mgmt == "dracs"
-
-    - name: start IMPI service
-      shell: >
-        sudo chkconfig ipmi on;
-        sudo service ipmi start
-
-    - name: get tools to validate instackenv.json/nodes.json
-      git: >
-        repo="https://github.com/rthallisey/clapper.git"
-        dest="{{instack_user_home}}/clapper"
-
-    - name: validate instackenv.json
-      shell: >
-        chdir={{instack_user_home}}
-        python clapper/instackenv-validator.py -f {{ instack_user_home }}/instackenv.json
-      register: instackenv_validator_output
-
-    - name: fail if instackenv.json fails validation
-      fail: msg="instackenv.json didn't validate."
-      when: instackenv_validator_output.stdout.find("SUCCESS") == -1
-
-    - name: get number of overcloud nodes
-      shell: >
-        export IP_LENGTH=(`cat {{ instack_user_home }}/instackenv.json | grep -o 'pm_addr.*' | cut -f2-  -d':' | wc -l`);
-        echo $(($IP_LENGTH))
-      register: node_length
-
-    - name: power off node boxes - IPMI
-      shell: >
-         export IP=(`cat {{ instack_user_home }}/instackenv.json | grep -o 'pm_addr.*' | cut -f2-  -d':' | sed 's/[},\"]//g'`);
-         export USER=(`cat {{ instack_user_home }}/instackenv.json | grep -o 'pm_user.*' | cut -f2-  -d':' |rev | cut -c 2- | rev | sed 's/[},\"]//g'`);
-         export PASSWORD=(`cat {{ instack_user_home }}/instackenv.json | grep -o 'pm_password.*' | cut -f2-  -d':' |rev | cut -c 2- | rev | sed 's/[},\"]//g'`);
-         ipmitool -I lanplus -H ${IP[item]} -U ${USER[item]} -P ${PASSWORD[item]} power off
-      with_sequence: count="{{node_length.stdout}}"
-      when: hw_env.remote_mgmt == "ipmi"
-
-    - name: power off node boxes - DRACS
-      shell: >
-         export IP=(`cat {{ instack_user_home }}/instackenv.json | grep -o 'pm_addr.*' | cut -f2-  -d':' | sed 's/[},\"]//g'`);
-         export USER=(`cat {{ instack_user_home }}/instackenv.json | grep -o 'pm_user.*' | cut -f2-  -d':' |rev | cut -c 2- | rev | sed 's/[},\"]//g'`);
-         export PASSWORD=(`cat {{ instack_user_home }}/instackenv.json | grep -o 'pm_password.*' | cut -f2-  -d':' |rev | cut -c 2- | rev | sed 's/[},\"]//g'`);
-         sshpass -p ${PASSWORD[item]} ssh -o "StrictHostKeyChecking=no" ${USER[item]}@${IP[item]} "racadm serveraction powerdown"
-      with_sequence: count="{{node_length.stdout}}"
-      when: hw_env.remote_mgmt == "dracs"
BREAKS HERE
-    until: cloudformation_out|succeeded
-  - debug:
-     var: cloudformation_out.stack_outputs
-
-    delay: 30
BREAKS HERE
-    why: In favor of become which is a generic framework
-    why: In favor of become which is a generic framework
-    why: In favor of become which is a generic framework
-    why: In favor of become which is a generic framework
-    why: In favor of become which is a generic framework
-    why: In favor of become which is a generic framework
-    why: In favor of become which is a generic framework
-    why: In favor of become which is a generic framework
-    why: In favor of become which is a generic framework
BREAKS HERE
-    register: coreos-cincinnati-stub-build
-    when: coreos-cincinnati-stub-build.changed
-    register: coreos-cincinnati-stub-config
-    register: coreos-cincinnati-deployment
-    when: coreos-cincinnati-stub-config.changed or coreos-cincinnati-deployment.changed
BREAKS HERE
-nova_default_schedule_zone: None
BREAKS HERE
-    - ceph-config
-    - ceph-nfs
BREAKS HERE
-    domain_name: "{{ openstack_domain|default('openstack.local') }}"
BREAKS HERE
-      with_items: "{{ static_nats }}"
-      with_items: "{{ static_nats }}"
BREAKS HERE
-  hosts: undercloud,overcloud_nodes
-  hosts: overcloud_nodes
-        with_items: "{{ groups.overcloud_nodes|default([]) }}"
BREAKS HERE
-    - neutron-agents
BREAKS HERE
-  command: "true"
-  changed_when: no
-      - notimplemented
-  command: "true"
-  changed_when: no
-      - notimplemented
BREAKS HERE
-  # - gather-easyfix::proxy
-  # - fedmsg::proxy-crl
BREAKS HERE
-      - name: net-iso virt setup vlans (ipv4)
-        when: installer.network.isolation == 'single_nic_vlans'
-      - name: net-iso virt setup vlans (ipv6)
-        shell: >
-            source {{ instack_user_home }}/stackrc;
-            sudo ovs-vsctl add-port br-ctlplane vlan10 tag=10 -- set interface vlan10 type=internal;
-            sudo ip l set dev vlan10 up; sudo ip addr add 2001:db8:fd00:1000:dead:beef:cafe:f00/64 dev vlan10;
BREAKS HERE
-      galaxy_fetch_eggs: no
BREAKS HERE
-          ipa_realm: "{{ipa_realm}}",
-          osbs_url: "{{osbs_url}}",
-          registry: "{{docker_registry}}",
-          parent_registry: "{{source_registry}}"
-          ipa_realm: "{{ipa_realm}}",
-          osbs_url: "{{osbs_url}}",
-          registry: "{{docker_registry}}",
-          parent_registry: "{{source_registry}}"
BREAKS HERE
-  user: root
-
-  vars_files:
-    - defaults/repo_packages/openstack_services.yml
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - lxc-hosts
BREAKS HERE
-#   enabled: whether the variable should be set or not
BREAKS HERE
-    - role: "{{ rolename }}"
BREAKS HERE
-    - set_fact:
BREAKS HERE
-    http_port: 9080
-    http_port: 9081
-    http_port: 9082
BREAKS HERE
-    - { role: olm_setup, when: (not broker_ci)}
BREAKS HERE
-  - debug:
-      var: drop_script
-      verbosity: 2
BREAKS HERE
-#      - name: Update apt cache and upgrade
-#        apt: update_cache=yes cache_valid_time=3600 upgrade=yes
-#      - name: Install stuff from Aptitude
-#        apt: name={{ item }} state=installed
-#        with_items:
-#         - git
-#         - tig
-#         - vim
-#         - gdb
-#         - cgdb
-#         - bash-completion # Not included in the docker image
-#  roles:
-#      - { role: dotfiles, sudo: false }
-#      - { role: golang, sudo: false }
-#      - { role: youcompleteme, sudo: false } # Comment out this to save time!!
BREAKS HERE
-    - name: grab boto version
-      command: python -c 'import boto3; print(boto3.__version__)'
-      register: py_cmd
-
-    - name: make sure we are running correct boto version
-      assert:
-        that:
-        - py_cmd.stdout.startswith('1.7')
-
-    - name: check for underscores
-      fail:
-        msg: "Amazon AWS does not allow underscores _ for s3 websites, please see https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html"
-      when:
-        - create_login_page is defined
-        - create_login_page
-        - "'_' in ec2_name_prefix"
-
-    - name: grab information about AWS user
-      aws_caller_facts:
-        region: "{{ ec2_region }}"
-      register: whoami
-
-    - name: save username of AWS user
-      set_fact:
-         linklight_user: '{{ whoami.arn.split("/")[-1] }}'
-
-    - debug: msg='{{ whoami.arn.split("/")[-1] }}'
-
BREAKS HERE
-    bodhi_version: 3.13.2-1.fc29.infra
BREAKS HERE
-            path: /tmp/example.playbook
BREAKS HERE
-  tasks:
BREAKS HERE
-  connection: local
-        ansible_host: "{{ spawned_out.server.private_v4 }}"
-        ansible_ssh_common_args: "-F {{ inventory_dir }}/ssh_config.{{ spawned_out.server.cloud }}"
-    - name: Spin waiting for instance to become accessible
-      command: >
-        ssh -o BatchMode=yes -o UserKnownHostsFile=/dev/null
-        -o StrictHostKeyChecking=no -o ConnectTimeout=10
-        {{ ansible_ssh_common_args }}
-        {{ ansible_user }}@{{ ansible_host }} /bin/true
-      register: alive
-      until: alive | success
-      retries: 45  # if it's not up in 1.5 minutes, give up
-      delay: 2
-      delegate_to: localhost
BREAKS HERE
-        drs_user: '{{alco_user}}'
-        drs_pub_key_file: '{{alco_key_file}}'
-        drd_user: "{{alco_user}}"
-        name: alco-anaconda
-        anaconda_docker_data_dir: '{{alco_data_dir}}'
-        anaconda_docker_net_name: '{{alco_docker_net_name}}'
-        anaconda_docker_bind_addr: '{{alco_docker_ip}}'
-        anaconda_docker_memory_limit: '2g' #todo: use half of available size
BREAKS HERE
-    - name: test raw module with ipconfig
-      hosts: windows
-      tasks:
-        - name: run ipconfig
-          win_command: ipconfig
-          register: ipconfig
-        - debug: var=ipconfig
BREAKS HERE
-        cp {{ instack_user_home }}/openstack-virtual-baremetal/templates/env.yaml.example {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
-        sed -i "s#http://10.0.0.1:5000/v2.0#$OS_AUTH_URL#g" {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
-        sed -i 's/os_password: password/os_password: '"$OS_PASSWORD"'/g'  {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
-        sed -i 's/CentOS-7-x86_64-GenericCloud-1503/{{ provisioner.image.name }}/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
-        sed -i 's/node_count: 2/node_count: {{ provisioner.node_count }}/g'  {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
-        sed -i 's/private_net: private/private_net: {{ tmp.node_prefix }}private/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
-        sed -i 's/public_net: public/public_net: {{ tmp.node_prefix }}public/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
-        sed -i 's/provision_net: provision/provision_net: {{ tmp.node_prefix }}provision/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
-        sed -i 's/bmc_prefix: bmc/bmc_prefix: {{ tmp.node_prefix }}bmc/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
-        sed -i 's/baremetal_prefix: baremetal/baremetal_prefix: {{ tmp.node_prefix }}baremetal/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
-        sed -i 's/key_name: default/key_name: {{ tmp.node_prefix }}default/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
-        sed -i 's/bmc_flavor: bmc/bmc_flavor: m1.small/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
-        sed -i 's/external_net: external/external_net: nova/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
-        heat stack-$op -f templates/virtual-baremetal.yaml -e env.yaml -e templates/resource-registry.yaml "{{product.name}}-{{ tmp.node_prefix }}baremetal"
BREAKS HERE
-
-  tasks:
-    - debug: var=nodejs_install_npm_user
BREAKS HERE
-  - name: swap on /dev/vda 100GB volume for tmpfs mock plugin
-    command: swapon /dev/vda
BREAKS HERE
-        path: "/var/lib/libvirt/images/{{ hostvars[item]['server_hostname'] }}.img"
-            paths: /var/lib/libvirt/images
-            -b /var/lib/libvirt/images/{{ hostvars[item]['server_hostname'] }}-base.img
-            /var/lib/libvirt/images/{{ hostvars[item]['server_hostname'] }}.img
BREAKS HERE
-      horizon_server_name: "{{ container_name }}"
BREAKS HERE
-  hosts: all
-    - { role: tests,
-        tags: test }
BREAKS HERE
-          envs:
-          drdc_env: "{{ {}|combine(envs.common, envs[_logspout_target]|default({})) }}"
-          envs: "{{containers_env}}"
-          envs: "{{env|combine(containers_env, front_env_extra|default({}) )}}"
-    - name: Including ClickHouse maintain role
-      block:
-      - import_tasks: tasks/ch_migrate.yml
-        vars:
-          operate_db: "{{ch_db}}"
-          operate_host: "127.0.0.1"
-          migrations_path: clickhouse_migrations
-      tags: ['never', 'chmigrate']
-          envs: "{{env|combine(containers_env)}}"
-          envs: "{{env|combine(containers_env)}}"
BREAKS HERE
-      when: osrelease | version_compare('3.7.9', 'eq')
-      when: osrelease | version_compare('3.7.9', 'eq')
BREAKS HERE
-nova_libvirt_vif_driver: nova.virt.libvirt.vif.NeutronLinuxBridgeVIFDriver
BREAKS HERE
-    security_unattended_upgrades_enabled: true
-    security_unattended_upgrades_notifications: true
-    security_rhel7_automatic_package_updates: yes
BREAKS HERE
-        - {
-          href: "http://rpms.famillecollet.com/enterprise/remi-release-6.rpm",
-        }
BREAKS HERE
-  hosts: iscsigws
BREAKS HERE
-      name: "{{ security_group }}"
-      region: "{{ region }}"
-      region: "{{ region }}"
-      group: "{{ security_group }}"
-      region: "{{ region }}"
-      group: "{{ security_group }}"
-  - name: Provision Gitlab
-    ec2:
-      aws_access_key: "{{ec2_access_key}}"
-      aws_secret_key: "{{ec2_secret_key}}"
-      key_name: "{{ec2_key}}"
-      region: "{{ region }}"
-      group: "{{ security_group }}"
-      instance_type: t2.medium
-      image: "{{ ami_id }}"
-      wait: true
-      exact_count: "{{ number_of_git_systems }}"
-      count_tag:
-        identity: git
-      instance_tags:
-        identity: git
-    register: git_server
-
BREAKS HERE
-	        }]
-      changed_when: response.status.code == 0
BREAKS HERE
-    - inventory_hostname == groups['nova_api_os_compute'][0]
-    - inventory_hostname == groups['nova_api_os_compute'][0]
-    - nova_placement_service_enabled | bool
-    - inventory_hostname in groups['nova_api_placement']
-    - inventory_hostname in groups['nova_compute']
-    - inventory_hostname == groups['nova_api_os_compute'][0]
BREAKS HERE
-
BREAKS HERE
-          
-        - name: check that graceful error message is returned when creation with cpu_options and old botocore 
-              - 'ec2_instance_cpu_options_creation.msg == "cpu_options is only supported with botocore >= 1.10.16"' 
BREAKS HERE
-# Allow the user to optionally perform a yum repo setup
-# Default is set to false as the quickstart images will provide the
-# correct yum repos.
-- include: repo-setup.yml
-  when: virthost_repo_setup|default(false)
-
BREAKS HERE
-    command: oc tag openshift/jenkins:latest openshift/jenkins:v{{osrelease}} -n openshift
-    command: oc tag --source=docker  openshift/jenkins:latest registry.access.redhat.com/openshift3/jenkins-2-rhel7:v{{osrelease}} -n openshift
BREAKS HERE
-- include: biocache-hubs.yml
BREAKS HERE
-    - copy:
-        dest: "{{ tester.dir}}/with_venv"
-    - template: dest=/root/configure-tempest.sh src=../templates/configure-tempest.sh owner=root group=root mode=0755
-    - command: "{{ tester.dir }}/with_venv /root/configure-tempest.sh"
-    - shell: "{{ tester.dir }}/with_venv testr init"
-        chdir: "{{ tester.dir }}"
-        creates: "{{ tester.dir }}/.testrepository"
BREAKS HERE
-    notify: Upgrade Rocket.Chat
BREAKS HERE
-  vars:
-    mon_group_name:       mons
-    osd_group_name:       osds
-    mds_group_name:       mdss
-    rgw_group_name:       rgws
-    rbdmirror_group_name: rbd-mirrors
-    nfs_group_name:       nfss
-    client_group_name:       clients
-
-    - "{{ mon_group_name }}"
-    - "{{ osd_group_name }}"
-    - "{{ mds_group_name }}"
-    - "{{ rgw_group_name }}"
-    - "{{ rbdmirror_group_name }}"
-    - "{{ nfs_group_name }}"
-    - "{{ client_group_name }}"
-    - "{{ mds_group_name }}"
-    - "{{ rgw_group_name }}"
-    - "{{ rbdmirror_group_name }}"
-    - "{{ nfs_group_name }}"
-    - "{{ osd_group_name }}"
-    - "{{ mon_group_name }}"
-    mon_group_name:       mons
-    osd_group_name:       osds
-    mds_group_name:       mdss
-    rgw_group_name:       rgws
-    rbdmirror_group_name: rbd-mirrors
-    nfs_group_name:       nfss
-    client_group_name:       clients
-
-    - "{{ mon_group_name }}"
-    - "{{ osd_group_name }}"
-    - "{{ mds_group_name }}"
-    - "{{ rgw_group_name }}"
-    - "{{ rbdmirror_group_name }}"
-    - "{{ nfs_group_name }}"
-    - "{{ client_group_name }}"
-  vars:
-    mon_group_name:       mons
-    osd_group_name:       osds
-    mds_group_name:       mdss
-    rgw_group_name:       rgws
-    rbdmirror_group_name: rbdmirrors
-    nfs_group_name:       nfss
-    restapi_group_name:   restapis
-
BREAKS HERE
-        url: 'https://github.com/AdoptOpenJDK/openjdk10-releases/releases/download/jdk-10.0.1%2B10/OpenJDK10_x64_Linux_201807101745.tar.gz'
BREAKS HERE
-# - name: Download servers
-#   hosts: download
-#   user: root
-#   gather_facts: False
-#   accelerate: False
-#   vars_files:
-#    - /srv/web/infra/ansible/vars/global.yml
-#    - "{{ private }}/vars.yml"
-#    - /srv/web/infra/ansible/vars/{{ ansible_distribution }}.yml
-#   tasks:
-#   - include: "{{ tasks }}/accelerate_prep.yml"
-#   handlers:
-#   - include: "{{ handlers }}/restart_services.yml"
-  accelerate: False
BREAKS HERE
-- name: setup f5 nodes
-  hosts: f5
-  connection: local
-  gather_facts: no
-  roles:
-    - { role: f5_setup, when: f5workshop }
-
BREAKS HERE
-        version: "v8.3"
BREAKS HERE
-  pre_tasks:
-    - include: common/ensure-oslomsg.yml
-      rpc_vhost: "{{ ceilometer_oslomsg_rpc_vhost }}"
-      rpc_user: "{{ ceilometer_oslomsg_rpc_userid }}"
-      rpc_password: "{{ ceilometer_oslomsg_rpc_password }}"
-      notify_vhost: "{{ ceilometer_oslomsg_notify_vhost }}"
-      notify_user: "{{ ceilometer_oslomsg_notify_userid }}"
-      notify_password: "{{ ceilometer_oslomsg_notify_password }}"
-  roles:
-    - role: "os_ceilometer"
BREAKS HERE
-  - include_tasks: "{{ playbook_dir }}/openstack-image-setup.yml"
BREAKS HERE
-    - name: Clone AWX into configured directory (shallow clone if supported).
BREAKS HERE
-    when: "backend" in ansible_hostname
-    when: "web" in ansible_hostname
-    when: "backend" in ansible_hostname
-    when: "web" in ansible_hostname
BREAKS HERE
-- name: install Grafana
-  import_playbook: /usr/share/ansible/openshift-ansible/playbooks/openshift-grafana/config.yml
-  ignore_errors: true
-  tags:
-    - logging-metrics
-    - ocp-deploy
-  
-    - name: Install oc client
-      yum:
-        name: atomic-openshift-clients
-        state: latest
-      become: yes
-      tags: ocp-post
- 
BREAKS HERE
-    shell: $(lsblk --nodeps -no pkname "{{ item }}")
BREAKS HERE
-  hosts: cm-node
-  hosts: cm-node
-      template: src=library/cloudera/cluster.yamlj2 dest=/opt/
-
BREAKS HERE
-  when:
-    - ironic_oneview_enabled | bool
-    - inventory_hostname in groups['ironic_conductor']
-- include: ironic_api_post_install.yml
-  when: inventory_hostname in groups['ironic_api']
-- include: ironic_init.yml
BREAKS HERE
-    when: not "true" in nonagios
BREAKS HERE
-      when: install_opentlc_integration == true
BREAKS HERE
-      name: "ceph-rbd-mirror@{{ ansible_hostname }}"
BREAKS HERE
-      when: "'mongo' in group_names"
-      
BREAKS HERE
-        copy:
-            src: files/debug.yaml
-            dest: "{{ template_base }}"
BREAKS HERE
-  serial: '{{ serial|default("0") }}'
BREAKS HERE
-          testtools==0.9.34
BREAKS HERE
-  when: deploy_cns | default(true) | bool
BREAKS HERE
-## Common Ceilometer Overrides
-# ceilometer_db_type: mongodb
-# ceilometer_db_ip: localhost
-# ceilometer_db_port: 27017
-
BREAKS HERE
-      when: swapfile_size != false
BREAKS HERE
-        version: "v8.3"
BREAKS HERE
-      template_base: "{{ ansible_user_dir }}/splitstack"
-            nics_subfolder: ""
BREAKS HERE
-    - include: create-grant-db.yml
-      db_name: "{{ nova_galera_database }}"
-      db_password: "{{ nova_container_mysql_password }}"
-
-    - include: create-grant-db.yml
-      db_name: "{{ nova_api_galera_database }}"
-      db_password: "{{ nova_api_container_mysql_password }}"
-
-    - include: create-grant-db.yml
-      db_name: "{{ nova_cell0_database }}"
-      db_password: "{{ nova_api_container_mysql_password }}"
-      db_user: "{{ nova_api_galera_user }}"
-      db_append_privs: "yes"
-
BREAKS HERE
-  DAC_chmod: yes                                  # V-38543
-  DAC_fchmod: yes                                 # V-38547
-  DAC_fchmodat: yes                               # V-38550
BREAKS HERE
-# Make sure that chronyd service is started and enabled on boot
-- name: Start and enable chronyd service
BREAKS HERE
-    - set_fact: owasp_python_args="--enableExperimental true"
-    - set_fact: owasp_args="{{owasp_proxy_params}} {{owasp_build_h2_db_arg}} {{owasp_python_args}} --out {{owasp_report_file}} --scan {{owasp_scan_dir}} -P {{owasp_pom}} --project {{owasp_project_label}} --data {{nist_dl_data_dir}}"
-      set_fact: owasp_args="{{owasp_proxy_params}} {{owasp_build_h2_db_arg}} {{owasp_python_args}} --out {{owasp_report_file}} --scan {{owasp_scan_dir}} -P {{owasp_pom}} --project {{owasp_project_label}} --data {{nist_dl_data_dir}}"
BREAKS HERE
-    delegate_to: development
-    delegate_to: development
-      dest: "../{{ site }}/database_backup"
BREAKS HERE
-
BREAKS HERE
-      - fuse
-      - nano
-      - fail2ban
-      - wget
-      - lsb-release
-      - figlet
-      - update-notifier-common
-      - software-properties-common
-      - unrar
-      - unzip
-      - glances
-      - python-pip
-      - python3-pip
-      - python-passlib
-      - zip
-      - curl
-      - man-db
-      - htop
-      - openssh-server
-      - dirmngr
-      - npm
-      - zip
-      - apt-transport-https
-      - ca-certificates
-      - tree
-      - ncdu
-      - ctop
-      - dialog
-      - dnsutils
-      - mc
-      - apache2-utils
-      - lsof
-      - pwgen
-      - gawk
-      - python-lxml
-      - acl
-      - bc
-    #ignore_errors: yes
BREAKS HERE
-  gather_facts: no
BREAKS HERE
-  - name: Create the NAT SSH rule on Panorama
-      rule_name: "Web SSH"
-      from_zone: ["external"]
-      to_zone: "external"
-      source: ["any"]
-      destination: ["10.0.0.100"]
-      service: "service-tcp-221"
-      snat_type: "dynamic-ip-and-port"
-      snat_interface: "ethernet1/2"
-      dnat_address: "10.0.1.101"
-      dnat_port: "22"
BREAKS HERE
-  - udp_ports: []
BREAKS HERE
-  gather_facts: False
BREAKS HERE
-          drdc_dir: "{{build_dir}}/theia"
-          drdc_repo: "https://github.com/rockstat/theia-build.git"
BREAKS HERE
-                      default(hostvars[item.item].ansible_ssh_host }}"
BREAKS HERE
-  debug:
-    msg: "Post-Software checks completed successfully"
BREAKS HERE
-        exclusion_filters: ['__init__.py']
BREAKS HERE
-- name: Set LXC cache path fact
-  set_fact:
-    cache_path_fact: "{{ lxc_container_cache_path }}/{{ lxc_cache_map.distro }}/{{ lxc_cache_map.release }}/{{ lxc_cache_map.arch }}/{{ lxc_cache_default_variant }}"
-    cache_index_item: "{{ lxc_cache_map.distro }};{{ lxc_cache_map.release }};{{ lxc_cache_map.arch }};{{ lxc_cache_default_variant }}"
-    cache_time: "{{ ansible_date_time.epoch }}"
-  tags:
-    - always
-
BREAKS HERE
-    --data-disk-sizes 64 32 {{ infra_cns_size | default('') }}
-    --data-disk-sizes 64 32
-- name: Azure | Wait for master VM creation
BREAKS HERE
-    - role: "{{ rolename | basename }}"
-    - role: "{{ rolename | basename }}"
BREAKS HERE
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      vars:
-        user_name: "{{ aodh_galera_user }}"
-        password: "{{ aodh_container_db_password }}"
-        login_host: "{{ aodh_galera_address }}"
-        db_name: "{{ aodh_galera_database }}"
-      when: inventory_hostname == groups['aodh_all'][0]
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - aodh
BREAKS HERE
-nova_spice_html5proxy_base_proto: http
-nova_novncproxy_proto: http
BREAKS HERE
-- name: Retrieve CA Certificate
-  set_fact:
-    ca_file: "{{ lookup('file', letsencrypt_ca_file_path) }}"
BREAKS HERE
-    state: latest
BREAKS HERE
-    # eg: /home/zuul/src/git.openstack.org/keystone
BREAKS HERE
-  shell: echo "{{ service_vip }} netmaster" >> /etc/hosts
BREAKS HERE
-      # The $HOME directory is mocked to work with tox
-      #  by defining the 'ansible_env' hash. This should
-      #  NEVER be done outside of testing.
-      ansible_env:  ## NEVER DO THIS OUTSIDE OF TESTING
-        HOME: "/tmp"
BREAKS HERE
-      when: cinder_create | changed
-      when: cinder_create | changed
-      when: cinder_create | changed
-      when: cinder_create | changed
BREAKS HERE
-- name: /etc/postfix/main.cf
-  copy: src={{ item }} dest=/etc/postfix/main.cf
-  with_first_found:
-    - "{{ postfix_maincf }}"
-    - "postfix/main.cf/main.cf.{{ ansible_fqdn }}"
-    - "postfix/main.cf/main.cf.{{ host_group }}"
-    - "postfix/main.cf/main.cf.{{ postfix_group }}"
-    - "postfix/main.cf/main.cf.{{ datacenter }}"
-    - "postfix/main.cf/main.cf"
-  notify:
-  - restart postfix
-  tags:
-  - postfix
-  - config
-  - base
-
-- name: install /etc/postfix/master.cf file
-  copy: src={{ item }} dest=/etc/postfix/master.cf mode=0644
-  with_first_found:
-    - "{{ postfix_mastercf }}"
-    - "postfix/master.cf/master.cf.{{ ansible_fqdn }}"
-    - "postfix/master.cf/master.cf.{{ inventory_hostname }}"
-    - "postfix/master.cf/master.cf.{{ host_group }}"
-    - "postfix/master.cf/master.cf.{{ postfix_group }}"
-    - "postfix/master.cf/master.cf"
-  when: inventory_hostname.startswith('smtp-mm')
-  notify:
-  - restart postfix
-  tags:
-  - postfix
-  - config
-  - base
-
-- name: enable postfix to start
-  service: name=postfix state=running enabled=true
-  tags:
-  - service
-  - base
-
-- name: install /etc/postfix/transport file
-  copy: src="postfix/{{ postfix_transport_filename }}" dest=/etc/postfix/transport
-  when: inventory_hostname.startswith(('smtp-mm','bastion'))
-  notify:
-  - restart postfix
-  - rebuild postfix transport
-  tags:
-  - postfix
-  - base
-  - config
BREAKS HERE
-# TEMPORARY TASK TO STORE THE ROLLBACK PROCEDURE
-# rollback scheduler task if the playbook runs succesfully
BREAKS HERE
-- include_tasks: mq_setup.yml
-  with_items:
-    - oslomsg_setup_host: "{{ keystone_oslomsg_rpc_setup_host }}"
-      oslomsg_userid: "{{ keystone_oslomsg_rpc_userid }}"
-      oslomsg_password: "{{ keystone_oslomsg_rpc_password }}"
-      oslomsg_vhost: "{{ keystone_oslomsg_rpc_vhost }}"
-      oslomsg_transport: "{{ keystone_oslomsg_rpc_transport }}"
-    - oslomsg_setup_host: "{{ keystone_oslomsg_notify_setup_host }}"
-      oslomsg_userid: "{{ keystone_oslomsg_notify_userid }}"
-      oslomsg_password: "{{ keystone_oslomsg_notify_password }}"
-      oslomsg_vhost: "{{ keystone_oslomsg_notify_vhost }}"
-      oslomsg_transport: "{{ keystone_oslomsg_notify_transport }}"
-  no_log: true
-  tags:
BREAKS HERE
-    local_action: add_host hostname=$item groupname=myvms_new
-    with_items: mlist.list_vms
-    when: not ansible_domain.startswith('stg')
-    when: ansible_domain.startswith('stg')
BREAKS HERE
-    - name: Configure MySQL user
-      include: common-tasks/mysql-db-user.yml
-      vars:
-        user_name: "{{ keystone_galera_user }}"
-        password: "{{ keystone_container_mysql_password }}"
-        login_host: "{{ keystone_galera_address }}"
-        db_name: "{{ keystone_galera_database }}"
-      run_once: yes
-
BREAKS HERE
-  command: "true"
-      - notimplemented
-  command: "true"
-      - notimplemented
BREAKS HERE
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      vars:
-        user_name: "{{ heat_galera_user }}"
-        password: "{{ heat_container_mysql_password }}"
-        login_host: "{{ heat_galera_address }}"
-        db_name: "{{ heat_galera_database }}"
-      when: inventory_hostname == groups['heat_all'][0]
-
-  vars_files:
-    - defaults/repo_packages/openstack_services.yml
-  vars:
-    heat_galera_user: heat
-    heat_galera_database: heat
-    heat_galera_address: "{{ galera_address }}"
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - heat
BREAKS HERE
-        hostvars[mon_host]['ansible_hostname'] in (ceph_health_raw.stdout | default({}) |  from_json)["quorum_names"] or
-        hostvars[mon_host]['ansible_fqdn'] in (ceph_health_raw.stdout | default({}) | from_json)["quorum_names"]
-        hostvars[mon_host]['ansible_hostname'] in (ceph_health_raw.stdout | default({}) | from_json)["quorum_names"] or
-        hostvars[mon_host]['ansible_fqdn'] in (ceph_health_raw.stdout | default({}) | from_json)["quorum_names"]
BREAKS HERE
-  copy: src="provision/files" dest="/home/copr/provision/files"
BREAKS HERE
-    - name: Create Graylog role
-      graylog_roles:
-        action: create
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        name: "ansible_role"
-        description: "Ansible test role"
-        permissions:
-          - "dashboards:read"
-        read_only: "true"
-
-    - name: Update Graylog role
-      graylog_roles:
-        action: update
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        name: "ansible_role"
-        description: "Ansible test role update"
-        permissions:
-          - "dashboards:read"
-        read_only: "true"
-
-    - name: List Graylog roles
-      graylog_roles:
-        action: list
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-      register: graylog_roles
-
-    - name: Print roles
-      debug:
-        msg: "{{ graylog_roles }}"
-
-    - name: Create Graylog user
-      graylog_users:
-        action: create
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        username: "ansible_user"
-        full_name: "Ansible User"
-        password: "{{ password }}"
-        email: "old-email@aol.com"
-        roles:
-          - "ansible_role"
-
-    - name: Update Graylog user
-      graylog_users:
-        action: update
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        username: "ansible_user"
-        email: "new-email@aol.com"
-
-    - name: List Graylog users
-      graylog_users:
-        action: list
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-      register: graylog_users
-
-    - name: Print users
-      debug:
-        msg: "{{ graylog_users }}"
-
-    - name: Remove user
-      graylog_users:
-        action: delete
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        username: "ansible_user"
-
-    - name: Remove role
-      graylog_roles:
-        action: delete
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        name: "ansible_role"
-
-    - name: List streams
-      graylog_streams:
-        action: list
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-
-    - name: Create stream
-      graylog_streams:
-        action: create
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        title: "test_stream"
-        description: "Windows and IIS logs"
-        matching_type: "AND"
-        remove_matches_from_default_stream: False
-        rules:
-          - {"field":"message","type":1,"value":"test_stream rule","inverted": false,"description":"test_stream rule"}
-
-    - name: Get stream from stream name query
-      graylog_streams:
-        action: query_streams
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        stream_name: "test_stream"
-      register: stream
-
-    - name:  List single stream by ID
-      graylog_streams:
-        action: list
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        stream_id: "{{ stream.json.id }}"
-
-    - name: Update stream
-      graylog_streams:
-        action: update
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        stream_id: "{{ stream.json.id }}"
-        remove_matches_from_default_stream: True
-
-    - name: Create stream rule
-      graylog_streams:
-        action: create_rule
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        stream_id: "{{ stream.json.id }}"
-        description: "Windows Security Logs"
-        field: "winlogbeat_log_name"
-        type: "1"
-        value: "Security"
-        inverted: False
-      register: rule
-
-    - name: Update stream rule
-      graylog_streams:
-        action: update_rule
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        stream_id: "{{ stream.json.id }}"
-        rule_id: "{{ rule.json.streamrule_id }}"
-        description: "Windows Security and Application Logs"
-
-    - name: Delete stream rule
-      graylog_streams:
-        action: delete_rule
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        stream_id: "{{ stream.json.id }}"
-        rule_id: "{{ rule.json.streamrule_id }}"
-
-    - name: Create pipeline rule
-        action: create_rule
-        title: "test_rule"
-        description: "test"
-
-    - name: Create pipeline
-      graylog_pipelines:
-        action: create
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        title: "test_pipeline"
-        source: |
-          pipeline "test_pipeline"
-          stage 0 match either
-          end
-        description: "test_pipeline description"
-
-    - name: Get pipeline from pipeline name query_pipelines
-      graylog_pipelines:
-        action: query_pipelines
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        pipeline_name: "test_pipeline"
-      register: pipeline
-
-    - name: Update pipeline with rule
-      graylog_pipelines:
-        action: update
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        pipeline_id: "{{ pipeline.json.id }}"
-        source: |
-          pipeline "test_pipeline"
-          stage 0 match either
-          rule "test_rule_domain_threat_intel"
-          end
-
-    - name: Create Stream connection to processing pipeline
-      graylog_pipelines:
-        action: create_connection
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        pipeline_id: "{{ pipeline.json.id }}"
-        stream_ids:
-          - "{{ stream.json.id }}"
-
-    - name: Delete pipeline
-      graylog_pipelines:
-        action: delete
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        pipeline_id: "{{ pipeline.json.id }}"
-
-    - name: Delete pipeline rule
-      graylog_pipelines:
-        action: delete_rule
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        rule_id: "{{ pipeline_rule.json.id }}"
-
-    - name: Get all index sets
-      graylog_index_sets:
-        action: list
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-      register: index_sets
-
-    - name: Create index set
-      graylog_index_sets:
-        action: create
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        title: "test_index_set"
-        index_prefix: "test_index_"
-        description: "test index set"
-
-    - name: Get index set by name
-      graylog_index_sets:
-        action: query_index_sets
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        title: "test_index_set"
-      register: index_set
-
-    - name: Update stream with index set
-      graylog_streams:
-        action: update
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        stream_id: "{{ stream.json.id }}"
-        index_set_id: "{{ index_set.json.id }}"
-
-    - name: Delete stream
-      graylog_streams:
-        action: delete
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        stream_id: "{{ stream.json.id }}"
-
-    - name: Delete new index set
-      graylog_index_sets:
-        action: delete
-        endpoint: "{{ endpoint }}"
-        graylog_user: "{{ graylog_user }}"
-        graylog_password: "{{ graylog_password }}"
-        index_set_id: "{{ index_set.json.id }}"
BREAKS HERE
-  tags:
-    - prep
-  tags:
-    - prep
BREAKS HERE
-    panos_query:
-      source_ip: '10.0.0.1'
-      destination_ip: '1.2.3.4'
-      destination_port: '8889'
-      protocol: 'tcp'
BREAKS HERE
-  - import_tasks: "{{ tasks_path }}/yumrepos.yml"
BREAKS HERE
-    keys_tmp: "{{ keys_tmp|default([]) + [ { 'key': item.key, 'name': item.name, 'caps': { 'mon': item.mon_cap, 'osd': item.osd_cap|default(''), 'mds': item.mds_cap|default(''), 'mgr': item.mgr_cap|default('') } , 'mode': item.mode } ] }}"
BREAKS HERE
-  tags: dovecot
-    src: 10-master.conf
-    dest: /etc/dovecot/conf.d/10-master.conf
BREAKS HERE
-          name: "{{ zfs_package[ansible_distribution | lower] }}"
BREAKS HERE
-  - include: "{{ tasks }}/openvpn_client.yml" when: datacenter != 'phx2'
BREAKS HERE
-        - tmux send-keys "openstack-ansible setup-everything.yml" C-m
BREAKS HERE
-        acme_templates: "{{ core_apps | json_query(\"[?name=='acme'].services[0].templates[]\") | list }}"
-        force: true
-        force: true
-      ignore_errors: true
BREAKS HERE
-# NOTE: most of these vars come from group_vars/badges-web* or from hostvars
BREAKS HERE
-      package: name=curl state=latest
BREAKS HERE
-    - copy: dest={{playbook_dir}}/roles/showfile/tasks/testfile content='in role tasks'
-    - set_fact: role_out="in role files" play_out="in files" stage='setup'
-
-- name: remove from role/files
-  hosts: testhost
-  connection: local
-  gather_facts: false
-  tasks:
-    - file: path={{playbook_dir}}/roles/showfile/files/testfile state=absent
-    - set_fact: role_out="in role tasks" play_out="in files" stage='remove 1'
-
-- name: remove from role/tasks
-  hosts: testhost
-  connection: local
-  gather_facts: false
-  tasks:
-    - file: path={{playbook_dir}}/roles/showfile/tasks/testfile state=absent
-    - set_fact: role_out="in files" play_out="in files" stage='remote 2'
-
-- name: remove from role dir
-  hosts: testhost
-  connection: local
-  gather_facts: false
-  tasks:
-    - file: path={{playbook_dir}}/roles/showfile/testfile state=absent
-    - set_fact: role_out="in files" play_out="in files" stage='remove 3'
-
-- name: remove from play/files
-  hosts: testhost
-  connection: local
-  gather_facts: false
-  tasks:
-    - file: path={{playbook_dir}}/files/testfile state=absent
-    - set_fact: role_out="in local" play_out="in local" stage='remove 4'
BREAKS HERE
-  # XXX: fix this.  The above task should work just fine in a chroot, but
-  # fails for some reason when the chroot is also a container.
-  ignore_errors: "{{ rhel7stig_system_is_chroot and rhel7stig_system_is_container }}"
-  when: rhel_07_040520
BREAKS HERE
-    - include: ensure-rabbitmq.yml
-      vhost_name: "{{ neutron_rabbitmq_vhost }}"
-      user_name: "{{ neutron_rabbitmq_userid }}"
-      user_password: "{{ neutron_rabbitmq_password }}"
BREAKS HERE
-  tags: dovecot
-  tags: dovecot
-  tags: dovecot
BREAKS HERE
-    docker_exec_client_cmd: docker run -v /etc/ceph:/etc/ceph --entrypoint /usr/bin/{{ docker_exec_client_cmd_binary }} {{ ceph_docker_registry}}/{{ ceph_docker_image }}:{{ ceph_docker_image_tag }}
BREAKS HERE
-#   laptop: ansible-playbook ansible.yml -K --skip-tags "desktop"
-#   workstation: ansible-playbook ansible.yml -K
-# Install without Skype and slack
-#   laptop: ansible-playbook ansible.yml -K --skip-tags "desktop, flatpak-avoidable"
-#   workstation: ansible-playbook ansible.yml -K --skip-tags "flatpak-avoidable"
-        state: directory
BREAKS HERE
-        product_version: "{{ install.version }}"
-        when: install.version|openstack_distribution == 'RDO'
BREAKS HERE
-      validate: sshd -t -f %s
BREAKS HERE
-    - cassandra    
BREAKS HERE
-    - role: solr
-      tags: ['solr']
-      when: ansible_distribution == 'Debian' and ansible_distribution_major_version < '9'
-      when: ansible_distribution == 'Debian' and ansible_distribution_major_version >= '9'
BREAKS HERE
-   - /srv/web/infra/ansible/vars/global.yml
-   - "/srv/private/ansible/vars.yml"
-   - /srv/web/infra/ansible/vars/{{ ansible_distribution }}.yml
-  - base
-  - rkhunter
-  - nagios/client
-  - hosts
-  - fas_client
-  - collectd/base
-  - rsyncd
-  - sudo
-  - include: "{{ tasks }}/yumrepos.yml"
-  - include: "{{ tasks }}/2fa_client.yml"
-  - include: "{{ tasks }}/motd.yml"
-   - /srv/web/infra/ansible/vars/global.yml
-   - "/srv/private/ansible/vars.yml"
-   - /srv/web/infra/ansible/vars/{{ ansible_distribution }}.yml
-   - /srv/web/infra/ansible/vars/global.yml
-   - "/srv/private/ansible/vars.yml"
-   - /srv/web/infra/ansible/vars/{{ ansible_distribution }}.yml
-   - /srv/web/infra/ansible/vars/global.yml
-   - "/srv/private/ansible/vars.yml"
-   - /srv/web/infra/ansible/vars/{{ ansible_distribution }}.yml
BREAKS HERE
-rabbitmq_install_method: "file"
BREAKS HERE
-    - keystone_idp != {}
-    - keystone_idp == {}
BREAKS HERE
-            - (install.version|default(undercloud_version)|openstack_distribution) == 'OSP'
-            - (install.version|default(undercloud_version)|openstack_distribution) == 'OSP'
BREAKS HERE
-- include: repo-setup-virthost.yml
BREAKS HERE
-    - name: Pre-cache the keystone git repository
-    - name: Pre-cache the tempest git repository
BREAKS HERE
-        destination_port: '223'
BREAKS HERE
-        -  name: 'http_proxy'
-           value: 'my_http_proxyi4'
-           #section: 'cluster_settings3'
-      verify_ssl: False
BREAKS HERE
-  hosts: "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_support') | replace('-', '_') }}"
BREAKS HERE
-  hosts: localhost
-      name: common
-      item.metadata.annotations['idling.alpha.openshift.io/previous-scale']|trim() != ""
BREAKS HERE
-# test code for the win_feature module
-# (c) 2014, Chris Church <chris@ninemoreminutes.com>
-
-# This file is part of Ansible
-#
-# Ansible is free software: you can redistribute it and/or modify
-# it under the terms of the GNU General Public License as published by
-# the Free Software Foundation, either version 3 of the License, or
-# (at your option) any later version.
-#
-# Ansible is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-# GNU General Public License for more details.
-#
-# You should have received a copy of the GNU General Public License
-# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
-
-
-- name: check whether servermanager module is available (windows 2008 r2 or later)
-  raw: PowerShell -Command Import-Module ServerManager
-  register: win_feature_has_servermanager
-  ignore_errors: true
-
-
-- name: start with feature absent
-  win_feature:
-    name: "{{ test_win_feature_name }}"
-    state: absent
-  when: (win_feature_has_servermanager|success) and (ansible_powershell_version is defined) and (ansible_powershell_version >= 5)
-
-- name: Invoke DSC with check mode
-  win_dsc:
-    resource_name: windowsfeature
-    name: "{{ test_win_feature_name }}"
-  check_mode: yes
-  register: win_dsc_checkmode_result
-  when: (win_feature_has_servermanager|success) and (ansible_powershell_version is defined) and (ansible_powershell_version >= 5)
-
-- name: check result of Invoke DSC with check mode
-  assert:
-    that:
-      - "win_dsc_checkmode_result|changed"
-      - "win_dsc_checkmode_result|success"
-  when: (win_feature_has_servermanager|success) and (ansible_powershell_version is defined) and (ansible_powershell_version >= 5)
-
-- name: Make sure the feature is still absent
-  win_dsc:
-    resource_name: windowsfeature
-    name: "{{ test_win_feature_name }}"
-    ensure: absent
-  register: win_dsc_1_result
-  when: (win_feature_has_servermanager|success) and (ansible_powershell_version is defined) and (ansible_powershell_version >= 5)
-
-- name: check result of Make sure the feature is still absent
-  assert:
-    that:
-      - "not win_dsc_1_result|changed"
-      - "win_dsc_1_result|success"
-  when: (win_feature_has_servermanager|success) and (ansible_powershell_version is defined) and (ansible_powershell_version >= 5)
-
-- name: Install feature for realz
-  win_dsc:
-    resource_name: windowsfeature
-    name: "{{ test_win_feature_name }}"
-  register: win_dsc_2_result
-  when: (win_feature_has_servermanager|success) and (ansible_powershell_version is defined) and (ansible_powershell_version >= 5)
-
-- name: check result of Install feature for realz
-  assert:
-    that:
-      - "win_dsc_2_result|changed"
-      - "win_dsc_2_result|success"
-  when: (win_feature_has_servermanager|success) and (ansible_powershell_version is defined) and (ansible_powershell_version >= 5)
-
-- name: Ensure clean failure
-  win_dsc:
-    resource_name: some_unknown_resource_that_wont_ever_exist
-    name: "{{ test_win_feature_name }}"
-  register: win_dsc_3_result
-  ignore_errors: true
-  when: (win_feature_has_servermanager|success) and (ansible_powershell_version is defined) and (ansible_powershell_version >= 5)
-
-- name: check result of Ensure clean failure
-  assert:
-    that:
-      - "not win_dsc_3_result|changed"
-      - "not win_dsc_3_result|success"
-  when: (win_feature_has_servermanager|success) and (ansible_powershell_version is defined) and (ansible_powershell_version >= 5)
-
-- name: Make sure the feature is absent
-  win_dsc:
-    resource_name: windowsfeature
-    name: "{{ test_win_feature_name }}"
-    ensure: absent
-  register: win_dsc_4_result
-  when: (win_feature_has_servermanager|success) and (ansible_powershell_version is defined) and (ansible_powershell_version >= 5)
-
-- name: check result of Make sure the feature is absent
-  assert:
-    that:
-      - "win_dsc_4_result|changed"
-      - "win_dsc_4_result|success"
-  when: (win_feature_has_servermanager|success) and (ansible_powershell_version is defined) and (ansible_powershell_version >= 5)
-
-- name: Make sure the feature is still absent with no changes
-  win_dsc:
-    resource_name: windowsfeature
-    name: "{{ test_win_feature_name }}"
-    ensure: absent
-  register: win_dsc_5_result
-  when: (win_feature_has_servermanager|success) and (ansible_powershell_version is defined) and (ansible_powershell_version >= 5)
-
-- name: check result of Make sure the feature is absent
-  assert:
-    that:
-      - "not win_dsc_5_result|changed"
-      - "win_dsc_5_result|success"
-  when: (win_feature_has_servermanager|success) and (ansible_powershell_version is defined) and (ansible_powershell_version >= 5)
BREAKS HERE
-        when: product.repo.type in ['poodle']
BREAKS HERE
-- name: Check init system
-  command: cat /proc/1/comm
-  changed_when: false
-  register: _pid1_name
-  tags:
-    - always
-
-- name: Set the name of pid1
-  set_fact:
-    pid1_name: "{{ _pid1_name.stdout }}"
-  tags:
-    - always
-
BREAKS HERE
-      command: detroy
BREAKS HERE
-      - osrelease is versionCompare("3.10", "<")
-      - osrelease is versionCompare("3.10", ">=")
-      - osrelease is versionCompare("3.10", ">=")
BREAKS HERE
-  - { role: docker_setup, when: not not hostvars['bastion']['no_docker_storage_setup'], device: '/dev/vdb'}
BREAKS HERE
-    - name: Intentionally remote the tempest repo remote origin
-      shell: "git remote remove origin"
-      args:
-        chdir: "/var/www/repo/openstackgit/tempest"
-
BREAKS HERE
-    - name: Set SSL Certificate to self signed certificate if no certificate file specified
-      set_fact:
-        qe_quay_ssl_cert_file: "/tmp/ssl.cert"
-      when:
-        - qe_quay_ssl_cert_file is not defined or qe_quay_ssl_cert_file|trim == ""
-      block:
-      - name: Gather facts from machine
-        setup:
-        with_items:
-          - "{{ groups['quay_enterprise'] }}"
-      - name: Install Clair
-        include_role:
-          name: ../../roles/config-clair
-        vars:
-          database_host: "{{ qe_quay_clair_hostname }}"
-          quay_enterprise_address: "{{ (qe_quay_ssl_enable|bool)| ternary('https','http') }}://{{ qe_quay_hostname }}.{{ subdomain_base }}"
-          clair_ssl_trust_configure: "{{ qe_quay_ssl_enable|d(False)|bool }}"
-          clair_ssl_trust_src_file: "{{ qe_quay_ssl_cert_file }}"
-          postgresql_username: "{{ qe_clair_database_username }}"
-          postgresql_password: "{{ qe_clair_database_password }}"
-          postgresql_port: "{{ qe_clair_database_port | d('5433') }}"
-      when:
-        - groups['clair']| length > 0
-        - qe_quay_clair_enable|d(False)|bool
BREAKS HERE
-            ~{{ ansible_ssh_user }}/deploy_info.sh 2>&1 | tee -a /var/log/extra/deploy_resources.log
BREAKS HERE
-  - name: restart httpd on the koji-hubs.
BREAKS HERE
-CONDITINAL_BARE_VARS:
-    - With this setting on (True), runing conditional evaluation 'var' is treated differently 'var.subkey' as the first is evaluted
-      directly while the second goes though the Jinja2 parser. But 'false' strings in 'var' get evaluated as booleans.
-    - With this settting off they both evalutate the same but in cases in which 'var' was 'false' (a string) it won't get evaluated as a boolean anymore.
BREAKS HERE
-      shell: "aws route53 change-resource-record-sets --hosted-zone-id {{internal_zone_id}}  --change-batch file://../../workdir/internal_dns-{{ env_type }}-{{ guid }}.json --region={{aws_region}}"
BREAKS HERE
-      // These are the basic imports that Jenkin's interactive script console
-      // Enable Jenkins' slave agent port, because most everyone will want it.
-      def externalUrl = "{{ '' if jenkins_url_external == None else (jenkins_url_external | default('') | trim) }}"
-      if (!externalUrl.equals(locationConfig.url)) {
-  changed_when: "(shell_jenkins_config_misc | success) and 'Changed' not in shell_jenkins_config_misc.stdout"
BREAKS HERE
-            command: "virt-sparsify --compress /var/lib/libvirt/images/{{ undercloud }}-disk1.qcow2 {{ undercloud_image_file }}"
BREAKS HERE
-      - plugin: lmenezes/elasticsearch-kopf
-        version: master
BREAKS HERE
-    - include: common/create-grant-db.yml
-      db_name: "{{ designate_galera_user }}"
-      db_password: "{{ designate_galera_password }}"
BREAKS HERE
-  vars_files:
-  - vars/guests.yml
-  - name: Add bastion to dynamic host
-    add_host:
-      name: bastion
-      group: bast
-  - name: Add other hosts to nodes group
-    add_host:
-      name: "{{item.name}}"
-      group: nodes
-    when: item.name != 'bastion'
-    with_items: "{{guests}}"
BREAKS HERE
-  negative_cache_enabled: true # Nexus gui default. For proxies only
-  negative_cache_ttl: 1440 # Nexus gui default. For proxies only
-  negative_cache_enabled: true # Nexus gui default. For proxies only
-  negative_cache_ttl: 1440 # Nexus gui default. For proxies only
-  negative_cache_enabled: true # Nexus gui default. For proxies only
-  negative_cache_ttl: 1440 # Nexus gui default. For proxies only
-  negative_cache_enabled: true # Nexus gui default. For proxies only
-  negative_cache_ttl: 1440 # Nexus gui default. For proxies only
-  negative_cache_enabled: true # Nexus gui default. For proxies only
-  negative_cache_ttl: 1440 # Nexus gui default. For proxies only
-  negative_cache_enabled: true # Nexus gui default. For proxies only
-  negative_cache_ttl: 1440 # Nexus gui default. For proxies only
-  negative_cache_enabled: true # Nexus gui default. For proxies only
-  negative_cache_ttl: 1440 # Nexus gui default. For proxies only
-  negative_cache_enabled: true # Nexus gui default. For proxies only
-  negative_cache_ttl: 1440 # Nexus gui default. For proxies only
-  negative_cache_enabled: true # Nexus gui default. For proxies only
-  negative_cache_ttl: 1440 # Nexus gui default. For proxies only
BREAKS HERE
-  until: install_packages|success
-  until: remove_packages|success
BREAKS HERE
-- name: "MEDIUM | RHEL-07-040680 | AUDIT | The system must be configured to prevent unrestricted mail relaying."
-  command: "postconf -n smtpd_client_restrictions"
-  changed_when: no
-  register: rhel_07_040680_audit
-  tags:
-      - cat2
-      - medium
-      - audit
-      - RHEL-07-040680
-- name: "MEDIUM | RHEL-07-040680 | PATCH | The system must be configured to prevent unrestricted mail relaying."
-  command: "postconf -e 'smtpd_client_restrictions=permit_mynetworks, reject'"
-  when:
-    - rhel_07_040680
-    - rhel_07_040680_audit.stdout != 'smtpd_client_restrictions = permit_mynetworks, reject'
BREAKS HERE
----
- - name: Get date
-   hosts: localhost
-  
-   tasks:
-   
-   - name: date1
-     debug: msg="{{ lookup('pipe', 'date') }}"
-     
-   - name: date2
-     debug: msg="{{ lookup('pipe', 'date +%m%d-%H%M%S') }}"
-
-
-
-
BREAKS HERE
-security_require_sha512_password_storage: yes                # RHEL-07-010180
-# Set a maximum lifetime limit for user passwords.
-security_create_home_directory_default: yes                  # RHEL-07-020630
BREAKS HERE
-  - agents
BREAKS HERE
-    - { role: ovb-manage-stack, ovb_manage_stack_mode: 'create' }
-
BREAKS HERE
-    SSLCertificateChainFile: "{{wildcard_int_name}}"
BREAKS HERE
-      # create shade venv on target machine
-      # TODO(yfried): remove this if/when packstack uses floating ips and localhost can access cloud
-      - block:
-          - name: install virtualenv requirments for pip
-            become: yes
-            package:
-                name: "{{ item }}"
-                state: present
-            with_items:
-               - libffi-devel
-               - python-virtualenv
-               - gcc
-               - python-devel
-
-          - name: create shade virtualenv
-            pip:
-                name: "{{ item.key }}"
-                version: "{{ item.value }}"
-                virtualenv: "{{ path_venv }}"
-            with_dict:
-                setuptools: "17.0"
-                pytz: "2016.4"
-                shade: "1.19.0"
-                pip: "9.0.1"
-        vars:
-            path_venv: "/var/tmp/venv_shade"
-
BREAKS HERE
-    - name: Cinder extra lxc config
-      lxc_container:
-        name: "{{ container_name }}"
-        container_config:
-          - "lxc.aa_profile=unconfined"
-          - "lxc.cgroup.devices.allow=a *:* rmw"
-      delegate_to: "{{ physical_host }}"
-      when: (is_metal == false or is_metal == "False") and inventory_hostname not in groups['cinder_volume']
-      tags:
-        - cinder-container-setup
-          - "lxc.aa_profile=unconfined"
-        (is_metal == false or is_metal == "False") and
-      when: is_metal == true or is_metal == "True"
-      when: is_metal == true or is_metal == "True"
BREAKS HERE
-      - name: check if custom overcloud_deploy.sh exists
-      - name: copy our overcloud deployment script if provided by the user
-        tags: overcloud_deploy
-      - name: append extra templates provided by user
-        include_tasks: tasks/environment_templates.yml
-        with_items: "{{ install.overcloud.templates|default([]) }}"
-        loop_control:
-            loop_var: templates_list_file
-        # avoid empty strings
-        when: "templates_list_file != ''"
-
-      - name: add environment plan to deply script
BREAKS HERE
-      apt_package_pinning_file_name: test.pref
-      apt_pinned_packages:
-        - { package: "test-package-version", version: "9.9.9-version" }
-        - { package: "test-package-origin", origin: "test-origin.org" }
-        - { package: "test-package-release.*", release: "TestRelease" }
BREAKS HERE
-  hosts: local:!manual:!skip_provision
BREAKS HERE
-  hosts: people02.fedoraproject.org
-  accelerate: "{{ accelerated }}"
-
-  hosts: people02.fedoraproject.org
-  accelerate: "{{ accelerated }}"
-  - { role: denyhosts, when: ansible_distribution_major_version|int != 7 }
-  - { role: collectd/fedmsg-service, process: fedmsg-hub }
-  - apache
-  - git/hooks
-  - git/make_checkout_seed
-  - git/server
-  - gitolite/base
-  - gitolite/check_fedmsg_hooks
-  - distgit
- 
-- name: setup fedmsg on people
-  hosts: people02.fedoraproject.org
-  user: root
-  gather_facts: True
-
-  vars_files:
-  - /srv/web/infra/ansible/vars/global.yml
-  - "/srv/private/ansible/vars.yml"
-  - /srv/web/infra/ansible/vars/{{ ansible_distribution }}.yml
-
-  roles:
-  - fedmsg/base
-  - fedmsg/hub
-
-  handlers:
-  - include: "{{ handlers }}/restart_services.yml"
BREAKS HERE
-  with_items: "{{ keys }}"
BREAKS HERE
-  gather_facts: "{{ gather_facts | default(True) }}"
BREAKS HERE
-    register: public_ip
BREAKS HERE
-        haproxy_backend: "{{ item }}-back"
-        haproxy_backend: "{{ item }}-back"
BREAKS HERE
-# This means that the firewall rules will be updated
-# for machines which were not provisioned by ansible too.
BREAKS HERE
-    - include: common-tasks/oslomsg-rpc-vhost-user.yml
-      static: no
-      vars:
-        rpc_user: "{{ magnum_oslomsg_rpc_userid }}"
-        rpc_password: "{{ magnum_oslomsg_rpc_password }}"
-        rpc_vhost: "{{ magnum_oslomsg_rpc_vhost }}"
-      when:
-        - inventory_hostname == groups['magnum_all'][0]
-        - groups[magnum_oslomsg_rpc_host_group] | length > 0
BREAKS HERE
-     - name: Set all app owners to have cluster-monitoring-view
BREAKS HERE
-      always_run: true
-- hosts: mdss:rgws:clients 
BREAKS HERE
-# ansible-playbook shrink-osd.yml -e osd_id=0,2,6
-  gather_facts: false
-  tasks:
-  - include_vars: roles/ceph-defaults/defaults/main.yml
-  - include_vars: group_vars/all.yml
-
-  - name: exit playbook, if user did not mean to shrink cluster
-    fail:
-      msg: "Exiting shrink-osd playbook, no osd(s) was/were removed..
-         To shrink the cluster, either say 'yes' on the prompt or
-         or use `-e ireallymeanit=yes` on the command line when
-         invoking the playbook"
-    when: ireallymeanit != 'yes'
-
-  - name: exit playbook, if no osd(s) was/were given
-    fail:
-      msg: "osd_ids must be declared
-        Exiting shrink-osd playbook, no OSD()s was/were removed.
-         On the command line when invoking the playbook, you can use
-         -e osd_ids=0,1,2,3 argument."
-    when: osd_ids is not defined
-
-  - name: test if ceph command exist
-    command: command -v ceph
-    changed_when: false
-    failed_when: false
-    register: ceph_command
-
-  - name: exit playbook, if ceph command does not exist
-    debug:
-      msg: "The ceph command is not available, please install it :("
-    run_once: true
-    when:
-      - ceph_command.rc != 0
-
-  - name: exit playbook, if cluster files do not exist
-    stat:
-      path: "{{ item }}"
-    register: ceph_conf_key
-    with_items:
-      - /etc/ceph/{{ cluster }}.conf
-      - /etc/ceph/{{ cluster }}.client.admin.keyring
-    failed_when: false
-
-  - fail:
-      msg: "Ceph's configuration file is not present in /etc/ceph"
-    with_items: "{{ ceph_conf_key.results }}"
-    when:
-      -  item.stat.exists == false
-
-  - name: exit playbook, if can not connect to the cluster
-    command: timeout 5 ceph --cluster {{ cluster }} health
-    register: ceph_health
-    until: ceph_health.stdout.find("HEALTH") > -1
-    retries: 5
-    delay: 2
-
-# NOTE (leseb): just in case, the complex filters mechanism below does not work anymore.
-# This will be a quick and easy fix but will require using the shell module.
-#  - name: find the host where the osd(s) is/are running on
-#    shell: |
-#      ceph --cluster {{ cluster }} osd find {{ item }} | grep -Po '(?<="ip": ")[^:]*'
-#    with_items: "{{osd_ids.split(',')}}"
-#    register: osd_hosts
-#
-  - name: find the host where the osd(s) is/are running on
-    command: ceph --cluster {{ cluster }} osd find {{ item }}
-    with_items: "{{ osd_ids.split(',') }}"
-    register: osd_hosts
-
-  - set_fact: ip_item="{{(item.stdout | from_json).ip}}"
-    with_items: "{{ osd_hosts.results }}"
-    register: ip_result
-
-  - set_fact: ips="{{ ip_result.results | map(attribute='ansible_facts.ip_item') | list }}"
-
-  - set_fact: real_ips="{{ ips | regex_replace(':[0-9][0-9][0-9][0-9]\/[0-9]+', '') }}"
-
-  - name: check if ceph admin key exists on the osd nodes
-    stat:
-      path: "/etc/ceph/{{ cluster }}.client.admin.keyring"
-    register: ceph_admin_key
-    with_items: "{{ real_ips }}"
-    delegate_to: "{{ item }}"
-    failed_when: false
-
-  - fail:
-      msg: "The Ceph admin key is not present on the OSD node, please add it and remove it after the playbook is done."
-    with_items: "{{ ceph_admin_key.results }}"
-    when:
-      -  item.stat.exists == false
-
-  - name: deactivating osd(s)
-    command: ceph-disk deactivate --cluster {{ cluster }} --deactivate-by-id {{ item.0 }} --mark-out
-    register: deactivate 
-    ignore_errors: yes
-    with_together:
-      - "{{ osd_ids.split(',') }}"
-      - "{{ real_ips }}"
-    delegate_to: "{{ item.1 }}"
-
-  - name: set osd(s) out when ceph-disk deactivating fail
-    command: ceph --cluster {{ cluster }} osd out osd.{{ item.0 }}
-    with_together:
-      - "{{ osd_ids.split(',') }}"
-      - "{{ deactivate.results }}"
-    when: 
-      - item.1.stderr|length > 0
-
-  - name: destroying osd(s)
-    command: ceph-disk destroy --cluster {{ cluster }} --destroy-by-id {{ item.0 }} --zap
-    register: destroy 
-    ignore_errors: yes
-    with_together:
-      - "{{ osd_ids.split(',') }}"
-      - "{{ real_ips }}"
-    delegate_to: "{{ item.1 }}"
-
-  - name: remove osd(s) from crush_map when ceph-disk destroy fail
-    command: ceph --cluster {{ cluster }} osd crush remove osd.{{ item.0 }}
-    with_together:
-      - "{{ osd_ids.split(',') }}"
-      - "{{ destroy.results }}"
-    when: 
-      - item.1.stderr|length > 0
-
-  - name: delete osd(s) auth key when ceph-disk destroy fail 
-    command: ceph --cluster {{ cluster }} auth del osd.{{ item.0 }}
-    with_together:
-      - "{{ osd_ids.split(',') }}"
-      - "{{ destroy.results }}"
-    when: 
-      - item.1.stderr|length > 0
-
-  - name: deallocate osd(s) id when ceph-disk destroy fail 
-    command: ceph --cluster {{ cluster }} osd rm {{ item.0 }}
-    with_together:
-      - "{{ osd_ids.split(',') }}"
-      - "{{ destroy.results }}"
-    when: 
-      - item.1.stderr|length > 0
BREAKS HERE
-  - name: put in hourly cron job for syncing
-    action: copy src="{{ files }}/download/cron-daily-sync.sh"  dest=/etc/cron.daily/sync-mirror.sh owner=root group=root mode=755
BREAKS HERE
-  pre_tasks:
-    - include: create-grant-db.yml
-      db_name: "{{ horizon_galera_database }}"
-      db_password: "{{ horizon_container_mysql_password }}"
-  roles:
-    - role: "{{ horizon_rolename | default('os_horizon') }}"
-  vars_files:
-    - test-vars.yml
BREAKS HERE
-          shell ip n | grep "`virsh dumpxml master01 | grep "mac address" | sed "s/.*'\(.*\)'.*/\1/g"`" | awk '{ gsub(/[\(\)]/,"",$1); print $1" {{item.name}}" }'
-      - name: Get master01 ip addr
-          shell ip n | grep "`virsh dumpxml infranode01 | grep "mac address" | sed "s/.*'\(.*\)'.*/\1/g"`" | awk '{ gsub(/[\(\)]/,"",$1); print $1" {{item.name}}" }'
BREAKS HERE
-          dashboard_from: '{{netdata_from}}'
-          badges_from: '{{netdata_from}}'
-          conf_from: '{{netdata_conf_from}}'
-          connections_from: '{{netdata_streaming_from}}'
-          streaming_from: '{{netdata_streaming_from}}'
BREAKS HERE
-      when: etcosbs_kt:_stat.stat.exists == false
BREAKS HERE
-      - name: Cnfiguring RBAC for the Dashboard service account.
BREAKS HERE
-            roles_sshd: "{{ install.version|default(undercloud_version)|openstack_release >= 11 or tht_version|version_compare('5.2.0', '>') or (tht_version|version_compare('5.2.0', '==') and tht_release|version_compare('15', '>=')) }}"
BREAKS HERE
-  - virtualenv-tools
BREAKS HERE
-  - name: add rule to new security group (ssh-from-persistent)
-          security_group: 'ssh-from-persistent-{{item.name}}'
-  - name: add rule to new security group (ssh-from-persistent)
-          security_group: 'ssh-from-persistent-{{item}}'
-  - name: add rule to new security group (ssh-from-persistent)
-  - name: add rule to new security group (ssh-from-persistent)
-  - name: add rule to new security group (ssh-from-persistent)
-  - name: add rule to new security group (ssh-from-persistent)
-  - name: add rule to new security group (ssh-from-persistent)
-  - name: add rule to new security group (ssh-from-persistent)
-  - name: add rule to new security group (ssh-from-persistent)
-  - name: add rule to new security group (ssh-from-persistent)
-  - name: add rule to new security group (fedmsg-relay-persistent)
-  - name: add rule to new security group (ssh-from-persistent)
BREAKS HERE
-          path: "{{ template_base }}/roles/roles_data.yaml"
-          regexp: "HostnameFormatDefault: '%stackname%-(.*)'"
-          replace: 'HostnameFormatDefault: "\1"'
BREAKS HERE
-    queue_name: "openqa_checkcomp{{ checkcompose_env_suffix }}"
BREAKS HERE
-# The four roles that apply to the ceph hosts will be applied: ceph-common,
-# ceph-mon, ceph-osd and ceph-mds. So any changes to configuration, package updates, etc,
-# will be applied as part of the rolling update process.
-
-# /!\ DO NOT FORGET TO CHANGE THE RELEASE VERSION FIRST! /!\
BREAKS HERE
-        lago init "{{ prefix }}" "{{ playbook_dir }}/LagoInitFile.yml"
BREAKS HERE
-- name: Group hosts by provisioner
-  hosts: local
-  sudo: no
-  tasks:
-    - group_by: key={{ provisioner.type }}
-    - group_by: key={{ provisioner.network.type }}
-  tags:
-    - provision
-
-- name: Cleanup libvirt guests from libvirt host
-  hosts: libvirt_host:&libvirt
-  gather_facts: False
-  roles:
-    - { role: cleanup_nodes/libvirt }
-
-- name: Cleanup libvirt networks from libvirt host
-  hosts: libvirt_host:&libvirt
-  gather_facts: False
-  roles:
-    - { role: cleanup_networks/libvirt }
-
-- name: Cleanup openstack nodes from nova
-  hosts: local:&openstack:&nova
-  sudo: no
-  gather_facts: False
-  roles:
-    - { role: cleanup_nodes/openstack/fip-nova }
-    - { role: cleanup_nodes/openstack/remove_nodes }
-
-- name: Cleanup openstack nodes from nova/neutron-network
-  hosts: local:&openstack:&neutron
-  sudo: no
-  gather_facts: False
-  roles:
-    - { role: cleanup_nodes/openstack/fip-neutron }
-    - { role: cleanup_nodes/openstack/remove_nodes }
-
-- name: Cleanup rax nodes from nova
-  hosts: local:&rax
-  gather_facts: False
-  roles:
-    - { role: cleanup_nodes/rax }
BREAKS HERE
-swift_service_publicuri: "{{ swift_service_proto }}://{{ external_lb_vip_address }}:{{ swift_proxy_port }}"
-swift_service_adminuri: "{{ swift_service_proto }}://{{ internal_lb_vip_address }}:{{ swift_proxy_port }}"
-swift_service_internaluri: "{{ swift_service_proto }}://{{ internal_lb_vip_address }}:{{ swift_proxy_port }}"
BREAKS HERE
-- hosts: local
-        path: "{{ change_dir }}/vagrant_variables.yml"
BREAKS HERE
-- include: neutron_check.yml
-  tags:
-    - always
-
-- include: neutron_lbaas.yml
-  tags:
-    - neutron-install
-
BREAKS HERE
-    - role: "{{ rolename | basename }}"
BREAKS HERE
-    # - name: deploy NSX Manager OVA
-    #   nsxt_deploy_ova:
-    #     ovftool_path: "/usr/bin"
-    #     datacenter: "private_dc"
-    #     datastore: "data store"
-    #     portgroup: "VM Network"
-    #     cluster: "nsxt_cluster"
-    #     vmname: "nsxt-manager"
-    #     hostname: "nsxt-manager-10"
-    #     dns_server: "20.162.244.213"
-    #     dns_domain: "eng.vmware.com"
-    #     ntp_server: "123.110.200.124"
-    #     gateway: "10.112.203.253"
-    #     ip_address: "40.112.201.24"
-    #     netmask: "255.255.224.0"
-    #     admin_password: "Admin!23Admin"
-    #     cli_password: "Admin!23Admin"
-    #     path_to_ova: "http://build-squid.eng.vmware.com/build/mts/release/bora-8411846/publish/nsx-unified-appliance/exports/ovf"
-    #     ova_file: "nsx-unified-appliance-2.2.0.0.0.8411854.ovf"
-    #     vcenter: "10.161.244.213"
-    #     vcenter_user: "administrator@vsphere.local"
-    #     vcenter_passwd: "Admin!23"
-    #     deployment_size: "small"
-    #     role: "nsx-manager"
-    # - name: Check manager status
-    #   nsxt_manager_status:
-    #       hostname: "{{hostname}}"
-    #       username: "{{username}}"
-    #       password: "{{password}}"
-    #       validate_certs: False
-    #       wait_time: 50
-    #
-    # - name: Deploy compute manager
-    #   nsxt_fabric_compute_managers:
-    #       hostname: "{{hostname}}"
-    #       username: "{{username}}"
-    #       password: "{{password}}"
-    #       validate_certs: False
-    #       display_name: "vCenter"
-    #       server: "10.161.244.213"
-    #       origin_type: vCenter
-    #       credential:
-    #         credential_type: UsernamePasswordLoginCredential
-    #         username: "administrator@vsphere.local"
-    #         password: "Admin!23"
-    #         thumbprint: "36:43:34:D9:C2:06:27:4B:EE:C3:4A:AE:23:BF:76:A0:0C:4D:D6:8A:D3:16:55:97:62:07:C2:84:0C:D8:BA:66"
-    #       state: present
-    #   register: compute_manager
-    # - name: Deploy controller
-    #   nsxt_controllers:
-    #       hostname: "{{hostname}}"
-    #       username: "{{username}}"
-    #       password: "{{password}}"
-    #       validate_certs: False
-    #       deployment_requests:
-    #       - roles:
-    #         - CONTROLLER
-    #         form_factor: "MEDIUM"
-    #         user_settings:
-    #           cli_password: "Admin!23Admin"
-    #           root_password: "Admin!23Admin"
-    #         deployment_config:
-    #           placement_type: VsphereClusterNodeVMDeploymentConfig
-    #           vc_id: "{{compute_manager.id}}"
-    #           management_network_id: "network-44"
-    #           hostname: "controller-1"
-    #           compute_id: "domain-c49"
-    #           storage_id: "datastore-43"
-    #           default_gateway_addresses:
-    #           - 11.122.203.253
-    #           management_port_subnets:
-    #           - ip_addresses:
-    #             - 11.142.201.25
-    #             prefix_length: "19"
-    #       clustering_config:
-    #         clustering_type: ControlClusteringConfig
-    #         shared_secret: "123456"
-    #         join_to_existing_cluster: false
-    #       state: present
BREAKS HERE
-      command: timeout 5 ceph --cluster {{ cluster }} health
-      command: ceph --cluster {{ cluster }} osd find {{ item }}
-    - set_fact:
-    - fail:
-      command: ceph --cluster {{ cluster }} osd out osd.{{ item.0 }}
-      command: ceph --cluster {{ cluster }} osd crush remove osd.{{ item.0 }}
-        - item.1.stderr|length > 0
-      command: ceph --cluster {{ cluster }} auth del osd.{{ item.0 }}
-        - item.1.stderr|length > 0
-      command: ceph --cluster {{ cluster }} osd rm {{ item.0 }}
-        - item.1.stderr|length > 0
-      command: ceph --cluster {{ cluster }} -s
-      command: ceph --cluster {{ cluster }} osd tree
BREAKS HERE
-    proxyurl: http://qa-prod01.qa.fedoraproject.org
-    proxyurl: http://phab.qa-prod01.qa.fedoraproject.org
-    proxyurl: http://docs.qa-prod01.qa.fedoraproject.org
BREAKS HERE
-    gluster_brick_dir: /srv/glusterfs/
-    gluster_brick_dir: /srv/glusterfs/
BREAKS HERE
-      - "{{item.stat.mode}} == 0700"
BREAKS HERE
-    - Restart uWSGI on first node
-    - Restart uWSGI on other nodes
-    - Restart web server on first node
-    - Restart web server on other nodes
-    - Restart uWSGI on first node
-    - Restart uWSGI on other nodes
-    - Restart web server on first node
-    - Restart web server on other nodes
-    - Restart uWSGI on first node
-    - Restart uWSGI on other nodes
-    - Restart web server on first node
-    - Restart web server on other nodes
BREAKS HERE
-  shell: bundle install --path http/vendor/bundle
-    chdir: "/srv/lobste.rs/http"
BREAKS HERE
-    - playbooks/test-vars.yml
-    - playbooks/test-vars.yml
BREAKS HERE
-    galaxy_hostname: "{{ lookup('env','INSTALL_HOSTNAME')|default('localhost') }}"
-    galaxy_user: "{{ lookup('env','GALAXY_USER')|default('galaxy') }}"
-    galaxy_admin: "{{ lookup('env','GALAXY_ADMIN')|default('artimed@gmail.com') }}"
-    galaxy_port: "{{ lookup('env','GALAXY_PORT')|default('8080') }}"
-    ftp_port: "{{ lookup('env','FTP_PORT')|default('21') }}"
-    galaxy_api: "{{ lookup('env','GALAXY_KEY')|default('379ab2a47d714a74b5cee4081703368e') }}"
-    galaxy_data: "{{ lookup('env','GALAXY_DATA')|default('/galaxy_data') }}"
-    galaxy_database: "{{ lookup('env','GALAXY_DATABASE')|default('/galaxy_database') }}"
-    galaxy_db: "postgresql:///{{ galaxy_user }}?host=/var/run/postgresql" # Used by galaxy roles
-    #move postgresql db to a persistent dir
-      galaxy_user_name: "{{ galaxy_user }}"      
-#      include: tools.yml
BREAKS HERE
-
-
-- name: Install Zabbix Agent on all hosts
-  hosts: "{{ ('tag_Project_' ~ env_type ~ '_' ~ guid) | replace('-', '_') }}"
-  gather_facts: False
-  become: yes
-  vars_files:
-    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
-  run_once: true
-  roles:
-    - { role: "{{ ANSIBLE_REPO_PATH }}/roles/zabbix-client" }
-  tags:
-    - env-specific
-    - install_zabbix
BREAKS HERE
-          - name: Install Test Reqs
-            pip:
-                    name: .[tests]
-                    virtualenv: /tmp/linchpin
-            args:
-                    chdir: ../../
-          - name: pyTest
-            shell: |
-                    source /tmp/linchpin/bin/activate
-                    python setup.py test
-            args:
-                    chdir: ../../
-          - name: Flake8 tests
-            shell: |
-                    source /tmp/linchpin/bin/activate
-                    flake8 --exclude=\.eggs,tests,docs --ignore=E124,E303,W504 --max-line-length 80 .
-            args:
-                    chdir: ../../
BREAKS HERE
-# The RHEL 7 STIG content was first added in the Ocata release.
-# The RHEL 6 STIG content is deprecated in the Ocata release.
-#   ____  _   _ _____ _        __     ____ _____ ___ ____
-#  |  _ \| | | | ____| |      / /_   / ___|_   _|_ _/ ___|
-#  | |_) | |_| |  _| | |     | '_ \  \___ \ | |  | | |  _
-#  |  _ <|  _  | |___| |___  | (_) |  ___) || |  | | |_| |
-#  |_| \_\_| |_|_____|_____|  \___/  |____/ |_| |___\____|
-#  The default configurations after this marker apply to the RHEL 6 STIG
-#  content in the ansible-hardening role. Review the comments below
-#  as well as the main ansible-hardening documentation:
-#    http://docs.openstack.org/developer/ansible-hardening/
-## AIDE
-# The default Ubuntu configuration for AIDE will cause it to wander into some
-# terrible places on the system, such as /var/lib/lxc and images in /opt.
-# The following three default exclusions are highly recommended for AIDE to
-# work properly, but additional exclusions can be added to this list if needed.
-security_aide_exclude_dirs:
-  - /openstack
-  - /opt
-  - /run
-  - /var
-#
-# By default, the AIDE database won't be initialized immediately since it can
-# consume plenty of CPU and I/O resources while it runs. To initialize the
-# AIDE database immediately when the playbook finishes, set the following
-# variable to 'true':
-security_initialize_aide: false
-
-## Audit daemon
-# V-38438 requires that auditd is enabled at boot time with a parameter in the
-# GRUB configuration.
-#
-# If 'security_enable_audit_during_boot' is set to 'yes', then the 'audit=1'
-# parameter will be added in /etc/default/grub.d/.
-# If 'security_enable_grub_update is set to 'yes', the grub.cfg will be
-# updated automatically.
-security_enable_audit_during_boot: yes            # V-38438
-security_enable_grub_update: yes                  # V-38438
-
-# The following booleans control the rule sets added to auditd's default
-# set of auditing rules.  To see which rules will be added for each boolean,
-# refer to the templates/osas-auditd.j2 file.
-#
-# If the template changes due to booleans being adjusted, the new template
-# will be deployed onto the host and auditd will get the new rules loaded
-# automatically with augenrules.
-#
-security_audit_account_modification: yes          # V-38531, V-38534, V-38538
-security_audit_change_localtime: yes              # V-38530
-security_audit_change_system_time: yes            # V-38635
-security_audit_clock_settime: yes                 # V-38527
-security_audit_clock_settimeofday: yes            # V-38522
-security_audit_clock_stime: yes                   # V-38525
-security_audit_DAC_chmod: no                      # V-38543
-security_audit_DAC_chown: no                      # V-38545
-security_audit_DAC_lchown: no                     # V-38558
-security_audit_DAC_fchmod: no                     # V-38547
-security_audit_DAC_fchmodat: no                   # V-38550
-security_audit_DAC_fchown: no                     # V-38552
-security_audit_DAC_fchownat: no                   # V-38554
-security_audit_DAC_fremovexattr: no               # V-38556
-security_audit_DAC_lremovexattr: no               # V-38559
-security_audit_DAC_fsetxattr: no                  # V-38557
-security_audit_DAC_lsetxattr: no                  # V-38561
-security_audit_DAC_setxattr: no                   # V-38565
-security_audit_deletions: no                      # V-38575
-security_audit_failed_access: no                  # V-38566
-security_audit_filesystem_mounts: yes             # V-38568
-security_audit_kernel_modules: yes                # V-38580
-security_audit_mac_changes: yes                   # V-38541
-security_audit_network_changes: yes               # V-38540
-security_audit_sudoers: yes                       # V-38578
-#
-# **DANGER**
-# Changing the options below can cause systems to go offline unexpectedly or
-# stop serving requests as a security precaution. Read the developer notes for
-# each STIG prior to adjusting the following variables.
-# **DANGER**
-#
-# Set an action to occur when there is a disk error. Review the
-# documentation for V-38464 before changing this option.
-security_disk_error_action: SYSLOG                # V-38464
-#
-# Set an action to occur when the disk is full. Review the documentation for
-# V-38468 before changing this option.
-security_disk_full_action: SYSLOG                 # V-38468
-#
-# V-38678 - Set the amount of megabytes left when the space_left_action
-# triggers. The STIG guideline doesn't specify a size, but Ubuntu chooses a
-# default of 75MB, which is reasonable.
-security_space_left: 75                           # V-38678
-#
-# Set an action to occur when the disk is approaching its capacity.
-# Review the documentation for V-38470 before changing this option.
-security_space_left_action: SYSLOG                # V-38470
-#
-# Set the maximum size of a rotated log file. Ubuntu's default
-# matches the STIG requirement of 6MB.
-security_max_log_file: 6                          # V 38633
-#
-# Sets the action to take when log files reach the maximum file size.
-# Review the documentation for V-38634 before changing this option.
-security_max_log_file_action: ROTATE              # V-38634
-#
-# Set the number of rotated audit logs to keep.  Ubuntu has 5 as the default
-# and this matches the STIG's requirements.
-security_num_logs: 5                              # V-38636
-#
-# Set the email address of someone who can receive and respond to notifications
-# about low disk space for log volumes.
-security_action_mail_acct: root                   # V-38680
-#
-# **IMMINENT DANGER**
-# The STIG says that the system should switch to single user mode when the
-# storage capacity gets very low. This can cause serious service disruptions
-# and should only be set to 'single' for deployers in extremely high security
-# environments. Ubuntu's default is SUSPEND, which will suspend logging.
-# **IMMENENT DANGER**
-security_admin_space_left_action: SUSPEND         # V-54381
-## Chrony (NTP) configuration
-# Install and enable chrony to sync time with NTP servers.
-security_enable_chrony: yes                       # V-38620
-# Adjust the following NTP servers if necessary.
-security_ntp_servers:
-  - 0.north-america.pool.ntp.org
-  - 1.north-america.pool.ntp.org
-  - 2.north-america.pool.ntp.org
-  - 3.north-america.pool.ntp.org
-# Chrony limits access to clients that are on certain subnets.  Adjust the
-# following subnets here to limit client access to chrony servers.
-security_allowed_ntp_subnets:
-  - 10/8
-  - 192.168/16
-  - 172.16/12
-# Listen for NTP requests only on local interfaces.
-security_ntp_bind_local_interfaces_only: yes
-## Core dumps
-# V-38675 requires disabling core dumps for all users unless absolutely
-# necessary. Set this variable to 'no' to skip this change.
-security_disable_core_dumps: yes                  # V-38675
-
-## Services
-# The STIG recommends ensuring that some services are running if no services
-# utilizing it are enabled.  Setting a boolean to 'yes' here will ensure that
-# a service isn't actively running and will not be started after boot-up.
-# Setting a 'no' will ensure that this Ansible role does not alter the service
-# in any way from its current configuration.
-#
-security_disable_abrtd: yes                       # V-38641
-security_disable_atd: yes                         # V-38640
-security_disable_autofs: yes                      # V-38437
-security_disable_avahi: yes                       # V-31618
-security_disable_bluetooth: yes                   # V-38691
-security_disable_netconsole: yes                  # v-38672
-security_disable_qpidd: yes                       # V-38648
-security_disable_rdisc: yes                       # V-38650
-security_disable_rsh: yes                         # V-38594
-security_disable_ypbind: yes                      # V-38604
-security_disable_xinetd: yes                      # V-38582
-#
-# The STIG recommends ensuring that some services aren't installed at ANY time.
-# Those services are listed here.  Setting a boolean here to 'yes' wiil
-# ensure that the STIG is followed and the service is removed.  Setting a
-# boolean to 'no' means that the playbook will not alter the service.
-#
-security_remove_ldap_server: yes                  # V-38627
-security_remove_rsh_server: yes                   # V-38591
-security_remove_sendmail: yes                     # V-38671
-security_remove_telnet_server: yes                # V-38587
-security_remove_tftp_server: yes                  # V-38606
-security_remove_xinetd: yes                       # V-38584
-security_remove_xorg: yes                         # v-38676
-security_remove_ypserv: yes                       # V-38603
-#
-# The STIG does not allow the system to run a graphical interface. Set this
-# variable to 'no' if you need a graphical interface on the server.
-security_disable_x_windows: yes                   # V-38674
-
-## SSH configuration
-# The following configuration items will adjust how the ssh daemon is
-# configured.  The recommendations from the RHEL 6 STIG are shown below, but
-# they can be adjusted to fit a particular environment.
-#
-# Set a 15 minute time out for SSH sessions if there is no activity
-security_ssh_client_alive_interval: 900           # V-38608
-#
-# Timeout ssh sessions as soon as ClientAliveInterval is reached once
-security_ssh_client_alive_count_max: 0            # V-38610
-#
-# The ssh daemon must not permit root logins. The default value of 'yes' is a
-# deviation from the STIG requirements due to how openstack-ansible operates,
-# especially within OpenStack CI gate jobs. See documentation for V-38613 for
-# more details.
-security_ssh_permit_root_login: 'yes'             # V-38613
-
-## Kernel
-# Set these booleans to 'yes' to disable the kernel module (following the
-# STIG requirements). Set the boolean to 'no' to ensure no changes are made.
-security_disable_module_bluetooth: yes            # V-38682
-security_disable_module_dccp: yes                 # V-38514
-security_disable_module_rds: yes                  # V-38516
-security_disable_module_sctp: yes                 # V-38515
-security_disable_module_tipc: yes                 # V-38517
-security_disable_module_usb_storage: no           # V-38490
-security_disable_icmpv4_redirects: no             # V-38524
-security_disable_icmpv4_redirects_secure: no      # V-38526
-security_disable_icmpv6_redirects: no             # V-38548
-#
-# ** DANGER **
-# It's strongly recommended to fully understand the effects of changing the
-# following sysctl tunables. Refer to the documentation under 'Developer
-# Notes' for each of the STIGs below before making any changes.
-# ** DANGER **
-#
-security_sysctl_enable_tcp_syncookies: yes        # V-38539
-security_sysctl_enable_martian_logging: no        # V-38528
-#
-# Deployers who wish to disable IPv6 entirely must set this configuration
-# variable to 'yes'. See the documentation for V-38546 before making this
-# change.
-security_disable_ipv6: no                         # V-38546
-
-# Sets the global challenge ACK counter to a large value such
-# that a potential attacker could not reasonably come up against it.
-security_set_tcp_challenge_ack_limit: yes    # CVE-2016-5696
-
-## Mail
-# The STIG requires inet_interfaces to be set to 'localhost', but Ubuntu will
-# configure it to be 'all' when dpkg-reconfigure is unavailable (as it is when
-# Ansible installs packages). The default here is 'localhost' to meet the STIG
-# requirement, but some deployers may want this set to 'all' if their hosts
-# need to receive emails over the network (which isn't common).
-#
-# See the documentation for V-38622 for more details.
-security_postfix_inet_interfaces: localhost       # V-38622
-#
-# Configuring an email address here will cause hosts to forward the root user's
-# email to another address.
-#
-#security_root_forward_email: user@example.com
-
-## Linux Security Module (LSM)
-# AppArmor and SELinux provide powerful security controls on a Linux system
-# by setting policies for allowed actions. By setting the following variable
-# to true, the appropriate LSM will be enabled for the Linux distribution:
-#
-#    Ubuntu: AppArmor
-#    CentOS: SELinux
-#
-# See the ansible-hardening documentation for more details.
-security_enable_linux_security_module: yes        # V-51337
-
-## PAM and authentication
-# V-38497 requires that accounts with null passwords aren't allowed to
-# authenticate via PAM. Ubuntu 14.04's default allows these logins -- see the
-# documentation for V-38497 for more details. Set the variable below to 'yes'
-# to remove 'nullok_secure' from the PAM configuration or set it to 'no' to
-# leave the PAM configuration unaltered.
-security_pam_remove_nullok: yes                   # V-38497
-#
-# V-38501 requires that failed login attempts must lock a user account using
-# pam_faillock, but Ubuntu doesn't package that PAM module. Instead, fail2ban
-# can be installed to lock out IP addresses with failed logins for 15 minutes.
-# Set the variable below to 'yes' to install and configure fail2ban.
-security_install_fail2ban: no                     # V-38501
-#
-# The STIG requires bans to last 15 minutes. Adjust the following variable
-# to set the time an IP is banned by fail2ban (in seconds).
-security_fail2ban_bantime: 900                    # V-38501
-
-## Password complexity and aging
-# V-38475 - There is no password length requirement by default in Ubuntu 14.04.
-# To set a password length requirement, uncomment
-# security_password_minimum_length below. The STIG recommendation is 14
-# characters.
-#security_password_minimum_length: 14             # V-38475
-# V-38477 - There is no password change limitation set by default in Ubuntu. To
-# set the minimum number of days between password changes, uncomment the
-# security_password_minimum_days variable below.  The STIG recommendation is 1
-# day.
-#security_password_minimum_days: 1                # V-38477
-# V-38479 - There is no age limit on password by default in Ubuntu. Uncomment
-# line below to use the STIG recommendation of 60 days.
-#security_password_maximum_days: 60               # V-38479
-# V-38480 - To warn users before their password expires, uncomment the line
-# below and they will be warned 7 days prior (following the STIG).
-#security_password_warn_age: 7                    # V-38480
-# V-38684 - Setting the maximum number of simultaneous logins per user. The
-# STIG sets a limit of 10.
-#security_max_simultaneous_logins: 10             # V-38684
-# V-38692 - Lock accounts that are inactive for 35 days.
-#security_inactive_account_lock_days: 35          # V-38692
-
-## sudo
-# V-58901 requires that 'NOPASSWD' and '!authenticate' do not appear in any
-# sudoers files since they could lead to a compromise. Set the following
-# variables to 'yes' to comment out any lines found with these prohibited
-# parameters or leave them set to 'no' (the default) to leave sudoers files
-# unaltered. Deployers are urged to review the documentation for this STIG
-# before making changes.
-security_sudoers_remove_nopasswd: no              # V-58901
-security_sudoers_remove_authenticate: no          # V-58901
-
-## umask settings
-# The STIG recommends changing various default umask settings for users and
-# daemons via different methods. However, this could cause serious issues for
-# production OpenStack environements which haven't been tested with these
-# changes.
-#
-# The variables below are set to match the STIG requirements, but they are
-# commented out to ensure they require deployers to opt-in for each change. To
-# opt in for one of the changes below, simply uncomment the line and run the
-# playbook. Deployers are strongly advised to review the documentation for
-# these changes and review their systems to ensure these changes won't cause
-# service disruptions.
-#
-# V-38642 - Set umask for daemons in init scripts to 027 or 022
-#security_umask_daemons_init: 027                 # V-38642
-#
-# V-38645 - System default umask in /etc/login.defs must be 077
-#security_umask_login_defs: 077                   # V-38645
-#
-# V-38649 - System default umask for csh must be 077
-#security_umask_csh: 077                          # V-38649
-#
-# V-38651 - System default umask for bash must be 077
-#security_umask_bash: 077                         # V-38651
-
-## Unattended upgrades (APT) configuration
-security_unattended_upgrades_enabled: false
-security_unattended_upgrades_notifications: false
-
-###############################################################################
-#  ____  _   _ _____ _       _____   ____ _____ ___ ____
-# |  _ \| | | | ____| |     |___  | / ___|_   _|_ _/ ___|
-# | |_) | |_| |  _| | |        / /  \___ \ | |  | | |  _
-# |  _ <|  _  | |___| |___    / /    ___) || |  | | |_| |
-# |_| \_\_| |_|_____|_____|  /_/    |____/ |_| |___\____|
-#
-###############################################################################
-
-## Accounts (accounts)
-# Set minimum password lifetime to 1 day for interactive accounts.
-security_set_minimum_password_lifetime: no                   # V-71927
-security_set_maximum_password_lifetime: no                   # V-71931
-
-## AIDE (aide)
-# Initialize the AIDE database immediately (may take time).
-security_rhel7_initialize_aide: no                           # V-71973
-
-## Audit daemon (auditd)
-# Send audit records to a different system using audisp.
-#security_audisp_remote_server: '10.0.21.1'                  # V-72083
-# Encrypt audit records when they are transmitted over the network.
-#security_audisp_enable_krb5: yes                            # V-72085
-# Set the auditd failure flag. WARNING: READ DOCUMENTATION BEFORE CHANGING!
-security_rhel7_audit_failure_flag: 1                         # V-72081
-# Set the action to take when the disk is full or network events cannot be sent.
-security_rhel7_auditd_disk_full_action: syslog               # V-72087
-security_rhel7_auditd_network_failure_action: syslog         # V-72087
-# Size of remaining disk space (in MB) that triggers alerts.
-security_rhel7_auditd_space_left: "{{ (ansible_mounts | selectattr('mount', 'equalto', '/') | map(attribute='size_total') | first * 0.25 / 1024 / 1024) | int }}" # V-72089
-# Action to take when the space_left threshold is reached.
-security_rhel7_auditd_space_left_action: email               # V-72091
-# Send auditd email alerts to this user.
-security_rhel7_auditd_action_mail_acct: root                 # V-72093
-# Add audit rules for commands/syscalls.
-security_rhel7_audit_chsh: yes                               # V-72167
-security_rhel7_audit_chage: yes                              # V-72155
-security_rhel7_audit_chcon: yes                              # V-72139
-security_rhel7_audit_chmod: no                               # V-72105
-security_rhel7_audit_chown: no                               # V-72097
-security_rhel7_audit_creat: yes                              # V-72123
-security_rhel7_audit_crontab: yes                            # V-72183
-security_rhel7_audit_delete_module: yes                      # V-72189
-security_rhel7_audit_fchmod: no                              # V-72107
-security_rhel7_audit_fchmodat: no                            # V-72109
-security_rhel7_audit_fchown: no                              # V-72099
-security_rhel7_audit_fchownat: no                            # V-72103
-security_rhel7_audit_fremovexattr: no                        # V-72119
-security_rhel7_audit_fsetxattr: no                           # V-72113
-security_rhel7_audit_ftruncate: yes                          # V-72133
-security_rhel7_audit_init_module: yes                        # V-72187
-security_rhel7_audit_gpasswd: yes                            # V-72153
-security_rhel7_audit_lchown: no                              # V-72101
-security_rhel7_audit_lremovexattr: no                        # V-72121
-security_rhel7_audit_lsetxattr: no                           # V-72115
-security_rhel7_audit_mount: yes                              # V-72171
-security_rhel7_audit_newgrp: yes                             # V-72165
-security_rhel7_audit_open: yes                               # V-72125
-security_rhel7_audit_openat: yes                             # V-72127
-security_rhel7_audit_open_by_handle_at: yes                  # V-72129
-security_rhel7_audit_pam_timestamp_check: yes                # V-72185
-security_rhel7_audit_passwd: yes                             # V-72149
-security_rhel7_audit_postdrop: yes                           # V-72175
-security_rhel7_audit_postqueue: yes                          # V-72177
-security_rhel7_audit_pt_chown: yes                           # V-72181
-security_rhel7_audit_removexattr: no                         # V-72117
-security_rhel7_audit_rename: yes                             # V-72199
-security_rhel7_audit_renameat: yes                           # V-72201
-security_rhel7_audit_restorecon: yes                         # V-72141
-security_rhel7_audit_rmdir: yes                              # V-72203
-security_rhel7_audit_semanage: yes                           # V-72135
-security_rhel7_audit_setsebool: yes                          # V-72137
-security_rhel7_audit_setxattr: no                            # V-72111
-security_rhel7_audit_ssh_keysign: yes                        # V-72179
-security_rhel7_audit_su: yes                                 # V-72159
-security_rhel7_audit_sudo: yes                               # V-72161
-security_rhel7_audit_sudoedit: yes                           # V-72169
-security_rhel7_audit_truncate: yes                           # V-72131
-security_rhel7_audit_umount: yes                             # V-72173
-security_rhel7_audit_unix_chkpwd: yes                        # V-72151
-security_rhel7_audit_unlink: yes                             # V-72205
-security_rhel7_audit_unlinkat: yes                           # V-72207
-security_rhel7_audit_userhelper: yes                         # V-72157
-# Add audit rules for other events.
-security_rhel7_audit_account_access: yes                     # V-72143
-security_rhel7_audit_sudo_config_changes: yes                # V-72163
-security_rhel7_audit_insmod: yes                             # V-72191
-security_rhel7_audit_rmmod: yes                              # V-72193
-security_rhel7_audit_modprobe: yes                           # V-72195
-security_rhel7_audit_account_actions: yes                    # V-72197
BREAKS HERE
-      with_items:
-        - celery
-        - Flask
-        - Flask-HTTPAuth
-        - Flask-RESTful
-        - flask-restful-swagger
-        - flower
-        - redis
-        name: "{{ item }}"
BREAKS HERE
-          when_failed: $result
-          when_failed: $result
-          when_failed: $result
BREAKS HERE
-          println "Checking: ${agentProtocol.name}"
-              println "Changed protocols: removed protocol '${agentProtocol.name}'."
-      // Enable agent-->master access control, as it's strongly recommended.
-      // Enable CSRF, as it's strongly recommended.
-  register: shell_jenkins_security
-  changed_when: "(shell_jenkins_security | success) and 'Changed' not in shell_jenkins_security.stdout"
BREAKS HERE
-    - service: name=mysqld state=running enabled=true
-    - apt: name=python-mysql.connector
-      action: service name=mysqld state=restarted
BREAKS HERE
-        dest: "{{ fetch_directory }}/{{ fsid }}/"
BREAKS HERE
-      long_name: "{{ 's' + item + '-windc' }}.{{ name_prefix }}.{{ public_dns_zone }}"
-      long_name: "{{ 's' + item + '-win1' }}.{{ name_prefix }}.{{ public_dns_zone }}"
-      long_name: "{{ 's' + item + '-work' }}.{{ name_prefix }}.{{ public_dns_zone }}"
-      long_name: "{{ 's' + item + '-tower' }}.{{ name_prefix }}.{{ public_dns_zone }}"
-      long_name: "gitlab.{{ name_prefix }}.{{ public_dns_zone }}"
-      long_name: "docs.{{ name_prefix }}.{{ public_dns_zone }}"
BREAKS HERE
-      - name: Adding a CD-ROM drive to be used with Cloud-init.
-          seconds: 30
-          host: "{{ item }}"
-          port: 22
-          msg: "VMs did not become available after 5 minutes. Network error?"
-          - "{{ k8s_master_hn }}"
-          - "{{ k8s_node1_hn }}"
-          - "{{ k8s_node2_hn }}"
-          - "{{ k8s_node3_hn }}"
-          seconds: 90
BREAKS HERE
-      private: false
-      private: false
-      private: false
-      private: false
BREAKS HERE
-    when: not do_not_subscribe
-    when: not do_not_subscribe
BREAKS HERE
-    ssh_server_permit_environment_vars: ['PWD','HTTP_PROXY']
-    ssh_authorized_principals :
BREAKS HERE
-nova_default_schedule_zone: nova
BREAKS HERE
-      ansible_ssh_private_key_file: "{{ provision.key.file }}"
BREAKS HERE
-          {%-     set _ = filtered_role_list.append(role) %}
-        {% for role in zuul_roles %}
-                {{ role.src | regex_replace('https://git.openstack.org/', '') }} \
BREAKS HERE
-      
BREAKS HERE
-    - student_workload|d("")|length >0
-        name: "../ansible/roles/ocp-workload-{{ workload[1] }}"
BREAKS HERE
-    - apt: pkg={{ item }} state=present update_cache=yes cache_valid_time=3600
-      loop:
-        - ca-certificates
-        - curl
-        - strace
-        - vim
-    - copy: src=import1.sql dest=/tmp/import1.sql
-    - mysql_db: name={{ item }} state=import target=/tmp/import1.sql
-    - set_fact:
-    - copy: src=import2.sql dest=/tmp/import2.sql
-    - mysql_db: name={{ item }} state=import target=/tmp/import2.sql
-    - mysql_replication: mode=getslave
-    - fail: msg="Slave issue"
-- hosts: mariadbgalera
-  vars:
-    mariadb_galera_primary_node: '{% if is_docker %}docker-{% else %}vbox-{% endif %}{{ ansible_distribution_release }}-upstream-mariadbgalera-1'
-    mariadb_wsrep_node_address: "{{ ansible_all_ipv4_addresses[0] }}"
-    mariadb_version: '10.1'
-    - set_fact:
-        mariadb_wsrep_node_address: "{{ ansible_eth1.ipv4.address }}"
-      when: not is_docker
-
BREAKS HERE
-        register: introspection_result
-        failed_when: introspection_result.rc != 0
BREAKS HERE
-    - set_fact:
BREAKS HERE
-      - "hw:numa_mempolicy=preferred"
BREAKS HERE
-# Swift ceilometer rabbitmq settings
-swift_rabbitmq_telemetry_userid: "swift"
-swift_rabbitmq_telemetry_vhost: "/swift"
-swift_rabbitmq_telemetry_port: "5672"
-swift_rabbitmq_telemetry_servers: "127.0.0.1"
-# For now swift ceilometer does not work with SSL - this is a speculative option in the hope it gets added
-swift_rabbitmq_telemetry_use_ssl: "False"
BREAKS HERE
-        name=["pdc-client", "pdc-updater", "libmodulemd", "python2-productmd"]
-        state=latest
BREAKS HERE
-# Before running this playbook, assert that:
-# current number of nodes = node_instance_count - new_node_instance_count
-# This will prevent misuse of the playbook, and unexpected tag 'newnode' on instances.
-- hosts: localhost
-  connection: local
-  gather_facts: false
-  become: false
-  vars_files:
-    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
-  tasks:
-    - name: Assert new_node_instance_count and node_instance_count are setup properly
-      tags: assert_new_node
-      assert:
-        that:
-          - "(groups[('tag_' ~ project_tag ~ '_node') | replace('-', '_')] | length) == \
-          (node_instance_count|int) - (new_node_instance_count|int)"
-
-    - "{{ ('tag_' ~ project_tag ~ '_node') | replace('-', '_') }}:&tag_newnode_true"
-    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/ssh_vars.yml"
-    - "{{ ('tag_' ~ project_tag ~ '_node') | replace('-', '_') }}:&tag_newnode_true"
-    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/ssh_vars.yml"
-        src: "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/files/scaleup_hosts_template.j2" ## path is wrong
-  hosts: "{{ ('tag_' ~ project_tag ~ '_bastion') | replace('-', '_') }}"
-    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/ssh_vars.yml"
-- name: Run OpenShift Scaleup playbook
-  hosts: "{{ ('tag_' ~ project_tag ~ '_bastion') | replace('-', '_') }}"
-    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/ssh_vars.yml"
-        resource: "{{ hostvars[item].ec2_id }}"
-      with_items: "{{ groups[('tag_' ~ project_tag ~ '_node') | replace('-', '_')] | intersect(groups['tag_newnode_true']) }}"
-  hosts: "{{ ('tag_' ~ project_tag ~ '_bastion') | replace('-', '_') }}"
-    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/ssh_vars.yml"
BREAKS HERE
-    - usr.lib.dovecot.auth
-    - usr.lib.dovecot.config
-    - usr.lib.dovecot.lmtp
-    - usr.lib.dovecot.managesieve
-  command: 'aa-complain {{ aa_config }}'
-    - usr.lib.dovecot.auth
-    - usr.lib.dovecot.config
BREAKS HERE
-      retries: 5
-      delay: 60
BREAKS HERE
-  gather_facts: false # Already gathered previously
BREAKS HERE
-        chproxy_users: "{{hst_chproxy_users}}"
BREAKS HERE
-    src: "/tmp/Plex-Trakt-Scrobbler-master/Trakttv.bundle"
-    dest: "/opt/plex/Library/Application Support/Plex Media Server/Plug-ins"
BREAKS HERE
-#- name: dole out the service-specific config
-#  hosts: bugzilla2fedmsg;bugzilla2fedmsg-stg
-#  user: root
-#  gather_facts: True
-#  accelerate: "{{ accelerated }}"
-#
-#  roles:
-#  - fedmsg/hub
-#  - bugzilla2fedmsg
-#  - role: collectd/fedmsg-service
-#    process: fedmsg-hub
-#
-#  vars_files: 
-#   - /srv/web/infra/ansible/vars/global.yml
-#   - "{{ private }}/vars.yml"
-#   - "{{ vars_path }}/{{ ansible_distribution }}.yml"
-#
-#  handlers:
-#  - include: "{{ handlers }}/restart_services.yml"
BREAKS HERE
-  ###### Install PGLog
BREAKS HERE
-  - import_tasks: "{{ tasks_path }}/yumrepos.yml"
BREAKS HERE
-- name: Install and Configure MySQL for Quay
-        mysql_name: "{{ database_service_name | default('mysql-quay') }}"
-        mysql_username: "{{ database_username }}"
-        mysql_password: "{{ database_password }}"
-        mysql_root_username: "{{ database_admin_username }}"
-        mysql_root_password: "{{ database_admin_password }}"
-        mysql_database: "{{ database_name }}"
-
-      block:
-        - name: Install PostgreSQL for Clair
-          include_role:
-            name: config-postgresql
-          vars:
-            mode: containerized
-            postgresql_name: "{{ clair_database_service_name | default('postgresql-clair') }}"
-            postgresql_username: "{{ clair_database_username }}"
-            postgresql_password: "{{ clair_database_password }}"
-            postgresql_host_port: "{{ clair_database_port | default('5433') }}"
-            postgresql_admin_password: "{{ clair_database_admin_password }}"
-            postgresql_database: "{{ clair_database_name }}"
-          when: groups['clair']| length > 0
-        clair_host_proxy_port: "{{ clair_endpoint_port | default('6060') }}"
BREAKS HERE
-      image: "{{ node_dict.image | default(provision.image) }}"
-      name: "{{ prefix }}{{ node_dict.name }}-{{ item }}"
-      # use regiterd networks
-      ansible_ssh_user: "{{ provision.username }}"
BREAKS HERE
-  - name: Abort if {{ site }} folder doesn't exists
-    - name: Export staging/production database
-    - name: Pull exported database from staging/production to development
-        src: "{{project_web_dir}}/{{ backup_file }}"
-    - name: Delete exported database from staging/production
-      delegate_to: development_host
BREAKS HERE
-    - assert:
-    - assert:
BREAKS HERE
-    image: plexinc/pms-docker
BREAKS HERE
-- include_tasks: mq_setup.yml
-  with_items:
-    - oslomsg_setup_host: "{{ designate_oslomsg_rpc_setup_host }}"
-      oslomsg_userid: "{{ designate_oslomsg_rpc_userid }}"
-      oslomsg_password: "{{ designate_oslomsg_rpc_password }}"
-      oslomsg_vhost: "{{ designate_oslomsg_rpc_vhost }}"
-      oslomsg_transport: "{{ designate_oslomsg_rpc_transport }}"
-    - oslomsg_setup_host: "{{ designate_oslomsg_notify_setup_host }}"
-      oslomsg_userid: "{{ designate_oslomsg_notify_userid }}"
-      oslomsg_password: "{{ designate_oslomsg_notify_password }}"
-      oslomsg_vhost: "{{ designate_oslomsg_notify_vhost }}"
-      oslomsg_transport: "{{ designate_oslomsg_notify_transport }}"
-  no_log: true
BREAKS HERE
-	        }]
-        params: [{url: "/dvmdb/adom/lab"}]
BREAKS HERE
-
-          notify:
-          action: service name=apache2 state=restarted
BREAKS HERE
-      shell: pvs --units m -o pv_mda_count,pv_mda_free,pv_mda_size {{ CINDER_PHYSICAL_VOLUME.physical_volume }}
BREAKS HERE
-  - name: disable nagios alerts for this host's webserver service
BREAKS HERE
-    target: https://apps.fedoraproject.org/voting
BREAKS HERE
-    symfony_project_composer_opts: '--no-dev --optimize-autoloader'
-    symfony_project_keep_releases: 5
BREAKS HERE
-  - {
-    role: osbs-secret,
-      osbs_namespace: "{{ osbs_worker_namespace }}",
-      osbs_secret_name: kojisecret,
-        - { source: "/etc/pki/koji/fedora-builder.pem",
-            dest: cert
-          },
-    }
-  - {
-    role: osbs-secret,
-      osbs_namespace: "{{ osbs_worker_namespace }}",
-      osbs_secret_name: registry-secret,
-        - { source: "{{private}}/files/koji/{{docker_cert_name}}.cert.pem",
-            dest: registry.crt },
-        - { source: "{{private}}/files/koji/{{docker_cert_name}}.key.pem",
-            dest: registry.key },
-    }
BREAKS HERE
-  - mirrormanager/frontend
BREAKS HERE
-        url: https://{{ ansible_tower_ip }}/api/v1/job_templates/{{ job_template_id }}"/launch/
-          extra_vars: "guid: {{guid}}"
BREAKS HERE
-      when: (workarounds['rhbz1284133']['enabled'] is defined and workarounds['rhbz1284133']['enabled'] | bool)
-- name: create the requisite networks for network isolation
-  roles:
-    - { role: undercloud_network, when: "product.full_version != '7-director' and installer.network.isolation != 'none'"}
BREAKS HERE
-      seconds: 10
BREAKS HERE
-          applianceBay: 2
-    - debug: var=result
-          managerBay: 1
-          managerBay: 1
BREAKS HERE
-
-      update_cache: yes
BREAKS HERE
-    - name: Add hosts to host list
-      add_host:
-        name="{{ item.value.name }}"
-        groups="{{ item.value.groups| join(',') }}"
-        node_label="{{ item.key }}"
-        ansible_ssh_user="{{ item.value.ssh_user }}"
-        ansible_ssh_host="{{ item.value.ssh_host }}"
-        ansible_ssh_private_key_file="{{ item.value.ssh_key_file }}"
-      with_dict: provisioner.hosts
-  vars:
-    - ansible_ssh_user: root
-    - name: Check if CPU supports INTEL based KVM
-      shell: egrep -c 'vmx' /proc/cpuinfo
-      ignore_errors: True
-      register: kvm_intel
-
-    - name: Check if CPU supports AMD based KVM
-      shell: egrep -c 'svm' /proc/cpuinfo
-      ignore_errors: True
-      register: kvm_amd
-  vars:
-    - ansible_ssh_user: root
-  tasks:
-    - name: Enable KVM modules
-      modprobe:
-          name: kvm
-          state: present
-      ignore_errors: True
-      when: kvm_intel.rc == 0
-
-    - name: Enable nested KVM
-      lineinfile:
-          dest: "/etc/modprobe.d/dist.conf"
-          line: "options kvm_intel nested=1"
-          state: present
-          create: yes
-      when: kvm_intel.rc == 0
-
-    # A change in the modprove requires to reload the module
-    - name: Unload Intel KVM module
-      modprobe:
-          name: kvm_intel
-          state: absent
-      ignore_errors: True
-      when: kvm_intel.rc == 0
-
-    - name: Load Intel KVM module
-      modprobe:
-          name: kvm_intel
-          state: present
-      ignore_errors: True
-      when: kvm_intel.rc == 0
-
-    - name: Install required QEMU-KVM packages
-      yum: name=qemu-kvm state=present
-      when: kvm_intel.rc == 0
-
-- name: Enable KVM for AMD
-  hosts: virthost
-  gather_facts: no
-  vars:
-    - ansible_ssh_user: root
-    - name: Enable KVM modules
-      modprobe:
-          name: kvm
-          state: present
-      ignore_errors: True
-      when: kvm_amd.rc == 0
-
-    - name: Enable nested KVM support for AMD
-      lineinfile:
-          dest: "/etc/modprobe.d/dist.conf"
-          line: "options amd nested=1"
-          state: present
-          create: yes
-
-    # A change in the modprove requires to reload the module
-    - name: Unload AMD KVM module
-      modprobe:
-          name: kvm_amd
-          state: absent
-      ignore_errors: True
-      when: kvm_amd.rc == 0
-
-    - name: load AMD KVM module
-      modprobe:
-          name: kvm_amd
-          state: present
-      ignore_errors: True
-      when: kvm_amd.rc == 0
-
-    - name: Install required QEMU-KVM packages
-      yum: name=qemu-kvm state=present
-      when: kvm_amd.rc == 0
-            xml: "{{ lookup('template', 'network/' + item.key + '.xml.j2') }}"
-        with_dict: provisioner.networks
-        with_dict: provisioner.networks
-        with_dict: provisioner.networks
-        shell: "qemu-img create -f qcow2 {{ provisioner.image.name }} {{ provisioner.image.disk.size }}G"
-            virt-install --ram {{ item.value.memory }} \
-                         --network network:default \
-                         --network network:{{ provisioner.networks.data.name }} \
-                         --name {{ item.key }}
-            value: "-o ControlMaster=auto -o ControlPersist=60s -F {{ inventory_dir }}/ansible.ssh.config"
BREAKS HERE
-      #TODO(abregman): remove when bug 1334732 is resolved
-      - name: Set selinux to permissive
-        selinux: policy=targeted 
-                 state=permissive
-
BREAKS HERE
-- name: Check init system
-  command: cat /proc/1/comm
-  changed_when: false
-  register: _pid1_name
-  tags:
-    - always
-
-- name: Set the name of pid1
-  set_fact:
-    pid1_name: "{{ _pid1_name.stdout }}"
-  tags:
-    - always
-
BREAKS HERE
-  become_user: root
-  become_method: sudo
-    - common
-    - upgrade
-  hosts: logging-01
-  become_user: root
-  become_method: sudo
BREAKS HERE
-  when: >
-    inventory_hostname in groups['ironic_api']
-  when: >
-    inventory_hostname in groups['ironic_conductor']
-  when: >
-    inventory_hostname in groups['ironic_api']
-  when: >
-    inventory_hostname in groups['ironic_conductor']
-  when: >
-    inventory_hostname == groups['ironic_conductor'][0]
-  when: >
-    inventory_hostname == groups['ironic_api'][0]
BREAKS HERE
-- name: Prepare MQ/DB services
-    - name: Configure MySQL user
-      include: common-tasks/mysql-db-user.yml
-      vars:
-        user_name: "{{ glance_galera_user }}"
-        password: "{{ glance_container_mysql_password }}"
-        login_host: "{{ glance_galera_address }}"
-        db_name: "{{ glance_galera_database }}"
-      run_once: yes
-
BREAKS HERE
-    ports:
BREAKS HERE
-    galaxy_port: "{{ GALAXY_PORT|default('8080') }}"
BREAKS HERE
-    - role: "pip_install"
-      tags:
-        - pip
BREAKS HERE
-    header_expect: true
BREAKS HERE
-  vars:
-    debug: yes
BREAKS HERE
-- name: Installation and setup of Nova
-  hosts: nova_all
-  max_fail_percentage: 20
-  pre_tasks:
-    - include: common-tasks/dynamic-address-fact.yml
-      vars:
-        network_address: "management_address"
-    - include: common-tasks/os-lxc-container-setup.yml
-      vars:
-        extra_container_config_no_restart:
-          - "lxc.start.order=69"
-    - include: common-tasks/rabbitmq-vhost-user.yml
-      static: no
-        - inventory_hostname == groups['nova_all'][0]
-    - include: common-tasks/rabbitmq-vhost-user.yml
-      static: no
-        - inventory_hostname == groups['nova_all'][0]
-    - include: common-tasks/os-log-dir-setup.yml
-      vars:
-        log_dirs:
-          - src: "/openstack/log/{{ inventory_hostname }}-nova"
-            dest: "/var/log/nova"
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      when: inventory_hostname == groups['nova_all'][0]
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      when: inventory_hostname == groups['nova_all'][0]
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      when: inventory_hostname == groups['nova_all'][0]
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      when: inventory_hostname == groups['nova_all'][0]
-    - include: common-tasks/package-cache-proxy.yml
-
-    - name: Add nbd devices to the compute
-      shell: |
-        for i in /dev/nbd*;do
-          lxc-device -n {{ container_name }} add $i $i
-        done
-      failed_when: false
-      register: device_add
-      changed_when: >
-        'added' in device_add.stdout.lower()
-      delegate_to: "{{ physical_host }}"
-      when:
-        - "inventory_hostname in groups['nova_compute']"
-        - "not is_metal | bool"
-      tags:
-        - always
-    - name: Add net/tun device to the compute
-      command: |
-        lxc-device -n {{ container_name }} add /dev/net/tun /dev/net/tun
-      delegate_to: "{{ physical_host }}"
-      when:
-        - "inventory_hostname in groups['nova_compute']"
-        - "not is_metal | bool"
-      tags:
-        - always
-    - name: Check if kvm device exists
-      stat:
-        path: /dev/kvm
-      delegate_to: "{{ physical_host }}"
-      register: kvm_device
-      when:
-        - "inventory_hostname in groups['nova_compute']"
-        - "not is_metal | bool"
-      tags:
-        - always
-    - name: Add kvm device to the compute
-      command: |
-        lxc-device -n {{ container_name }} add /dev/kvm /dev/kvm
-      delegate_to: "{{ physical_host }}"
-      register: device_add
-      failed_when: false
-      changed_when: >
-        'added' in device_add.stdout.lower()
-        - "inventory_hostname in groups['nova_compute']"
-        - "not is_metal | bool"
-        - kvm_device.stat.ischr is defined and kvm_device.stat.ischr
-      tags:
-        - always
-  roles:
-    - role: "os_nova"
-      nova_novncproxy_git_repo: "{{ openstack_repo_git_url }}/novnc"
-      nova_novncproxy_git_install_branch: "{{ novncproxy_git_install_branch }}"
-      nova_spicehtml5_git_repo: "{{ openstack_repo_git_url }}/spice-html5"
-      nova_spicehtml5_git_install_branch: "{{ spicehtml5_git_install_branch }}"
-      nova_management_address: "{{ management_address }}"
-    - role: "ceph_client"
-      openstack_service_system_user: "{{ nova_system_user_name }}"
-      openstack_service_venv_bin: "{{ nova_bin }}"
-        - inventory_hostname in groups['nova_compute']
-        - nova_libvirt_images_rbd_pool is defined or
-          cinder_backends_rbd_inuse | default(false) | bool
-      tags:
-        - ceph
-    - role: "openstack_openrc"
-      tags:
-        - openrc
-    - role: "rsyslog_client"
-      rsyslog_client_log_rotate_file: nova_log_rotate
-      rsyslog_client_log_dir: "/var/log/nova"
-      rsyslog_client_config_name: "99-nova-rsyslog-client.conf"
-      tags:
-        - rsyslog
-    - role: "system_crontab_coordination"
-      tags:
-        - crontab
-  vars_files:
-    - defaults/repo_packages/nova_consoles.yml
-  vars:
-    is_metal: "{{ properties.is_metal|default(false) }}"
-    nova_galera_user: nova
-    nova_galera_database: nova
-    nova_api_galera_user: nova_api
-    nova_api_galera_database: nova_api
-    nova_placement_galera_user: nova_placement
-    nova_placement_galera_database: nova_placement
-    nova_cell0_database: nova_cell0
-    nova_galera_address: "{{ galera_address }}"
-    nova_api_galera_address: "{{ galera_address }}"
-    nova_placement_galera_address: "{{ galera_address }}"
-    glance_host: "{{ internal_lb_vip_address }}"
BREAKS HERE
-  when: >
-    inventory_hostname == groups['swift_all'][0]
BREAKS HERE
-    site_name: docs.teamsilverblue.org
-    ssl: true
-    sslonly: true
-    certbot: true
-    tags:
-    - docs.teamsilverblue.org
-
-  - role: httpd/website
BREAKS HERE
-      dest: "/etc/keystone/policy.json"
-    - Restart Keystone APIs on first node
-    - Restart Keystone APIs on other nodes
-    - Restart service on first node
-    - Restart service on other nodes
-    - Restart Keystone APIs on first node
-    - Restart Keystone APIs on other nodes
-    - Restart service on first node
-    - Restart service on other nodes
-    - Restart Keystone APIs on first node
-    - Restart Keystone APIs on other nodes
-    - Restart service on first node
-    - Restart service on other nodes
BREAKS HERE
-#    ldap_auth: 'simple'
-#    ldap_auth_username: 'username' (or DN)
-#    ldap_auth_password: 'password'
BREAKS HERE
-# This playbook was tested with Ansible 1.8.4.
-    venv_dir: "{{ jenkins_home }}/opt/cinch"
-    temp_dir: "{{ venv_dir }}/tmp"
-    python: "{{ venv_dir }}/bin/python"
-    latest_tip: false
-    beaker_kerberos: true
-    - name: check for /var/lib/jenkins/opt/cinch directory
-        path: "{{ venv_dir }}"
-        fail if pre-existing cinch installation at
-        /var/lib/jenkins/opt/cinch is found and it cannot be deleted
-        msg: "directory {{ venv_dir }} exists, but 'delete_venv' setting is False"
-      when: venv_stat_result.stat.exists and not (delete_venv|bool)
-        path: "{{ venv_dir }}"
-    - name: create virtualenv
-      command: virtualenv --no-setuptools "{{ venv_dir }}"
-      args:
-        creates: "{{ venv_dir }}"
-
-    - name: create temp dir in root of virtualenv
-      file:
-        path: "{{ temp_dir }}"
-        state: directory
-
-    - name: download latest version of pip (version included with RHEL7 is too old)
-      get_url:
-        url: https://bootstrap.pypa.io/get-pip.py
-        dest: "{{ temp_dir }}"
-
-    - name: install pip manually by running get-pip.py script
-      command: "{{ python }} {{ temp_dir }}/get-pip.py"
-      args:
-        creates: "{{ venv_dir }}/lib/python2.7/site-packages/setuptools"
-
-    - name: install released versions of cinch+linch-pin using pip
-        name: cinch
-        virtualenv: "{{ venv_dir }}"
-      when: not (latest_tip|bool)
-    # This pip install should be non-editable, but the pip module in Ansible
-    # 1.8.4. does not support that flag
-    - name: install latest tip of cinch+linch-pin instead of latest release from pypi
-      command: >-
-        "{{ venv_dir }}/bin/pip" install -U
-        https://github.com/CentOS-PaaS-SIG/linchpin/archive/develop.tar.gz
-        https://github.com/RedHatQE/cinch/archive/master.tar.gz
-      when: (latest_tip|bool)
-
-    - name: install beaker-client and python-krbV with pip to use kerberos with Beaker
-        name: "{{ item }}"
-        virtualenv: "{{ venv_dir }}"
-      with_items:
-        - beaker-client
-        - python-krbV
-      when: (beaker_kerberos|bool)
-    ## https://dmsimard.com/2016/01/08/selinux-python-virtualenv-chroot-and-ansible-dont-play-nice/
-    - name: >-
-        set up symlink in virtualenv for selinux module in system site-packages
-        since it's not pip installable
-      file:
-        src: /usr/lib64/python2.7/site-packages/selinux
-        dest: "{{ venv_dir }}/lib/python2.7/site-packages/selinux"
-        state: link
BREAKS HERE
-
-- hosts: localhost
-  pre_tasks:
-    - import_tasks: ../prep-inventory.yml
-  roles:
-    - role: manage-aws-infra
-      operation: inventory_gen_e2c
-
-  pre_tasks:
-    - import_tasks: ../prep-inventory.yml
-    - role: manage-aws-infra
-      operation: deploy
-    - meta: refresh_inventory
-
-- hosts: localhost
-  pre_tasks:
-    - import_tasks: ../prep-inventory.yml
-  roles:
-    - role: manage-aws-infra
-      operation: inventory_gen_hosts
-
-- name: Refresh Server inventory
-  hosts: localhost
-  connection: local
-    - meta: refresh_inventory
-
-- hosts: cluster_hosts
-  gather_facts: false
-  tasks:
-    - name: Debug hostvar
-      debug:
-        msg: "{{ hostvars[inventory_hostname] }}"
-        verbosity: 2
-    - import_role:
-        name: ../../../galaxy/infra-ansible/roles/update-host
-        tasks_from: wait-for-host
-
-- hosts: cluster_hosts
-  tasks:
-    - name: Set Openshift Hostnames
-      set_fact:
-        openshift_hostname: "{{ ec2_private_dns_name }}"
-        openshift_public_hostname: "{{ ec2_public_dns_name }}"
-
BREAKS HERE
-# when using an ssl termonating reverse proxy, you'' want to set this to:
BREAKS HERE
-      when: rhel_07_040680_postconf_audit.stdout != 'smtpd_client_restrictions = permit_mynetworks, reject'
BREAKS HERE
-rabbitmq_policies:
BREAKS HERE
-  - role: keytab/service
-    kt_location: /etc/httpd/admin.keytab
-    service: HTTP
-    host: admin.fedoraproject.org
-  - role: keytab/service
-    kt_location: /etc/httpd/admin.keytab
-    service: HTTP
-    host: admin.fedoraproject.org
BREAKS HERE
-  command: "true"
-  changed_when: no
-      - notimplemented
BREAKS HERE
-    - debug:
-        msg: "Environment: netbox_stable={{ netbox_stable }} netbox_git={{ netbox_git }} netbox_python={{ netbox_python }}"
-    - name: Ensure that NetBox returns a successful HTTP response.
BREAKS HERE
-- name: Prepare MQ/DB services
-    - name: Configure MySQL user
-      include: common-tasks/mysql-db-user.yml
-      vars:
-        user_name: "{{ cinder_galera_user }}"
-        password: "{{ cinder_container_mysql_password }}"
-        login_host: "{{ cinder_galera_address }}"
-        db_name: "{{ cinder_galera_database }}"
-      run_once: yes
-
BREAKS HERE
-    - role: "{{ rolename | basename }}"
BREAKS HERE
-      name: ceph.ceph-common
BREAKS HERE
-      - fuse
-      - nano
-      - fail2ban
-      - wget
-      - lsb-release
-      - figlet
-      - update-notifier-common
-      - software-properties-common
-      - unrar
-      - unzip
-      - glances
-      - python-pip
-      - python3-pip
-      - python-passlib
-      - zip
-      - curl
-      - man-db
-      - htop
-      - openssh-server
-      - dirmngr
-      - npm
-      - zip
-      - apt-transport-https
-      - ca-certificates
-      - tree
-      - ncdu
-      - ctop
-      - dialog
-      - dnsutils
-      - mc
-      - apache2-utils
-      - lsof
-      - pwgen
-      - gawk
-      - python-lxml
-      - acl
-      - bc
-    #ignore_errors: yes
BREAKS HERE
-  hosts: localhost
BREAKS HERE
-        openshift_auth_profile: "fedoraidp",
BREAKS HERE
-          chproxy_build_dir: "{{build_dir}}"
BREAKS HERE
-# vars file for openstack-ansible-security
BREAKS HERE
-      job_template_organizations:
-        - "Test Organization"
BREAKS HERE
-    when: not do_not_subscribe
BREAKS HERE
-    - file: dest="{{ lookup('env', 'PWD') }}/hosts" state=absent
-        dest: "{{ lookup('env', 'PWD') }}/{{ tmp.node_prefix }}hosts"
-        src: "{{ lookup('env', 'PWD') }}/{{ tmp.node_prefix }}hosts"
BREAKS HERE
-    - name: Configure MySQL user
-      include: common-tasks/mysql-db-user.yml
-      static: no
-      vars:
-        user_name: "{{ neutron_galera_user }}"
-        password: "{{ neutron_container_mysql_password }}"
-        login_host: "{{ neutron_galera_address }}"
-        db_name: "{{ neutron_galera_database }}"
-      run_once: yes
-
BREAKS HERE
-  - { role: docker_setup, when: not no_docker_storage_setup, device: '/dev/vdb'}
BREAKS HERE
-    lineinfile: dest=/root/.ssh/known_hosts regexp=^{{ item }} state=absent
BREAKS HERE
-        openshift_ansible_version: "release-3.6",
-        openshift_ansible_version: "release-3.6",
BREAKS HERE
-        tower-cli --insecure setting modify LICENSE @/root/license.txt
BREAKS HERE
-    # # files/secret/patient0/staging/credentials.vault.yaml
BREAKS HERE
-    cert_name: "{{wildcard_cert_name}}"
BREAKS HERE
-        - 40-services
-        - 60-docker
-        state: present
BREAKS HERE
-          org: system:masters
BREAKS HERE
-      tags: ['openshift-cluster-aarch','ansible-ansible-openshift-ansible']
BREAKS HERE
-        name: "{{ item.name | default(item) }}"
-        install_options: "{{ item.install_options | default(omit) }}"
-      with_items: "{{ homebrew_mandatory_packages + homebrew_installed_packages }}"
-        name: "{{ item }}"
-      with_items: "{{ pip_apps }}"
BREAKS HERE
-      rabbitmq_server: "rabbitmq01.phx2.fedoraproject.org"
BREAKS HERE
-when: deploy_cns | default(true) | bool
BREAKS HERE
-    deleteme_name: 'terry'
-    deleteme_rules: >
-        path "secret/{{deleteme_name}}/*" {
-        path "secret/{{deleteme_name}}" {
-        } 
-    - name: "Create a policy"
-        name: "{{deleteme_name}}"
-        rules: "{{deleteme_rules}}"
-        token: "{{ vault_root_token }}"
-    - name: "Get '{{deleteme_name}}' policy"
-        name: '{{deleteme_name}}'
-          - "'{{vault_policy_get.rules}}' == '{{deleteme_rules}}'"
-    - name: "List the policies, check if '{{deleteme_name}}' policy is listed"
-    - fail:
-        msg: "policy {{deleteme_name}} not in list"
-      when: deleteme_name not in vault_policy_list.policies
-    - name: "Delete '{{deleteme_name}}' policy"
-        name: '{{deleteme_name}}'
-    - name: "List the policies, check that '{{deleteme_name}}' is no longer listed"
-    - fail:
-        msg: "policy {{deleteme_name}} in list"
-      when: deleteme_name in vault_policy_list.policies
BREAKS HERE
-    path: {{ item }}
BREAKS HERE
-# - include: ../../common/tasks/postgresql.yml
-# - include: ../../common/tasks/java.yml
-# - include: ../../common/tasks/tomcat.yml
-# - include: ../../common/tasks/apache.yml
-  command: "wget {{images_url}} --output-document={{tomcat_webapps}}{{images_app}}.war"
-    - "{{data_dir}}/{{images_app}}/config"
-  template: src=config/config.properties dest={{data_dir}}/{{images_app}}/config/{{images_app}}-config.properties
BREAKS HERE
-          path: "{{ undercloud_disk_path }}"
-        when: undercloud_disk_path | default(False)
BREAKS HERE
-  when: system.ssl == 'letsencrypt'
-  when: system.ssl == 'letsencrypt'
-    dest: /etc/dovecot/conf.d/10-master.conf      
BREAKS HERE
-          clickhouse_listen_host_default: ['::1', '127.0.0.1', if_inner]
-          clickhouse_networks_default: "{{ ['::1', '127.0.0.1', docker_net] }}"
BREAKS HERE
-# run ansible-playbook playbook/default.yml -t nexus_restore -e "nexus_restore_point=(# date of choice -> %y-%m-%d #)"
-  - name: 'blob-raw'
-    path: "{{ nexus_data_dir }}/blobs/blob-raw"
-  - name: 'blob-pypi'
-    path: "{{ nexus_data_dir }}/blobs/blob-pypi"
-  - name: 'blob-docker'
-    path: "{{ nexus_data_dir }}/blobs/blob-docker"
-  - name: 'blob-ruby'
-    path: "{{ nexus_data_dir }}/blobs/blob-ruby"
-  - name: 'blob-bower'
-    path: "{{ nexus_data_dir }}/blobs/blob-bower"
-  - name: 'blob-npm'
-    path: "{{ nexus_data_dir }}/blobs/blob-npm"
-  - name: 'blob-mvn'
-    path: "{{ nexus_data_dir }}/blobs/blob-mvn"
-    write_policy: allow_once
-  - name: db-backup
-#  - name: blob-backup
-#    cron: '0 0 22 * * ?'
-#    typeId: script
-#    taskProperties:
-#      language: bash
-#      source: "{{ nexus_script_dir }}/nexus-blob-backup.sh"
-  blob_store: blob-mvn # Note : cannot be updated once the repo has been created
-  write_policy: allow_once # allow_once or allow
-  blob_store: blob-pypi # Note : cannot be updated once the repo has been created
-  write_policy: allow_once # allow_once or allow
-    write_policy: allow_once
-  blob_store: blob-raw # Note : cannot be updated once the repo has been created
-  write_policy: allow_once # allow_once or allow
-  - name: jenkins-mirror
-    remote_url: 'http://mirrors.jenkins-ci.org/'
-  - name: jenkins-updates
-    remote_url: 'http://updates.jenkins-ci.org/'
-  - name: ubuntu-security
-    remote_url: 'http://security.ubuntu.com/ubuntu'
-  - name: ubuntu-openstack
-    remote_url: 'http://ubuntu-cloud.archive.canonical.com/ubuntu'
-  - name: ubuntu-galera
-    remote_url: 'http://nyc2.mirrors.digitalocean.com/mariadb/repo/10.1/ubuntu'
-  - name: ubuntu-mongodb
-    remote_url: 'http://repo.mongodb.org/apt/ubuntu'
-      - jenkins-mirror
-      - jenkins-updates
-      - ubuntu-security
-      - ubuntu-openstack
-      - ubuntu-galera
-      - ubuntu-mongodb
-  blob_store: blob-docker # Note : cannot be updated once the repo has been created
-  write_policy: allow_once # allow_once or allow
-  blob_store: blob-ruby # Note : cannot be updated once the repo has been created
-  write_policy: allow_once # allow_once or allow
-    blob_store: blob-ruby
-    blob_store: blob-ruby
-    blob_store: blob-ruby
-  blob_store: blob-bower # Note : cannot be updated once the repo has been created
-  write_policy: allow_once # allow_once or allow
-    blob_store: blob-bower
-    blob_store: blob-bower
-    blob_store: default
-  blob_store: blob-npm # Note : cannot be updated once the repo has been created
-  write_policy: allow_once # allow_once or allow
-    blob_store: blob-npm
-    blob_store: blob-npm
-    blob_store: blob-npm
BREAKS HERE
-    - include: "common-tasks/test-set-nodepool-vars.yml"
BREAKS HERE
-    - { role: elasticsearch, es_api_port:9200, es_config: { "http.port": 9200, "transport.tcp.port":9300, discovery.zen.ping.unicast.hosts: "localhost:9300",
BREAKS HERE
-          ansible_user: "{{ansible_ssh_user}}"
-    - name: ADD LINKLIGHT REPO INTO TOWER
-        scm_url: "https://github.com/network-automation/linklight"
BREAKS HERE
-  # TODO: move to plugin
-PARAMIKO_PROXY_COMMAND:
-  # TODO: move to plugin
-  default:
-  description: 'TODO: write it'
-  env: [{name: ANSIBLE_PARAMIKO_PROXY_COMMAND}]
-  ini:
-  - {key: proxy_command, section: paramiko_connection}
-PARAMIKO_PTY:
-  # TODO: move to plugin
-  default: True
-  description: 'TODO: write it'
-  env: [{name: ANSIBLE_PARAMIKO_PTY}]
-  ini:
-  - {key: pty, section: paramiko_connection}
-  type: boolean
-PARAMIKO_RECORD_HOST_KEYS:
-  # TODO: move to plugin
-  default: True
-  description: 'TODO: write it'
-  env: [{name: ANSIBLE_PARAMIKO_RECORD_HOST_KEYS}]
-  ini:
-  - {key: record_host_keys, section: paramiko_connection}
-  type: boolean
BREAKS HERE
-      job: "echo {{pgrole.stdout}} > /tmp/program_var && bash /opt/plexguide/menu/pgvault/pgcron"
BREAKS HERE
-      when: datadog_key is defined
BREAKS HERE
-# the structured response DOES include the 'rpc-reply' key.
-# the structured response is returned in the 'output' key.
BREAKS HERE
-        retries: 1
-          debug: var=api_check
BREAKS HERE
-  hosts: compute
-   - post_install/compute_tasks
BREAKS HERE
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      vars:
-        user_name: "{{ designate_galera_user }}"
-        password: "{{ designate_galera_password }}"
-        login_host: "{{ designate_galera_address }}"
-        db_name: "{{ designate_galera_database_name }}"
-      when: inventory_hostname == groups['designate_all'][0]
-
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - designate
BREAKS HERE
-    - playbooks/test-vars.yml
-    - playbooks/test-vars.yml
BREAKS HERE
-
BREAKS HERE
-
BREAKS HERE
-- include: openstack_lvm_config.yml
-  tags:
-    - openstack_hosts-install
-    - openstack_hosts-config
-
BREAKS HERE
-    - { role: test_includes, tags: test_includes }
BREAKS HERE
-- name: Drop Keystone WSGI Configs
-  template:
-    src: "{{ item.src }}"
-    dest: "{{ item.dest }}"
-    owner: "{{ keystone_system_user_name }}"
-    group: "{{ keystone_system_group_name }}"
-    mode: "{{ item.mode|default('0644') }}"
-  with_items:
-    - { src: "keystone-wsgi.py.j2", dest: "/var/www/cgi-bin/keystone/admin", mode: "0755" }
-    - { src: "keystone-wsgi.py.j2", dest: "/var/www/cgi-bin/keystone/main", mode: "0755" }
-  notify:
-    - Restart Apache
BREAKS HERE
-    shell: "jq -c . < {{ ANSIBLE_REPO_PATH }}/workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template }}-orig >  {{ ANSIBLE_REPO_PATH }}/workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template }}"
BREAKS HERE
-  when: >
-    inventory_hostname == groups['horizon_all'][0]
BREAKS HERE
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      vars:
-        user_name: "{{ ironic_galera_user }}"
-        password: "{{ ironic_container_mysql_password }}"
-        login_host: "{{ ironic_galera_address }}"
-        db_name: "{{ ironic_galera_database }}"
-      when: inventory_hostname == groups['ironic_all'][0]
-
-  vars:
-    ironic_galera_user: ironic
-    ironic_galera_database: ironic
-    ironic_galera_address: "{{ galera_address }}"
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - ironic
BREAKS HERE
-# But don't want to enable modules at this point
BREAKS HERE
-- include: designate_init_common.yml
BREAKS HERE
-      - name: Check for any stopped/failed pcs resources
BREAKS HERE
-        ansible_ssh_user="{{ distro.images[distro.name][distro.version].remote_user }}"
BREAKS HERE
-    _oslomsg_configure_notify: "{{ (nova_ceilometer_enabled | bool) or (nova_designate_enabled | bool) }}"
BREAKS HERE
-    - java8
-    - tomcat7-java8
BREAKS HERE
-    nagios: action=downtime minutes=15 service=host host={{ inventory_hostname }}
BREAKS HERE
-  vars_files:
-    - "login_creds.vault"
-        provider: "{{ login_creds }}"
BREAKS HERE
-
-
-
BREAKS HERE
-  - bugyou
BREAKS HERE
-        delegate_to: localhost
BREAKS HERE
-      goss_file: goss/should-{{ should }}.yml
BREAKS HERE
-  max_fail_percentage: 0
-  max_fail_percentage: 0
-  max_fail_percentage: 20
BREAKS HERE
-        playbooks_dir: /usr/local/loopabull-playbooks/ansible,
BREAKS HERE
-#vim: set ft=ansible:
-  become: yes
-  gather_facts: false
-  become: yes
-  become: yes
-  become: yes
-  become: yes
BREAKS HERE
-    # - role: system/netdata
-    #   tags: system_netdata
-    - role: system/virtualbox
-      tags: system_virtualbox
BREAKS HERE
-  - name: Replacing quotes in conf file
-    replace: dest="{{ ha_base_dir }}"/ganesha-ha.conf
-             regexp='"(((VIP.*)\n*)*)"$'
-             replace='\1'
BREAKS HERE
-    ignore_errors: "{{ ignore_register_errors | default(True) }}"
-    ignore_errors: "{{ ignore_attach_pool_errors | default(True) }}"
-    ignore_errors: "{{ ignore_disable_errors | default(True) }}"
-    ignore_errors: "{{ ignore_enable_errors | default(True) }}"
BREAKS HERE
-        when: "{{ install.images.packages|default('') or install.images['update'] }}"
BREAKS HERE
-    - name: Check get-pip.py file
-      stat:
-        path: /opt/get-pip.py
-      register: get_pip_file
-          - "get_pip_file.stat.exists"
-          - "'pip 8.0.3 ' in pip_version.stdout"
BREAKS HERE
-      failed_when: "'pxe_ssh' not in driver_list.stdout"
BREAKS HERE
-  hosts: {{ dhost }}.stg.phx2.fedoraproject.org
BREAKS HERE
-        service_name: "{{ item }}"
-        service_action: "reloaded"
-        - "cinder-scheduler"
-        - "cinder-volume"
-        - "cinder-backup"
BREAKS HERE
-- include: lxc_install.yml
BREAKS HERE
-  - include_vars: ../roles/ceph-common/defaults/main.yml
-  - include_vars: ../roles/ceph-mds/defaults/main.yml
-  - include_vars: ../group_vars/all
-  - include_vars: ../group_vars/mdss
-  - include_vars: ../roles/ceph-common/defaults/main.yml
-  - include_vars: ../roles/ceph-rgw/defaults/main.yml
-  - include_vars: ../group_vars/all
-  - include_vars: ../group_vars/rgws
-  - include_vars: ../roles/ceph-common/defaults/main.yml
-  - include_vars: ../roles/ceph-rbd-mirror/defaults/main.yml
-  - include_vars: ../group_vars/all
-  - include_vars: ../group_vars/rbd-mirrors
-  - include_vars: ../roles/ceph-common/defaults/main.yml
-  - include_vars: ../roles/ceph-nfs/defaults/main.yml
-  - include_vars: ../group_vars/all
-  - include_vars: ../group_vars/nfss
-  - include_vars: ../roles/ceph-common/defaults/main.yml
-  - include_vars: ../roles/ceph-osd/defaults/main.yml
-  - include_vars: ../group_vars/all
-  - include_vars: ../group_vars/osds
-  - include_vars: ../roles/ceph-common/defaults/main.yml
-  - include_vars: ../roles/ceph-mon/defaults/main.yml
-  - include_vars: ../roles/ceph-restapi/defaults/main.yml
-  - include_vars: ../group_vars/all
-  - include_vars: ../group_vars/mons
-  - include_vars: ../group_vars/restapis
-  - include_vars: ../roles/ceph-common/defaults/main.yml
-  - include_vars: ../group_vars/all
-  - include_vars: ../group_vars/mdss
-  - include_vars: ../group_vars/rgws
-  - include_vars: ../group_vars/rbd-mirrors
-  - include_vars: ../group_vars/nfss
-  - include_vars: ../group_vars/osds
-  - include_vars: ../group_vars/mons
-  - include_vars: ../group_vars/restapis
BREAKS HERE
-    local_action: >
-      wait_for host=<your_host>
-      port=22
-      state=stopped
-    local_action: >
-      wait_for host=<your_host>
-      port=22
-      delay=10
-      timeout=3600
BREAKS HERE
-      - [ /etc/letsencrypt/archive, /etc/letsencrypt/live ]
BREAKS HERE
-    - defaults/repo_packages/openstack_services.yml
BREAKS HERE
-#- name: gather facts from the undercloud
-#  hosts: undercloud
-#  gather_facts: yes
-#  any_errors_fatal: true
BREAKS HERE
-        name: name: "{{ ANSIBLE_REPO_PATH }}/roles/ansible-versionlock"
BREAKS HERE
-    local_action: wait_for port=22 host={{ inventory_hostname }} state=started delay=10 timeout=500
BREAKS HERE
-      when: >
-        inventory_hostname in groups['nova_compute'] and
-        (is_metal == false or is_metal == "False")
-      when: >
-        inventory_hostname in groups['nova_compute'] and
-        not is_metal | bool
-      when: >
-        inventory_hostname in groups['nova_compute'] and
-        not is_metal | bool and
-        nova_virt_type == 'kvm'
-        management_bridge: "{{ 'ansible_' + hostvars[inventory_hostname]['container_networks']['container_address']['bridge'] | replace('-', '_') }}"
-        - hostvars[inventory_hostname]['container_networks']['container_address']['bridge'] is defined
-        - is_metal | bool
-        - is_metal | bool
-      tags:
-        - nova-config
-    - name: Set nova management bridge (is_metal no container network)
-      set_fact:
-        management_address: "{{ ansible_ssh_host }}"
-      when:
-        - hostvars[inventory_hostname]['container_networks']['container_address']['bridge'] is undefined
-        - is_metal | bool
-        - hostvars[inventory_hostname]['container_networks']['container_address']['address'] is defined
-        - not is_metal | bool
-    - name: Set nova management address (container no container network)
-        - hostvars[inventory_hostname]['container_networks']['container_address']['address'] is undefined
-        - not is_metal | bool
BREAKS HERE
-   - { role: openqa/fixes, tags: ['openqa_fixes', 'fedmsg'] }
BREAKS HERE
-          use_kerberos: True,
-          kerberos_keytab: "FILE:/etc/krb5.osbs_{{osbs_url}}",
-          kerberos_principal: "osbs/{{osbs_url}}@{{ipa_realm}}",
BREAKS HERE
-    package: name={{ item }} state=present
-    with_items:
-    - createrepo
-    - koji
-    - python-scandir
-    - python2-productmd
BREAKS HERE
-    command: subscription-manager register --force --username={{rhn_username}} --password={{rhn_password}}
-  - set_fact:
-      pwd: "{{rhn_password}}"
-      user: "{{rhn_username}}"
-      username: "{{ hostvars['bastion']['user'] }}"
-      password: "{{ hostvars['bastion']['pwd'] }}"
BREAKS HERE
-      
-        openshift_master_ha: false,
BREAKS HERE
-  delegate_to: "{{ first_conductor }}"
-    first_conductor: "{{ groups[nova_services['nova-conductor']['group']][0] }}"
-  delegate_to: "{{ first_conductor }}"
-  vars:
-    first_conductor: "{{ groups[nova_services['nova-conductor']['group']][0] }}"
BREAKS HERE
-        _local_ip: "{{ hostvars[inventory_hostname]['ansible_' + _overlay_network.bridge|default('br-mgmt')|replace('-', '_')]['ipv4']['address']|default(ansible_ssh_host) }}"
BREAKS HERE
-    group: apache
-  - fedmsg/base
BREAKS HERE
-        rules: 'AllowTcpForwarding yes'
-        rules: 'AllowTcpForwarding yes'
BREAKS HERE
-          Name: "{{ name_prefix }}-subnet"
-        tags:
-          Name: "{{ name_prefix }}-Public"
BREAKS HERE
-        name: "ceph-nfs@{{ ansible_hostname }}"
BREAKS HERE
-      when: item.key.startswith('overcloud-compute')
BREAKS HERE
-      - name: register hosts to instack
-            openstack baremetal import --json {{ instack_file_path.stat.path }}
-        tags:
-            - ironic
-            # FIXME(yfried) use "--os-cloud" instead of "source rc" and replace with command
-            - skip_ansible_lint
-      # FIXME(yfried): unify in above shell?
-      - name: assign the kernel and ramdisk before introspection begins
-        shell: |
-            source ~/stackrc
BREAKS HERE
-    - role: tests,
BREAKS HERE
-            curl -o /opt/libguestfs_appliance.tar.gx http://download.libguestfs.org/binaries/appliance/appliance-1.38.0.tar.xz
BREAKS HERE
-    # Note: Python must be present in the container to use other modules.
BREAKS HERE
-      - block:
-          - name: check if user provided external storage config
-            stat:
-                path: "vars/storage/config/{{ install.storage.config }}.yml"
-            register: install_storage_config_file
-          - fail:
-                msg: |
-                    "The file for external storage config is not found.
-                    Expected path: vars/storage/config/{{ install.storage.config }}.yml"
-            when:
-                - install_storage_config_file.stat.exists == False
-        when:
-            - install.deploy|default('')
-            - install.storage.external
BREAKS HERE
-- hosts: localhost
-  - name: "Create katello repository"
-    katello_repository:
-      username: "{{ foreman_username }}"
-      password: "{{ foreman_password }}"
-      server_url: "{{ foreman_server_url }}"
-      verify_ssl: "{{ foreman_verify_ssl }}"
-      organization: "Default Organization"
-      name: "repo2"
-      product: "product1"
-      content_type: "yum"
-      url: "https://repos.fedorapeople.org/pulp/pulp/demo_repos/zoo/"
-      download_policy: "on_demand"
-    delegate_to: localhost
BREAKS HERE
-    retries: 3
BREAKS HERE
-      when: "_logspout_enabled == True"
-            _r_docker_labels: "{{docker_band_lbls}}"
-          image: "{{images.front}}"
-          labels: "{{docker_band_lbls}}"
-            hostname: ebaloger
BREAKS HERE
-      # FIXME (atalmor) this task is a workaround for bugzilla 1452610, should be removed when bug is fixed
-      - name: veryfied ironic service is up
-        become: true
-        service:
-            name: openstack-ironic-inspector-dnsmasq
-            state: started
-            enabled: yes
-        when: (install.version|default(undercloud_version)|openstack_release) > 9
-
BREAKS HERE
-        - 'result["data"] == "overridden facts.py"'
-        - 'result["failed"] == True'
-        - '"Could not find imported module support code for test_failure.  Looked for either foo.py or zebra.py" == result["msg"]'
BREAKS HERE
-        plugin: fedmsg,
BREAKS HERE
-- hosts: "{{ hosts }}"
BREAKS HERE
-      pbkdf2: true
BREAKS HERE
-      environment: app_environment
BREAKS HERE
-        name: python2-pip
BREAKS HERE
-        name: "{{item}}"
-      with_items: "{{packages}}"
BREAKS HERE
-    # this task has a failed_when: false to handle the scenario where no mgr existed before the upgrade
BREAKS HERE
-      - name: Execute the release tool
-        shell: "{{ product.name }}-release {{ product.version|int }}"
-        register: shell_result
-        changed_when: "shell_result == 0"
-      - name: Create the RHOS poodle repository
-        shell: "rhos-release  -x {{ product.full_version|int }}{{ installer_host_repo | default('')}};  rhos-release  -d {{ product.full_version|int }}"
-        when: product.repo_type in ['poodle']
-
BREAKS HERE
-        - docker-ce
BREAKS HERE
-
BREAKS HERE
-    - name: stop rbd-target-gw
-        name: rbd-target-gw
-    - name: start rbd-target-gw
-        name: rbd-target-gw
BREAKS HERE
-
-  - debug: msg="{{ result.msg }}"
BREAKS HERE
-  - { role: collectd/fedmsg, process: fedmsg-hub }
BREAKS HERE
-        openshift_ansible_pre_playbook: "playbooks/prerequisites.yml",
BREAKS HERE
-    deployment_groups_uri: '/rest/deployment-groups/c5a727ef-71e9-4154-a512-6655b168c2e3'
-    build_plans_uri: '/rest/build-plans/ab65bb06-4387-48a0-9a5d-0b0da2888508'
-    build_plans_read_only: 'false'
-            resourceUri: '{{ build_plans_uri }}'
-            readOnly: '{{ build_plans_read_only }}'
-          deploymentGroupsUri: '{{ deployment_groups_uri }}'
-          deploymentGroupsUri: '{{ deployment_groups_uri }}'
-          deploymentGroupsUri: '{{ deployment_groups_uri }}'
BREAKS HERE
-#         {%   if IP_ARR.insert(loop.index,hostvars[host]['ansible_host']) %}
-#         {%   endif %}
-#         {% set elasticsearch_hosts = [IP_ARR | map('regex_replace', '$', ':' ~ elastic_port|string()) | map('regex_replace', '$', '"') | map('regex_replace', '^', '"') | list | join(',' )] %}
-#         -E 'output.elasticsearch.hosts={{ elasticsearch_hosts }}'
-#       until: templates | success
BREAKS HERE
-    - debug: var={{ created_nodes.results }}
-
-        ansible_ssh_host={%- if item.public_ip %}{{ item.public_ip }}{%- else %}{{ item.info.addresses[provisioner.network.nic.net_1.name][0].addr }}{% endif %}
BREAKS HERE
-    name: '{{ pkg }}'
-  with_items:
-    - bind9
-    - dnsutils
-  loop_control:
-    loop_var: pkg
BREAKS HERE
-  command: "true"
-  changed_when: no
-      - notimplemented
-  command: "true"
-  changed_when: no
-      - notimplemented
-  command: "true"
-  changed_when: no
-      - RHEL-07-020660
-      - notimplemented
-  command: "true"
-  changed_when: no
-      - RHEL-07-020670
-      - notimplemented
BREAKS HERE
-  when: jenkins_ui.content.find('Jenkins ver. 2.0-rc-1') == -1
BREAKS HERE
-            file_path: './archived.logs'
BREAKS HERE
-      shell: |
-        IFS=$'\n'
-        for i in $({{ venv_path }}/bin/cinder-manage service list | awk '/None/ {print $1, $2}'); do
-          eval "{{ venv_path }}/bin/cinder-manage service remove $i"
-        done
BREAKS HERE
-    - name: Enusure system was actually rebooted
BREAKS HERE
-                'mac': '`echo "$NODE_XML" | grep {{ provison_virsh_network_name }} -B 1 | grep mac | cut -d\' -f2`',
BREAKS HERE
-        src: "{{ zuul.executor.work_root }}/{{ zuul.project.src_dir }}/tests/templates/inventory.j2"
BREAKS HERE
-  when: system.ssl == 'letsencrypt'
-  vars:
-    details:
-      - [ /etc/letsencrypt, /etc/letsencrypt/archive, /etc/letsencrypt/live ]
-      - [ dovecot ]
-    path: '{{ item[0] }}'
-    entity: '{{ item[1] }}'
-  with_nested: '{{ details }}'
-  when: system.ssl == 'letsencrypt'
BREAKS HERE
-- debug:
BREAKS HERE
-    when: set_snmp.changed or remove_snmp.changed 
BREAKS HERE
-    - name: Install prerequisites.
-      apt:
-        name: apt-utils
-        state: present
-      when: ansible_distribution == 'Ubuntu'
-
BREAKS HERE
-#- include: proxies-websites.yml
BREAKS HERE
-
BREAKS HERE
-  strategy: free
BREAKS HERE
-httpd_server_name: nexus.vm
-httpd_default_admin_email: "admin@example.com"
BREAKS HERE
-    docker_experimenter_command: gosu www-data ember build --env staging
-    docker_isp_command: gosu www-data ember build --env staging
-    docker_lookit_command: gosu www-data ember build --env staging
BREAKS HERE
-- name: Installation and setup of Keystone
-  serial: "{{ keystone_serial }}"
-  max_fail_percentage: 20
-  pre_tasks:
-    - include: common-tasks/os-lxc-container-setup.yml
-      vars:
-        extra_container_config_no_restart:
-          - "lxc.start.order=89"
-    - include: common-tasks/rabbitmq-vhost-user.yml
-      static: no
-        - inventory_hostname == groups['keystone_all'][0]
-        - groups[keystone_rabbitmq_host_group] | length > 0
-    - include: common-tasks/rabbitmq-vhost-user.yml
-      static: no
-        - keystone_ceilometer_enabled | bool
-        - inventory_hostname == groups['keystone_all'][0]
-        - groups[keystone_rabbitmq_telemetry_host_group] is defined
-        - groups[keystone_rabbitmq_telemetry_host_group] | length > 0
-        - groups[keystone_rabbitmq_telemetry_host_group] != groups[keystone_rabbitmq_host_group]
-    - include: common-tasks/os-log-dir-setup.yml
-      vars:
-        log_dirs:
-          - src: "/openstack/log/{{ inventory_hostname }}-keystone"
-            dest: "/var/log/keystone"
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      when: inventory_hostname == groups['keystone_all'][0]
-    - include: common-tasks/package-cache-proxy.yml
-    - include: common-tasks/haproxy-endpoint-manage.yml
-      vars:
-        haproxy_state: disabled
-      when: "{{ groups['keystone_all'] | length > 1 }}"
-  vars:
-    is_metal: "{{ properties.is_metal|default(false) }}"
-    keystone_serial:
-      - 1
-      - 100%
-    keystone_admin_port: 35357
-    keystone_galera_user: keystone
-    keystone_galera_database: keystone
-    keystone_galera_address: "{{ galera_address }}"
BREAKS HERE
-    - setup:
-      register: facts
-      register: facts_results
-      register: facts_results
-      register: facts
-      register: facts
-  gather_subset: "network"
-          - 'ansible_user_id|default("UNDEF_MIN") != "UNDEF_MIN"'
-          - 'ansible_mounts|default("UNDEF_MOUNT") == "UNDEF_MOUNT"'
-          - 'ansible_virtualization_role|default("UNDEF_VIRT") == "UNDEF_VIRT"'
BREAKS HERE
-    - name: "Drush: Create drush bin symlink."
BREAKS HERE
-      security_groups: "{{ topology_node.security_groups | default([]) | map('regex_replace', '(.*)', prefix + '\\1') | list or omit }}"
BREAKS HERE
-- include: provision.yml
-  hosts: controller
BREAKS HERE
-          vbmc_nodes: "{{ groups.get('overcloud_nodes', []) }}"
BREAKS HERE
-    - "inventory_hostname == (groups['nova_conductor'] | intersect(ansible_play_hosts))[0]"
-    - "inventory_hostname == (groups['nova_conductor'] | intersect(ansible_play_hosts))[0]"
BREAKS HERE
-    command: oc tag --source=docker registry.access.redhat.com/openshift3/jenkins-2-rhel7:v{{osrelease}} openshift/jenkins:v{{osrelease}} -n openshift
-    command: oc tag openshift/jenkins:v{{osrelease}} openshift/jenkins:latest -n openshift
BREAKS HERE
-    - { role: compiler,   tags: ['compiler', 'benchmark']} # fails in do! check!
-    - { role: transcode,  tags: ['transcode', 'multimedia', 'benchmark']} # fails! check!
BREAKS HERE
-    service: name=NetworkManager enabled=yes
BREAKS HERE
-    >> {{ working_dir }}/virt-resize.log 2>&1
BREAKS HERE
-  - debug: msg="Project name {{ project_name }} - domain {{ domain_name }}"
BREAKS HERE
-              {%-   if role['src'] | regex_replace('https://git.openstack.org/', '') not in zuul_src_repo_list %}
-              {%-     set _ = filtered_role_list.append(role) %}
BREAKS HERE
-    - role: "{{ rolename | basename }}"
BREAKS HERE
-    - percona_server_users_present_hosts
-    - percona_server_users_absent_hosts
BREAKS HERE
-        var: "{{tower_installer_path}}"
BREAKS HERE
-  user: root
-        if [[ ! -d "{{ item.path | default('/etc/ansible/roles') }}/{{ item.name | default(item.src | basename) }}/.git" ]]; then
-          rm -rf "{{ item.path | default('/etc/ansible/roles') }}/{{ item.name | default(item.src | basename) }}"
BREAKS HERE
-    - { role: network_hostroutes, when: networking is defined and networking }
-  hosts: localhost
BREAKS HERE
-  - lb
-  - lb
BREAKS HERE
-        service_action: "reloaded"
BREAKS HERE
-    repo_setup_script: "repo-setup-{{ target_upgrade_version }}.sh"
-    repo_setup_script: "{{ working_dir }}/repo-setup-{{ target_upgrade_version }}.sh"
BREAKS HERE
-    - answerfile.yml
-        hostname: "{{ nsx_node1.hostname }}"
-        description: "{{ item.description }}"
-          resource_type: " {{ item.cluster_pforile_binding_type }}"
-        - transport_node_name: "edge-1"
-        - transport_node_name: "edge-2"
BREAKS HERE
-# This playbook requires ansible >= 2.8, or role Azure.azure_preview_modules.
-  roles:
-    - Azure.azure_preview_modules    
-      allow_forwarded_traffic: true
BREAKS HERE
-  - include: python_ironicclient_install.yml
-
BREAKS HERE
-     - name: Change log files size
-       shell: "sed -i -re 's/(log4j.appender.out.maxFileSize=)[^=]*$/\110MB/' /opt/opendaylight/etc/org.ops4j.pax.logging.cfg"
BREAKS HERE
-        url: 'https://api.adoptopenjdk.net/v2/binary/releases/openjdk10?openjdk_impl=hotspot&os=linux&arch=x64&release=jdk-10.0.1%2B10&type=jdk'
BREAKS HERE
-  hosts: mbs-web:mbs-web-stg
-  hosts: mbs-web:mbs-web-stg
-  hosts: mbs-web:mbs-web-stg
BREAKS HERE
-        name: "{{ playbook_dir }}/../../github"
BREAKS HERE
-## Nova virtualization Type, set to KVM if supported
-# Current supported choice: qemu or kvm
-nova_firewall_driver: nova.virt.firewall.NoopFirewallDriver
-nova_compute_driver: libvirt.LibvirtDriver
-nova_reserved_host_memory_mb: 2048
BREAKS HERE
-# This variable should be used with lxd when using a
-# storage backend that utilizes storage pools (zfs)
-#lxd_storage_pool:
BREAKS HERE
-  - "{{inventory_dir}}/{{fabric|default('fabric.yml')}}"
-  tags: [ config ]
-  vars_files:
-  - "{{inventory_dir}}/nodes.yml"
-  - name: Wait for LLDP to start
-    tags: [ validate ]
BREAKS HERE
-  - name: purge rh_storage.repo file in /etc/yum.repos.d
-      path: /etc/yum.repos.d/rh_storage.repo
BREAKS HERE
-- name: Create SSL key
-  gather_facts: False
-  hosts: localhost
-  vars_files:
-  - vars/guests.yml
-  tasks:
-  - name: Add bastion to dynamic host
-    add_host:
-      name: bastion
-      group: bast
-  - name: Add other hosts to nodes group
-    add_host:
-      name: "{{item.name}}"
-      group: nodes
-    when: item.name != 'bastion'
-    with_items: "{{guests}}"
-  - name: Create SSH key for root
-    user:
-      name: root
-      generate_ssh_key: yes
-      ssh_key_bits: 2048
-      ssh_key_file: .ssh/id_rsa
-
-- name: Prepare host and guests
BREAKS HERE
-      dest: "{{ ANSIBLE_REPO_PATH }}/workdir/linklight_{{ guid }}"
-  - import_playbook: "{{ ANSIBLE_REPO_PATH }}/workdir/linklight_{{ guid }}/provisioner/provision_lab.yml"
BREAKS HERE
-  register: public_ip
-
-  debug: msg="Will use ADVERTISE_IP with plex container http://{{public_ip.ipify_public_ip}}:{{plex.port}}/"
-      ADVERTISE_IP: "http://{{public_ip.ipify_public_ip}}:{{plex.port}}/"
-      ADVERTISE_IP: "http://{{public_ip.ipify_public_ip}}:{{plex.port}}/"
BREAKS HERE
-            upstream: '{{_nginx_upstreams}}'
-
-    - name: Including tasks 
-      include_tasks: "{{item}}"
-      with_fileglob:
-        - "tasks/custom_*"
-      vars:
-        _customs: "{{ customs|default({}) }}"
-      tags: ['never', 'custom']
-
-
BREAKS HERE
-  vars_prompt:
-    - name: "OS_USERNAME"
-      prompt: "Enter user name for  perform actions with OS"
-      private: no
-    - name: "OS_PASSWORD"
-      prompt: "Enter OS password"
-      private: yes
-  - include: "{{ tasks }}/persistent_cloud_new.yml"
BREAKS HERE
-      package: name="{{ item }}" state=installed
-      with_items:
-        - tar
-        - rsync
-        - dbus-python
-        - NetworkManager
-        - libselinux-python
-        - python3-PyYAML
BREAKS HERE
-  - name: debug checkout ec2_facts
-
-
-        name: hostvars['internaldns']
-      when: hostvars['internaldns'] is defined
BREAKS HERE
-      edge_module_path: /prod/oai-pmh
BREAKS HERE
-  # This set of tasks runs against localhost
-  # and requires root access, but tests run as
-  # the user running the playbook (zuul). As
-  # such, we use a local connection and become.
-  connection: local
-  become: yes
BREAKS HERE
-swift_requirements_git_repo: https://git.openstack.org/openstack/requirements
-swift_requirements_git_install_branch: master
BREAKS HERE
-  - { role: ansible-role-rsyslog-client, tags: [ 'rsyslog-client' ] }
BREAKS HERE
-- name: Install specified packages  
-- name: Vault server configuration
-    src: vault_main.hcl.j2
-
BREAKS HERE
-    - name: Check out drush master branch.
-        chdir={{ drupal_core_path }}
BREAKS HERE
-lxc_cache_resolvers:
-  - 'nameserver 8.8.8.8'
-  - 'nameserver 8.8.4.4'
-
BREAKS HERE
-nova_oslomsg_notify_userid: nova
-nova_oslomsg_notify_vhost: /nova
BREAKS HERE
-            - vbmc_host != 'undercloud' or (vbmc_host is not defined)
BREAKS HERE
-      when: project.repo is not defined or not project.repo | match("^ssh://.+@.+|.+@.+:.+")
BREAKS HERE
-    - include: "common-tasks/os-{{ container_tech | default('lxc') }}-container-setup.yml"
-      static: no
-    - include: common-tasks/unbound-clients.yml
-      static: no
BREAKS HERE
-    rbdmirror_group_name: rbd-mirrors
BREAKS HERE
-          mode: "{{dr_dir_mode}}"
-          dest: '{{dr_dir|default(dr_name)}}'
BREAKS HERE
-        when: "'hypervisor' not in groups"
-        when: "'hypervisor' in groups"
BREAKS HERE
-auditd_rules:
-  account_modification: yes                       # V-38531, V-38534, V-38538
-  apparmor_changes: yes                           # V-38541
-  change_localtime: yes                           # V-38530
-  change_system_time: yes                         # V-38635
-  clock_settime: yes                              # V-38527
-  clock_settimeofday: yes                         # V-38522
-  clock_stime: yes                                # V-38525
-  DAC_chmod: no                                   # V-38543
-  DAC_chown: yes                                  # V-38545
-  DAC_lchown: yes                                 # V-38558
-  DAC_fchmod: no                                  # V-38547
-  DAC_fchmodat: no                                # V-38550
-  DAC_fchown: yes                                 # V-38552
-  DAC_fchownat: yes                               # V-38554
-  DAC_fremovexattr: yes                           # V-38556
-  DAC_lremovexattr: yes                           # V-38559
-  DAC_fsetxattr: yes                              # V-38557
-  DAC_lsetxattr: yes                              # V-38561
-  DAC_setxattr: yes                               # V-38565
-  deletions: no                                   # V-38575
-  failed_access: no                               # V-38566
-  filesystem_mounts: yes                          # V-38568
-  kernel_modules: yes                             # V-38580
-  network_changes: yes                            # V-38540
-  sudoers: yes                                    # V-38578
-disable_services:
-  abrtd: yes                                      # V-38641
-  atd: yes                                        # V-38640
-  autofs: yes                                     # V-38437
-  avahi: yes                                      # V-31618
-  bluetooth: yes                                  # V-38691
-  qpidd: yes                                      # V-38648
-  rsh: yes                                        # V-38594
-  ypbind: yes                                     # V-38604
-  xinetd: yes                                     # V-38582
-remove_services:
-  ldap-server: yes                                # V-38627
-  rsh-server: yes                                 # V-38591
-  sendmail: yes                                   # V-38671
-  telnet_server: yes                              # V-38587
-  tftp-server: yes                                # V-38606
-  xinetd: yes                                     # V-38584
-  xorg: yes                                       # v-38676
-  ypserv: yes                                     # V-38603
-disable_module:
-  bluetooth: yes                                  # V-38682
-  dccp: yes                                       # V-38514
-  rds: yes                                        # V-38516
-  sctp: yes                                       # V-38515
-  tipc: yes                                       # V-38517
-  usb_storage: no                                 # V-38490
-sysctl_tunable:
-  tcp_syncookies: 1                               # V-38539
BREAKS HERE
-    resource_group_secondary: demo_vm_1
-    - Azure.azure_modules
BREAKS HERE
-          when: ocp4_installer_version is version_compare('0.17', '<')
-          when: ocp4_installer_version is version_compare('0.17', '<')
-          when: ocp4_installer_version is version_compare('4.0', '>=')
-          when: ocp4_installer_version is version_compare('4.0', '>=')
BREAKS HERE
-          - ansible_version.minor >= 7
-    - name: set facts for output
-      set_fact:
-        summary_information: |
-          {{summary_information}}
-          - Workshop name is {{ec2_name_prefix}}
-          - Instructor inventory is located at  {{playbook_dir}}/{{ec2_name_prefix}}/instructor_inventory.txt
-          - Private key is located at {{playbook_dir}}/{{ec2_name_prefix}}/{{ec2_name_prefix}}.pem
-
-  become: True
BREAKS HERE
-    - name: Create database_backup directory inside {{ site }} if it doesn't exist
BREAKS HERE
-    setype: httpd_git_sys_content_t
BREAKS HERE
-  - name: Ceate second virtual network
-      resource_group: "{{ resource_group }}"
-      virtual_network: "{{ vnet_name1 }}"
BREAKS HERE
-- name: Deploy OpenShift Cluster and OSBS
BREAKS HERE
-  hosts:
-    - all
BREAKS HERE
-        state: set_power_state
-            hostname : "172.18.6.15"
-        state: set_power_state
-            hostname : "172.18.6.15"
-            hostname : "172.18.6.15"
-            hostname : "172.18.6.15"
-      delegate_to: localhost
BREAKS HERE
-- name: Configure NFS host for user-vols if required
-  hosts: support
-  gather_facts: False
-  become: yes
-  vars_files:
-    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
-  tasks:
-    - name: Create user vols
-      shell: "mkdir -p /srv/nfs/user-vols/vol{1..{{user_vols}}}"
-      tags:
-        - openshift_nfs_config
-    - name: chmod the user vols
-      shell: "chmod -R 777 /srv/nfs/user-vols"
-      tags:
-        - openshift_nfs_config
BREAKS HERE
-- import_tasks: mq_setup.yml
-  when:
-    - "nova_services['nova-conductor']['group'] in group_names"
-    - "inventory_hostname == ((groups[nova_services['nova-conductor']['group']] | intersect(ansible_play_hosts)) | list)[0]"
-  vars:
-    _oslomsg_rpc_setup_host: "{{ nova_oslomsg_rpc_setup_host }}"
-    _oslomsg_rpc_userid: "{{ nova_oslomsg_rpc_userid }}"
-    _oslomsg_rpc_password: "{{ nova_oslomsg_rpc_password }}"
-    _oslomsg_rpc_vhost: "{{ nova_oslomsg_rpc_vhost }}"
-    _oslomsg_rpc_transport: "{{ nova_oslomsg_rpc_transport }}"
-    _oslomsg_notify_setup_host: "{{ nova_oslomsg_notify_setup_host }}"
-    _oslomsg_notify_userid: "{{ nova_oslomsg_notify_userid }}"
-    _oslomsg_notify_password: "{{ nova_oslomsg_notify_password }}"
-    _oslomsg_notify_vhost: "{{ nova_oslomsg_notify_vhost }}"
-    _oslomsg_notify_transport: "{{ nova_oslomsg_notify_transport }}"
-  tags:
-    - common-mq
-    - nova-config
-
BREAKS HERE
-  with_items: neutron_requires_pip_packages
BREAKS HERE
-#      TODO(aopincar): Add 'security groups' dynamic creation
BREAKS HERE
-- name: "Show debug info"
-  tasks:
-
-- name: "Apply the ambari-agent role to all nodes"
-  hosts: hadoop-cluster
-  become: yes
BREAKS HERE
-- name: run fasClient
-  hosts: all:!builders:!persistent-cloud:!jenkins-cloud:!bkernel
-  serial: 20
BREAKS HERE
-  hosts: first
BREAKS HERE
-        venv_build_host_wheel_path: "{{ repo_pypiserver_package_path | default('/var/www/repo/pools') }}"
-        venv_pip_install_args: >-
-          --index-url {{ repo_build_pip_default_index | default('https://pypi.python.org/simple') }}
-          --trusted-host {{ (repo_build_pip_default_index | default('https://pypi.python.org/simple')) | urlsplit('hostname') }}
BREAKS HERE
-  - { role: nfs/client, when: "'releng-secondary' in group_names", mnt_dir: '/pub/fedora-secondary',  nfs_src_dir: '/fedora_ftp/fedora.redhat.com/pub/fedora-secondary' }
BREAKS HERE
-  
-      
-# Use at your own risk.
BREAKS HERE
-  hosts: gnocchi_api
BREAKS HERE
-  tags:
-    - prep
-  tags:
-    - prep
BREAKS HERE
-  - name: AWS Generate CloudFormation Template
-    template:
-      src: "{{ANSIBLE_REPO_PATH}}/configs/{{ env_type }}/files/cloud_providers/{{cloud_provider}}_cloud_template.j2"
-      dest: "{{ANSIBLE_REPO_PATH}}/workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template-orig"
-    tags:
-      - aws_infrastructure_deployment
-      - gen_cf_template
-  # for SSH first access to ec2 instances we always use the key defined in the CloudFormation
-  # template by the name {{key_name}}
-  # This variable is used when generation ssh config.
-  - name: Get ssh pub key
-    set_fact:
-      ssh_key: "~/.ssh/{{key_name}}.pem"
-  ######################### Minimize template (avoid size limitation as much as possible)
-  - name: minimize json
-    shell: "jq -c . < ../workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template-orig >  ../workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template"
-    register: jq_minify
-    ignore_errors: true
-    tags:
-      - aws_infrastructure_deployment
-      - gen_cf_template
-      - minify_template
-  - name: use original if jq failed
-    command: "cp ../workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template-orig ../workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template"
-    when: jq_minify|failed
-    tags:
-      - aws_infrastructure_deployment
-      - gen_cf_template
-      - minify_template
-  ######################### Validate CF Template
-  - name: validate cloudformation template
-    environment:
-      AWS_ACCESS_KEY_ID: "{{aws_access_key_id}}"
-      AWS_SECRET_ACCESS_KEY: "{{aws_secret_access_key}}"
-      AWS_DEFAULT_REGION: "{{aws_region}}"
-    shell: "aws cloudformation validate-template --region {{ aws_region | default(region) | default('us-east-1')}} --template-body file://../workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template"
-    changed_when: false
-    tags:
-      - aws_infrastructure_deployment
-      - validate_cf_template
-  ######################### Launch CF Template
-  - name: Launch CloudFormation template
-    # environment:
-    #   AWS_ACCESS_KEY_ID: "{{aws_access_key_id}}"
-    #   AWS_SECRET_ACCESS_KEY: "{{aws_secret_access_key}}"
-    #   AWS_DEFAULT_REGION: "{{aws_region}}"
-    cloudformation:
-      aws_access_key: "{{ aws_access_key_id }}"
-      aws_secret_key: "{{ aws_secret_access_key }}"
-      stack_name: "{{ project_tag }}"
-      state: "present"
-      region: "{{ aws_region | default(region) | default('us-east-1')}}"
-      disable_rollback: true
-      template: "{{ANSIBLE_REPO_PATH}}/workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template"
-        Stack: "project {{ project_tag }}"
-        owner: "{{ email | default('unknown') }}"
-    tags:
-      - aws_infrastructure_deployment
-      - provision_cf_template
-    register: cloudformation_out
-    until:
-      - cloudformation_out|succeeded
-      - cloudformation_out.output in ["Stack CREATE complete", "Stack is already up-to-date."]
-    retries: "{{ cloudformation_retries | default(25) }}"
-    delay: 60
-    ignore_errors: yes
-  - name: debug cloudformation
-    debug:
-      var: cloudformation_out
-    tags: provision_cf_template
-    when: not cloudformation_out|succeeded
-  - name: report Cloudformation error
-    fail:
-      msg: "FAIL {{ project_tag }} Create Cloudformation"
-    when: not cloudformation_out|succeeded
-    tags:
-      - provision_cf_template
-  - name: debug cloudformation
-    debug:
-      var: cloudformation_out
-      verbosity: 2
-    tags: provision_cf_template
-  - name: Gather EC2 facts
-    ec2_remote_facts:
-      aws_access_key: "{{ aws_access_key_id }}"
-      aws_secret_key: "{{ aws_secret_access_key }}"
-      region: "{{ aws_region | default(region) | default('us-east-1')}}"
-      filters:
-        instance-state-name: running
-        "tag:Project": "{{project_tag}}"
-    register: ec2_facts
-    tags:
-      - create_inventory
-      - must
-  - name: debug ec2_facts
-    debug:
-      var: ec2_facts
-      verbosity: 2
-  - name: windows ostype workaround
-    set_fact:
-      project_tag_ostype: "{{project_tag}}_ostype"
-    tags:
-      - create_inventory
-      - must
-  - set_fact:
-      stack_tag: "{{env_type | replace('-', '_')}}_{{guid}}"
-    tags:
-      - create_inventory
-      - must
-  - add_host:
-      name: "{{item.tags.internaldns | default(item.private_dns_name)}}"
-      shortname: "{{item.tags.Name | default(item.private_dns_name)}}"
-      groups:
-        - "tag_Project_{{stack_tag}}"
-        - "tag_{{stack_tag}}_{{item['tags'][project_tag] | default('unknowns')}}"
-        - "tag_{{stack_tag}}_ostype_{{item['tags'][project_tag_ostype] | default('unknown')}}"
-        - "{{item.tags.ostype | default('unknowns')}}"
-        - "{{item['tags'][project_tag_ostype] | default('unknowns')}}"
-        - "{{ 'newnodes' if (item.tags.newnode|d()|bool) else 'all'}}"
-      ansible_ssh_user: ec2-user
-      remote_user: ec2-user
-      ansible_ssh_private_key_file: "{{item['key_name']}}"
-      key_name: "{{item['key_name']}}"
-      state: "{{item['state']}}"
-      internaldns: "{{item.tags.internaldns | default(item.private_dns_name)}}"
-      instance_id: "{{ item.id }}"
-      region: "{{item['region']}}"
-      public_dns_name: "{{item['public_dns_name']}}"
-      private_dns_name: "{{item['private_dns_name']}}"
-      private_ip_address: "{{item['private_ip_address']}}"
-      public_ip_address: "{{item['public_ip_address']}}"
-      placement: "{{item['placement']['zone']}}"
-      image_id: "{{item['image_id']}}"
-      ansible_ssh_extra_args: "-o StrictHostKeyChecking=no"
-    with_items: "{{ec2_facts['instances']}}"
-    loop_control:
-      label: "{{item.tags.internaldns | default(item.private_dns_name)}}"
-    tags:
-      - create_inventory
-      - must
-  # AnsibleGroup tag can have several comma-separated values. Ex: activedirectories,windows
-  - add_host:
-      name: "{{item.tags.internaldns | default(item.private_dns_name)}}"
-      groups: "{{item.tags.AnsibleGroup}}"
-    with_items: "{{ec2_facts['instances']}}"
-    loop_control:
-      label: "{{item.tags.internaldns | default(item.private_dns_name)}}"
-    tags:
-      - create_inventory
-      - must
-  - name: debug hostvars
-    debug:
-      var: hostvars
-      verbosity: 2
-  - name: debug groups
-    debug:
-      var: groups
-      verbosity: 2
BREAKS HERE
-  hosts: openqa-workers,openqa-stg-workers
-#  hosts: openqa-workers, openqa-stg-workers
BREAKS HERE
-    value: 4
-    description: "Password must have at most four characters repeated consecutively"
BREAKS HERE
-- include: "/srv/web/infra/ansible/playbooks/include/virt-create.yml myhosts=taskotron-dev,taskotron-stg,taskotron-prod"
BREAKS HERE
-    - name: create cert dir for openshift public facing REST API SSL
-      file:
-        path: "/etc/origin/master/named_certificates"
-        state: "directory"
-
-    - name: install cert for openshift public facing REST API SSL
-      copy:
-        src: "{{private}}/files/osbs/{{env}}/osbs-internal.pem"
-        dest: "/etc/origin/master/named_certificates/{{osbs_url}}.pem"
-
-    - name: install key for openshift public facing REST API SSL
-      copy:
-        src: "{{private}}/files/osbs/{{env}}/osbs-internal.key"
-        dest: "/etc/origin/master/named_certificates/{{osbs_url}}.key"
-
-    - name: ensure origin conf dir exists
-      file:
-        path: "/etc/origin"
-        state: "directory"
-
-    - name: place htpasswd file
-      copy:
-        src: "{{private}}/files/httpd/osbs-{{env}}.htpasswd"
-        dest: /etc/origin/htpasswd
-
BREAKS HERE
-    - name: Install Python2
-      raw: >
-        source /etc/os-release
-
-        case ${ID} in
-          centos|rhel)
-            yum list installed python
-            result=$?
-
-            if [ $result -eq 1 ]; then
-              yum -y install python2
-              result=2
-            fi
-
-            exit $result;
-          ;;
-          ubuntu)
-            dpkg-query -s python &> /dev/null
-            result=$?
-
-            if [ $result -eq 1 ]; then
-              apt-get -y install python
-              result=2
-            fi
-
-            exit $result;
-          ;;
-        esac
BREAKS HERE
-- name: Copy keystone config
-    src: "{{ item.src }}"
-  notify:
-    - Restart uWSGI on first node
-    - Restart uWSGI on other nodes
-    - Restart web server on first node
-    - Restart web server on other nodes
-
-- name: Retrieve and config_template upstream files
-  config_template:
-    content: "{{ lookup('pipe', item.content) | string }}"
-    dest: "{{ item.dest }}"
-    config_overrides: "{{ item.config_overrides }}"
-    config_type: "{{ item.config_type }}"
-  with_items:
-      content: |
-        cat {{ keystone_paste_default_file_path }} 2>/dev/null || \
-        curl -s {{ keystone_git_config_lookup_location }}/{{ keystone_paste_git_file_path }}
-      content: |
-        cat {{ keystone_policy_default_file_path }} 2>/dev/null || \
-        echo {}
-    content: |
-        cat {{ keystone_sso_callback_file_path }} 2>/dev/null || \
-        curl -s {{ keystone_git_config_lookup_location }}/{{ keystone_sso_callback_git_file_path }}
BREAKS HERE
-      path: /root/.ssh/known_hosts
-      key: "{{ lookup('file', '/root/.ssh/id_rsa.pub') }}"
BREAKS HERE
-          {%-    set _ = constraints.update({name:version}) %}
-       that: requirements_constraints_content.find( "{{ item.key }}<={{item.value}}" ) != -1
-      when: requirements_constraints_content.find(item.key) != -1
BREAKS HERE
-        osd_hosts: "{{ osd_hosts | default([]) + [ (item.stdout | from_json).crush_location.host ] }}"
-    - name: find lvm osd volumes on each host
-      ceph_volume:
-        action: "list"
-      environment:
-        CEPH_VOLUME_DEBUG: 1
-        CEPH_CONTAINER_IMAGE: "{{ ceph_docker_registry + '/' + ceph_docker_image + ':' + ceph_docker_image_tag if containerized_deployment else None }}"
-        CEPH_CONTAINER_BINARY: "{{ container_binary }}"
-      with_items: "{{ osd_hosts }}"
-      delegate_to: "{{ item }}"
-      register: osd_volumes
-
-    - name: filter osd volumes to kill by osd - non container
-      set_fact:
-        osd_volumes_to_kill_non_container: "{{ osd_volumes_to_kill_non_container | default([]) + [ (item.1.stdout|from_json)[item.0] ] }}"
-      with_together:
-        - "{{ osd_to_kill.split(',') }}"
-        - "{{ osd_volumes.results }}"
-
-    - name: generate (host / volume) pairs to zap - non container
-      set_fact:
-        osd_host_volumes_to_kill_non_container: "{%- set _val = namespace(devs=[]) -%}
-        {%- for host in osd_hosts -%}
-        {%- for dev in osd_volumes_to_kill_non_container[loop.index-1] -%}
-        {%- set _val.devs = _val.devs + [{\"host\": host, \"path\": dev.path}] -%}
-        {%- endfor -%}
-        {%- endfor -%}
-        {{ _val.devs }}"
-
-      with_together:
-        - "{{ osd_to_kill.split(',') }}"
-        - "{{ osd_hosts }}"
-      delegate_to: "{{ item.1 }}"
-        data: "{{ item.path }}"
-      delegate_to: "{{ item.host }}"
-      with_items: "{{ osd_host_volumes_to_kill_non_container }}"
BREAKS HERE
-- include: nova_virt_detect.yml
-  static: no
-- include: nova_pre_install.yml
-- include: nova_install.yml
-- include: nova_post_install.yml
-- include: nova_db_setup.yml
-  static: no
-- include: nova_uwsgi.yml
-- include: "nova_init_{{ ansible_service_mgr}}.yml"
-- include: nova_service_setup.yml
-  static: no
-- include: nova_compute.yml
-  static: no
-- include: nova_db_post_setup.yml
BREAKS HERE
-    command: mkfs.ext4 /dev/vdc
BREAKS HERE
-        playbooks_dir: /usr/local/loopabull-playbooks/ansible/,
BREAKS HERE
-    - name: Set local facts for new container(s)
-        filter: ansible_local
-        gather_subset: "!all"
BREAKS HERE
-      repo: deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.0 multiverse
-  - name: Check to see if this is the intial MongoDB setup
-    stat: path=/var/log/rocket.chat/app-0.log
-    register: rocket_chat_log
-    when: (ansible_distribution == "Ubuntu")
-          and (ansible_distribution_release == "trusty")
-
-  - name: Ensure the autostarted MongoDB server is stopped
-    shell: /usr/sbin/service mongod stop
-    when: (ansible_distribution == "Ubuntu")
-          and (ansible_distribution_release == "trusty")
-          and not (rocket_chat_log.stat.exists)
-
-
-  - name: Ensure the MongoDB shell replSet script has been deployed
-    template:
-      src: mongo_rs_initiate.js.j2
-      dest: /var/lib/mongodb/mongo_rs_initiate.js
-    when: rocket_chat_include_mongodb
-
-  - name: Ensure the MongoDB replSets have been initiated
-    shell: >-
-      mongo mongo_rs_initiate.js &&
-      mongo --eval 'rs.initiate()' &&
-      touch .mongo_rs_initialised
-    args:
-      chdir: /var/lib/mongodb
-      creates: /var/lib/mongodb/.mongo_rs_initialised
-    when: rocket_chat_include_mongodb
BREAKS HERE
-  sudo: yes
-      # This URL should be in the config file (khaleesi settings)
-    - name: Download cirros image
-      get_url: url=http://download.cirros-cloud.net/0.3.1/cirros-0.3.1-x86_64-disk.img dest={{ tester.rally.dir }}/etc/{{ tester.rally.cirros_image_file }} mode=0440
-      shell: "{{ tester.rally.dir }}/install_rally.sh -v"
-        creates: "{{ tester.rally.path }}/rally"
BREAKS HERE
-     - {{ auth_keys_from_fas }} @sysadmin-main {{ root_auth_users }}
BREAKS HERE
-    command: "subscription-manager attach --pool={{ poolid }}"
BREAKS HERE
-                  'dest_logical_unit_port': item[dr_target_host + '_logical_unit_port'] | default('3260', true),
-                  'dest_logical_unit_portal': item[dr_target_host + '_logical_unit_portal'] | default('1'|int, true),
BREAKS HERE
-            version={{ tester.git.revision }}
BREAKS HERE
-    - stackdriver_agent
BREAKS HERE
-           port: 22
BREAKS HERE
-      - "pip==8.0.3"
-          - "pip_version.stdout | search('8.0.3')"
BREAKS HERE
-  until: install_packages|success
BREAKS HERE
-    - { role: "{{ ANSIBLE_REPO_PATH }}/roles/set-repositories", when: 'repo_method is defined' }
-    - { role: "{{ ANSIBLE_REPO_PATH }}/roles/common", when: 'install_common' }
-    - { role: "{{ ANSIBLE_REPO_PATH }}/roles/set_env_authorized_key", when: 'set_env_authorized_key' }
BREAKS HERE
-- name: Fix NetworkManager on Azure
-  gather_facts: False
-  become: yes
-  hosts:
-    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_bastion') | replace('-', '_') }}"
-    - bastions
-  vars_files:
-    - "../configs/{{ env_type }}/env_vars.yml"
-  tags:
-    - installing_openshift
-  tasks:
-    ### FIX Networkmanager on Azure
-    # TODO: retry without this
-    - name: Run NetworkManager byo playbook
-      command: ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-node/network_manager.yml
-      when: cloud_provider == 'azure'
-
BREAKS HERE
-    - role: "{{ rolename | basename }}"
BREAKS HERE
-  sudo: True
BREAKS HERE
-- name: Check init system
-  command: cat /proc/1/comm
-  changed_when: false
-  register: _pid1_name
-  tags:
-    - always
-
-- name: Set the name of pid1
-  set_fact:
-    pid1_name: "{{ _pid1_name.stdout }}"
-  tags:
-    - always
-
BREAKS HERE
-    - include_role:
-        name: os_tempest
-        ansible_become: true
-        tempest_run: 'yes'
-        debug: true
-        tempest_use_tempestconf: true
-        tempest_service_setup_host: '{{ inventory_hostname }}'
-        tempest_image_dir: '/home/zuul/images'
-        tempest_service_available_cinder: true
-        tempest_service_available_neutron: true
-        tempest_service_available_swift: true
-        tempest_service_available_glance: true
-        tempest_service_available_horizon: true
-        tempest_service_available_nova: true
-        keystone_service_internaluri_insecure: false
-        tempest_public_net_physical_type: 'datacentre'
-        tempest_private_net_provider_type: 'geneve'
-        tempest_public_subnet_cidr: '{{ tempest_cidr }}'
-        tempest_public_subnet_gateway_ip: '{{ tempest_cidr|nthhost(1) }}'
-        tempest_public_subnet_allocation_pools: '{{ tempest_cidr|nthhost(100) ~ "-" ~ tempest_cidr|nthhost(120) }}'
-        tempest_workspace: '/home/zuul/tempest'
-        stackviz_venv_bin: "/home/zuul/stackviz_venv/bin"
BREAKS HERE
-
BREAKS HERE
-      - name: create .ssh direcotry for non-root user
-                dest: "{{ default_privatekey }}"
-            become_user: "{{ install.user.name }}"
-            become: yes
-
BREAKS HERE
-  hosts: controller
BREAKS HERE
-    - include: common/ensure-rabbitmq.yml
-      vhost_name: "{{ designate_rabbitmq_vhost }}"
-      user_name: "{{ designate_rabbitmq_userid }}"
-      user_password: "{{ designate_rabbitmq_password }}"
BREAKS HERE
-  tags: test
BREAKS HERE
-  hosts: all:!localhost:!hypervisor
BREAKS HERE
-            - nodes_memory|int > ansible_memfree_mb|int
BREAKS HERE
-      - provision_workshop_env.sh
-      - provision-ose-projects.sh
-      - deploy_scripts
BREAKS HERE
-      - name: set_fact group_vars_path
-        set_fact:
-          group_vars_path: "{{ change_dir + '/hosts' if 'ooo-collocation' in change_dir.split('/') else change_dir + '/group_vars/all' }}"
-      - name: change ceph_repository to 'dev'
-        replace:
-          regexp: "ceph_repository:.*"
-          replace: "ceph_repository: dev"
-          dest: "{{ group_vars_path }}"
-        when: change_dir is defined
-      - name: change nfs-ganesha repository to 'dev'
-        replace:
-          regexp: "nfs_ganesha_stable=.*"
-          replace: "nfs_ganesha_stable=False"
-          dest: "{{ group_vars_path }}"
-        replace:
-          regexp: "nfs_ganesha_dev:.*"
-          replace: "nfs_ganesha_dev=True"
-          dest: "{{ group_vars_path }}"
-        when: change_dir is defined
-      - name: print contents of {{ group_vars_path }}
-        command: "cat {{ group_vars_path }}"
BREAKS HERE
-    - role: "{{ rolename | basename }}"
BREAKS HERE
-    - name: Prepare non-openstack git clone list
-        filtered_roles: >
-          {%-   if not role.src | match(".*git.openstack.org.*") %}
-      with_items: "{{ filtered_roles | default(roles) }}"
-        {% for src in roles | map(attribute='src') %}
-        {%   if src | match(".*git.openstack.org.*") %}
-                {{ src | regex_replace('https://git.openstack.org/', '') }} \
-        {%   endif %}
BREAKS HERE
-- name: Ensure that the role declares all paremeters in defaults
BREAKS HERE
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-- name: "MEDIUM | RHEL-07-040290 | PATCH | The operating system must enable an application firewall, if available."
-  command: "true"
-- name: "MEDIUM | RHEL-07-040301 | PATCH | The system must display the date and time of the last successful account logon upon an SSH logon."
-  command: "true"
-      - RHEL-07-040301
-- name: "MEDIUM | RHEL-07-040310 | PATCH | The system must not permit direct logons to the root account using remote access via SSH."
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-  command: "true"
-
BREAKS HERE
-  - create_container_host_group
BREAKS HERE
-    - apt: "pkg={{ item }} state=installed"
BREAKS HERE
-    - pvfs_version: 2.8.7
-    - pvfs_url: "http://orangefs.org/downloads/LATEST/source/{{pvfs_tar}}"
BREAKS HERE
-  no_log: "{{ not lookup('env', 'MOLECULE_DEBUG') | bool }}"
BREAKS HERE
-  hosts: "fleet"
BREAKS HERE
-swift_authtoken_active: True
BREAKS HERE
-    kolla_ansible_src_dir: "src/{{ zuul.project.canonical_hostname }}/openstack/kolla-ansible"
-    kolla_ansible_full_src_dir: "{{ zuul.executor.work_root }}/{{ kolla_ansible_src_dir }}"
-      script: "{{ kolla_ansible_full_src_dir }}/tests/setup_disks.sh {{ disk_type }}"
-    kolla_ansible_src_dir: "src/{{ zuul.project.canonical_hostname }}/openstack/kolla-ansible"
-    kolla_ansible_full_src_dir: "{{ zuul.executor.work_root }}/{{ kolla_ansible_src_dir }}"
-    - name: ensure /etc/kolla exists
-      file:
-        path: "/etc/kolla"
-        state: "directory"
-        mode: 0777
-      become: true
-
-    - name: copy default ansible kolla-ansible inventory
-      template:
-        src: "{{ kolla_ansible_full_src_dir }}/tests/templates/inventory.j2"
-        dest: "{{ kolla_inventory_path }}"
-      delegate_to: "primary"
-
-      delegate_to: "primary"
-    - name: generate global.yml file
-      template:
-        src: "{{ kolla_ansible_full_src_dir }}/tests/templates/globals-default.j2"
-        dest: /etc/kolla/globals.yml
-      delegate_to: "primary"
-
-    - name: ensure nova conf overrides dir exists
-      file:
-        path: "/etc/kolla/config/nova"
-        state: "directory"
-        mode: 0777
-      when: scenario != "bifrost"
-      become: true
-      delegate_to: "primary"
-
-    - name: generate nova config overrides
-      template:
-        src: "{{ kolla_ansible_full_src_dir }}/tests/templates/nova-compute-overrides.j2"
-        dest: /etc/kolla/config/nova/nova-compute.conf
-      when: scenario != "bifrost"
-      delegate_to: "primary"
-
-    - name: ensure bifrost conf overrides dir exists
-      file:
-        path: "/etc/kolla/config/bifrost"
-        state: "directory"
-        mode: 0777
-      when: scenario == "bifrost"
-      become: true
-      delegate_to: "primary"
-
-    - name: generate bifrost DIB config overrides
-      template:
-        src: "{{ kolla_ansible_full_src_dir }}/tests/templates/bifrost-dib-overrides.j2"
-        dest: /etc/kolla/config/bifrost/dib.yml
-      when: scenario == "bifrost"
-      delegate_to: "primary"
-    - name: ensure /etc/docker exists
-        path: "/etc/docker"
-    - name: create deamon.json for nodepool cache
-      template:
-        src: "{{ kolla_ansible_full_src_dir }}/tests/templates/docker_daemon.json.j2"
-        dest: "/etc/docker/daemon.json"
-      become: true
-        requirements: "{{ ansible_env.HOME }}/{{ kolla_ansible_src_dir }}/requirements.txt"
-        src: "{{ kolla_ansible_full_src_dir }}/etc/kolla/passwords.yml"
-    - name: generate ceph config overrides
-      template:
-        src: "{{ kolla_ansible_full_src_dir }}/tests/templates/ceph-overrides.j2"
-        dest: /etc/kolla/config/ceph.conf
-      when: scenario == "ceph"
-      delegate_to: "primary"
-
-      shell:
-        cmd: tools/setup_gate.sh
-        NODEPOOL_TARBALLS_MIRROR: "http://{{ zuul_site_mirror_fqdn }}:8080/tarballs"
-          shell:
-            cmd: tests/deploy.sh
-          shell:
-            cmd: tests/test-openstack.sh
-          shell:
-            cmd: tests/test-scenario-nfv.sh
-          shell:
-            cmd: tests/reconfigure.sh
-      when: scenario != "bifrost"
BREAKS HERE
-    logical_switch_group_name: 'Logical Switch Group Cisco Nexus 55xx'  # Logical Switch Group which the logical switch is derived from
-    ip_address_switch_1: '172.18.16.1'  # IP address/hostname of Switch 1
-    ip_address_switch_2: '172.18.16.2'  # IP address/hostname of Switch 2
-    ssh_username: ''  # SSH Username
-    ssh_password: ''  # SSH Password
-    - name: Update the Logical Switch name and credentials
-            newName: 'Test Logical Switch - Renamed'
-    - name: Reclaim the top-of-rack switches in the logical switch
-      oneview_logical_switch:
-        config: "{{ config }}"
-        state: refreshed
-        data:
-          logicalSwitch:
-            name: 'Test Logical Switch - Renamed'
-      delegate_to: localhost
-
-    - name: Delete the Logical Switch
-      oneview_logical_switch:
-        config: "{{ config }}"
-        state: absent
-        data:
-          logicalSwitch:
-            name: 'Test Logical Switch - Renamed'
-      delegate_to: localhost
BREAKS HERE
-    - { role: kubedns, tags: kubedns }
BREAKS HERE
-
BREAKS HERE
-  with_items: volumes
BREAKS HERE
-      until: pgrep_apt_cacher_ng | success
BREAKS HERE
-      # environment:
-      #   AWS_ACCESS_KEY_ID: "{{aws_access_key_id}}"
-      #   AWS_SECRET_ACCESS_KEY: "{{aws_secret_access_key}}"
-      #   AWS_DEFAULT_REGION: "{{aws_region}}"
BREAKS HERE
-      delay: 30
BREAKS HERE
-  template: src={{ item }} dest=/srv/{{ demo_hostname }}/www/ mode=0666
BREAKS HERE
-# Drop the release file everywhere
BREAKS HERE
-      security_groups: "{{ topology_node.security_groups | default([]) | map('regex_replace', '(.*)', prefix + '\\1') | list or omit }}"
BREAKS HERE
-        command: "yum localinstall -y {{ product.rpmrepo[ansible_distribution] }}/{{ product.name }}-release-latest.noarch.rpm"
-      - name: Execute rhos-release for packstack puddle 
-        command: "rhos-release {{ product.version|int }}"
-        when: product.repo_type in ['puddle']
-
-      - name: Execute rhos-release for packstack poodle 
-        command: "rhos-release -d {{ product.version|int }}"
-        when: product.repo_type in ['poodle']
BREAKS HERE
-      when: ceph_docker_image_tag is search("ubuntu")
-      when: ceph_docker_image_tag is match("latest") or ceph_docker_image_tag is search("centos") or ceph_docker_image_tag is search("fedora")
-      when: ceph_docker_image_tag is match("latest") or ceph_docker_image_tag is search("ubuntu")
-      when: ceph_docker_image_tag is search("centos") or ceph_docker_image is search("rhceph") or ceph_docker_image_tag is search("fedora")
-      when: ceph_docker_image_tag is match("latest") or ceph_docker_image_tag is search("ubuntu")
-      when: ceph_docker_image_tag is match("latest") or ceph_docker_image_tag is search("centos") or ceph_docker_image_tag is search("fedora")
-      when: ceph_docker_image_tag is match("latest") or ceph_docker_image_tag is search("ubuntu")
-      when: ceph_docker_image_tag is search("centos") or ceph_docker_image is search("rhceph") or ceph_docker_image_tag is search("fedora")
-      when: ceph_docker_image_tag is match("latest") or ceph_docker_image_tag is search("ubuntu")
-      when: ceph_docker_image_tag is search("centos") or ceph_docker_image is search("rhceph") or ceph_docker_image_tag is search("fedora")
-      when: ceph_docker_image_tag is match("latest") or ceph_docker_image_tag is search("ubuntu")
-      when: ceph_docker_image_tag is search("centos") or ceph_docker_image is search("rhceph") or ceph_docker_image_tag is search("fedora")
-      when: ceph_docker_image_tag is match("latest") or ceph_docker_image_tag is search("ubuntu")
-      when: ceph_docker_image_tag is search("centos") or ceph_docker_image is search("rhceph") or ceph_docker_image_tag is search("fedora")
BREAKS HERE
-    - name: get osd numbers
-        name: ceph-osd@{{ item | basename }}
-      with_items: "{{ devices }}"
BREAKS HERE
-# Version of APR library required for compiling Apache httpd
-httpd_apr_version: 1.5.2
-
-# Version fo APR-util requried for compiling Apache httpd
-httpd_apr_util_version: 1.5.4
-
BREAKS HERE
-rabbitmq_ulimit: 4096
BREAKS HERE
-    vnet_name2: "myVnet{{ rpfx }}"
BREAKS HERE
-    username: "openqa{{ openqa_env_suffix }}"
BREAKS HERE
-            dev: "/dev/{{ mnaio_data_disk }}1"
-            src: "/dev/{{ mnaio_data_disk }}1"
BREAKS HERE
-- name: Debug Jenkins UI Content
-  debug:
-    var: jenkins_ui
-
BREAKS HERE
-    - shell: |
-    - command: rm -rf /tmp/isucon5-qualify
-      args:
-        removes: /tmp/isucon5-qualify
BREAKS HERE
-  pre_tasks:
-    - name: Set filter_with variable if galaxy is run behing proxy
-      set_fact:
-        filter_with: "filter-with = proxy-prefix"
-      when: nginx_galaxy_location is defined and nginx_galaxy_location != ""
BREAKS HERE
-    galaxy_ftp: "{{ galaxy_dir }}/database/ftp"
-    - shell: rm -r /tmp//slurm /etc/munge/munge.key
-      sql_query_user: "SELECT \"email, (CASE WHEN substring(password from 1 for 6) = 'PBKDF2' THEN substring(password from 38 for 69) ELSE password END) AS password2,512,512,'/home/galaxy/galaxy/database/files/%U','/bin/bash' FROM galaxy_user WHERE email='%U'\""
BREAKS HERE
-    - name: stop ceph mon
-        name: ceph-mon@{{ ansible_hostname }}
-        name: ceph-mon@{{ ansible_hostname }}
BREAKS HERE
-      always_run: true
-- hosts: mdss:rgws:clients 
BREAKS HERE
-    when: docker_setup is defined and docker_setup=false
-    when: docker_setup is defined and docker_setup=false
BREAKS HERE
-  - [ "roles/jenkins-slave/defaults/main.yml" ]
-  - [ "roles/jenkins-slave/vars/{{ ansible_distribution }}-{{ ansible_architecture }}.yml", "roles/jenkins-slave/vars/{{ ansible_distribution }}.yml" ]
BREAKS HERE
-# This is a beaker_server role. 
BREAKS HERE
-        pkg: "{{ item }}"
-      with_items: "{{ mnaio_host_distro_packages }}"
BREAKS HERE
-    stig_id: RHEL-07-030525
-    stig_id: RHEL-07-030513
-    stig_id: RHEL-07-030443
-    stig_id: RHEL-07-030390
-    stig_id: RHEL-07-030380
-    stig_id: RHEL-07-030420
-    stig_id: RHEL-07-030561
-    stig_id: RHEL-07-030671
-    stig_id: RHEL-07-030391
-    stig_id: RHEL-07-030392
-    stig_id: RHEL-07-030381
-    stig_id: RHEL-07-030383
-    stig_id: RHEL-07-030404
-    stig_id: RHEL-07-030401
-    stig_id: RHEL-07-030425
-    stig_id: RHEL-07-030670
-    stig_id: RHEL-07-030512
-    stig_id: RHEL-07-030382
-    stig_id: RHEL-07-030405
-    stig_id: RHEL-07-030402
-    stig_id: RHEL-07-030530
-    stig_id: RHEL-07-030524
-    stig_id: RHEL-07-030421
-    stig_id: RHEL-07-030422
-    stig_id: RHEL-07-030423
-    stig_id: RHEL-07-030630
-    stig_id: RHEL-07-030510
-    stig_id: RHEL-07-030540
-    stig_id: RHEL-07-030541
-    stig_id: RHEL-07-030560
-    stig_id: RHEL-07-030403
-    stig_id: RHEL-07-030750
-    stig_id: RHEL-07-030751
-    stig_id: RHEL-07-030444
-    stig_id: RHEL-07-030752
-    stig_id: RHEL-07-030441
-    stig_id: RHEL-07-030442
-    stig_id: RHEL-07-030400
-    stig_id: RHEL-07-030550
-    stig_id: RHEL-07-030521
-    stig_id: RHEL-07-030522
-    stig_id: RHEL-07-030526
-    stig_id: RHEL-07-030424
-    stig_id: RHEL-07-030531
-    stig_id: RHEL-07-030511
-    stig_id: RHEL-07-030753
-    stig_id: RHEL-07-030754
-    stig_id: RHEL-07-030514
-    stig_id: RHEL-07-010090
-    stig_id: RHEL-07-010100
-    stig_id: RHEL-07-010110
-    stig_id: RHEL-07-010120
-    stig_id: RHEL-07-010130
-    stig_id: RHEL-07-010140
-    stig_id: RHEL-07-010150
-    stig_id: RHEL-07-010160
-    stig_id: RHEL-07-010250
-    stig_id: RHEL-07-010180
-    stig_id: RHEL-07-010200
-    stig_id: RHEL-07-010220
-    stig_id: RHEL-07-010420
-    stig_id: RHEL-07-020230
-    stig_id: RHEL-07-020630
BREAKS HERE
-      yum: name={{ item }} state=latest
BREAKS HERE
-  - name: Cluster preparation
-    nsxClusterPrep:
BREAKS HERE
-- name: Check for the presence of a public key file on the host
-  stat:
-- name: Fail if a ssh public key is not set in a var and not present on the host
-    msg: "Please set the lxc_container_ssh_key variable or ensure that the host has the file /root/.ssh/id_rsa.pub present."
BREAKS HERE
-        hostvars[mon_host]['ansible_hostname'] in (ceph_health_raw.stdout | default('{}') |  from_json)["quorum_names"] or
-        hostvars[mon_host]['ansible_fqdn'] in (ceph_health_raw.stdout | default('{}') | from_json)["quorum_names"]
-        {{ container_binary }} exec ceph-mon-{{ hostvars[mon_host]['ansible_hostname'] }} ceph --cluster "{{ cluster }}" -s --format json
-        hostvars[mon_host]['ansible_hostname'] in (ceph_health_raw.stdout | default('{}') | from_json)["quorum_names"] or
-        hostvars[mon_host]['ansible_fqdn'] in (ceph_health_raw.stdout | default('{}') | from_json)["quorum_names"]
BREAKS HERE
-   - /srv/web/infra/ansible/vars/{{ ansible_distribution }}.yml
-   - /srv/web/infra/ansible/vars/{{ ansible_distribution }}.yml
-   - /srv/web/infra/ansible/vars/{{ ansible_distribution }}.yml
BREAKS HERE
-    when: 'pgbdr' not in koschei_pgsql_hostname
-    when: 'pgbdr' not in koschei_pgsql_hostname
BREAKS HERE
-        when: local_src
BREAKS HERE
-- name: Retrieve default configuration files
-  uri:
-    url: "{{ item }}"
-    return_content: yes
-    - "{{ keystone_git_config_lookup_location }}/{{ keystone_paste_git_file_path }}"
-    - "{{ keystone_git_config_lookup_location }}/{{ keystone_sso_callback_git_file_path }}"
-  register: _git_file_fetch
-    - dest: "/etc/keystone/keystone-paste.ini"
-      content: "{{ keystone_paste_user_content | default(keystone_paste_default_content, true) }}"
-    - dest: "/etc/keystone/policy.json-{{ keystone_venv_tag }}"
-      content: "{{ keystone_policy_user_content | default('{}', true) }}"
-    content: "{{ keystone_sso_callback_user_content | default(keystone_sso_callback_default_content, true) }}"
BREAKS HERE
-  changed_when: False
-  ignore_errors: True
-  check_mode: False
-  changed_when: False
-  check_mode: False
-  changed_when: False  # read-only task
-  ignore_errors: True
BREAKS HERE
-- hosts:
-  roles:
-    - { role: common,
-        tags: common }
-- hosts: haproxy
BREAKS HERE
-  vars:
-    container_runtime_docker_storage_type: overlay2
-    container_runtime_docker_storage_setup_device: /dev/sdd
-      shell: oc adm policy add-cluster-role-to-user cluster-admin admin
BREAKS HERE
-      - { proto: 'tcp', from_port: "{{ ssh_port }}", to_port: "{{ ssh_port }}",
-          cidr_ip: '0.0.0.0/0' }
-      - { proto: 'icmp', from_port: 8, to_port: -1, cidr_ip: '0.0.0.0/0' }
-      - { proto: -1, from_port: 0, to_port: 0, cidr_ip: '0.0.0.0/0' }
-    - include: security_groups.yml
-          'user': 'ubuntu',
BREAKS HERE
-        - (glance_default_store | default('none') | lower == 'rbd')
BREAKS HERE
-  - restrict_strategy
-  - NIST-800-53-AC-3(3)
-  - low_disruption
-  - NIST-800-53-AC-4
-  - NIST-800-53-AC-6
-  - NIST-800-53-SI-6(a)
-  - low_complexity
BREAKS HERE
-  - name: AWS Generate CloudFormation Template
-    template:
-      src: "{{ANSIBLE_REPO_PATH}}/configs/{{ env_type }}/files/cloud_providers/{{cloud_provider}}_cloud_template.j2"
-      dest: "{{ANSIBLE_REPO_PATH}}/workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template-orig"
-    tags:
-      - aws_infrastructure_deployment
-      - gen_cf_template
-  # for SSH first access to ec2 instances we always use the key defined in the CloudFormation
-  # template by the name {{key_name}}
-  # This variable is used when generation ssh config.
-  - name: Get ssh pub key
-    set_fact:
-      ssh_key: "~/.ssh/{{key_name}}.pem"
-  ######################### Minimize template (avoid size limitation as much as possible)
-  - name: minimize json
-    shell: "jq -c . < ../workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template-orig >  ../workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template"
-    register: jq_minify
-    ignore_errors: true
-    tags:
-      - aws_infrastructure_deployment
-      - gen_cf_template
-      - minify_template
-  - name: use original if jq failed
-    command: "cp ../workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template-orig ../workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template"
-    when: jq_minify|failed
-    tags:
-      - aws_infrastructure_deployment
-      - gen_cf_template
-      - minify_template
-  ######################### Validate CF Template
-  - name: validate cloudformation template
-    environment:
-      AWS_ACCESS_KEY_ID: "{{aws_access_key_id}}"
-      AWS_SECRET_ACCESS_KEY: "{{aws_secret_access_key}}"
-      AWS_DEFAULT_REGION: "{{aws_region}}"
-    shell: "aws cloudformation validate-template --region {{ aws_region | default(region) | default('us-east-1')}} --template-body file://../workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template"
-    changed_when: false
-    tags:
-      - aws_infrastructure_deployment
-      - validate_cf_template
-  ######################### Launch CF Template
-  - name: Launch CloudFormation template
-    # environment:
-    #   AWS_ACCESS_KEY_ID: "{{aws_access_key_id}}"
-    #   AWS_SECRET_ACCESS_KEY: "{{aws_secret_access_key}}"
-    #   AWS_DEFAULT_REGION: "{{aws_region}}"
-    cloudformation:
-      aws_access_key: "{{ aws_access_key_id }}"
-      aws_secret_key: "{{ aws_secret_access_key }}"
-      stack_name: "{{ project_tag }}"
-      state: "present"
-      region: "{{ aws_region | default(region) | default('us-east-1')}}"
-      disable_rollback: true
-      template: "{{ANSIBLE_REPO_PATH}}/workdir/{{ env_type }}.{{ guid }}.{{cloud_provider}}_cloud_template"
-        Stack: "project {{ project_tag }}"
-        owner: "{{ email | default('unknown') }}"
-    tags:
-      - aws_infrastructure_deployment
-      - provision_cf_template
-    register: cloudformation_out
-    until:
-      - cloudformation_out|succeeded
-      - cloudformation_out.output in ["Stack CREATE complete", "Stack is already up-to-date."]
-    retries: "{{ cloudformation_retries | default(25) }}"
-    delay: 60
-    ignore_errors: yes
-  - name: debug cloudformation
-    debug:
-      var: cloudformation_out
-    tags: provision_cf_template
-    when: not cloudformation_out|succeeded
-  - name: report Cloudformation error
-    fail:
-      msg: "FAIL {{ project_tag }} Create Cloudformation"
-    when: not cloudformation_out|succeeded
-    tags:
-      - provision_cf_template
-  - name: debug cloudformation
-    debug:
-      var: cloudformation_out
-      verbosity: 2
-    tags: provision_cf_template
-  - name: Gather EC2 facts
-    ec2_remote_facts:
-      aws_access_key: "{{ aws_access_key_id }}"
-      aws_secret_key: "{{ aws_secret_access_key }}"
-      region: "{{ aws_region | default(region) | default('us-east-1')}}"
-      filters:
-        instance-state-name: running
-        "tag:Project": "{{project_tag}}"
-    register: ec2_facts
-    tags:
-      - create_inventory
-      - must
-  - name: debug ec2_facts
-    debug:
-      var: ec2_facts
-      verbosity: 2
-  - name: windows ostype workaround
-    set_fact:
-      project_tag_ostype: "{{project_tag}}_ostype"
-    tags:
-      - create_inventory
-      - must
-  - set_fact:
-      stack_tag: "{{env_type | replace('-', '_')}}_{{guid}}"
-    tags:
-      - create_inventory
-      - must
-  - add_host:
-      name: "{{item.tags.internaldns | default(item.private_dns_name)}}"
-      shortname: "{{item.tags.Name | default(item.private_dns_name)}}"
-      groups:
-        - "tag_Project_{{stack_tag}}"
-        - "tag_{{stack_tag}}_{{item['tags'][project_tag] | default('unknowns')}}"
-        - "tag_{{stack_tag}}_ostype_{{item['tags'][project_tag_ostype] | default('unknown')}}"
-        - "{{item.tags.ostype | default('unknowns')}}"
-        - "{{item['tags'][project_tag_ostype] | default('unknowns')}}"
-        - "{{ 'newnodes' if (item.tags.newnode|d()|bool) else 'all'}}"
-      ansible_ssh_user: ec2-user
-      remote_user: ec2-user
-      ansible_ssh_private_key_file: "{{item['key_name']}}"
-      key_name: "{{item['key_name']}}"
-      state: "{{item['state']}}"
-      internaldns: "{{item.tags.internaldns | default(item.private_dns_name)}}"
-      instance_id: "{{ item.id }}"
-      region: "{{item['region']}}"
-      public_dns_name: "{{item['public_dns_name']}}"
-      private_dns_name: "{{item['private_dns_name']}}"
-      private_ip_address: "{{item['private_ip_address']}}"
-      public_ip_address: "{{item['public_ip_address']}}"
-      placement: "{{item['placement']['zone']}}"
-      image_id: "{{item['image_id']}}"
-      ansible_ssh_extra_args: "-o StrictHostKeyChecking=no"
-    with_items: "{{ec2_facts['instances']}}"
-    loop_control:
-      label: "{{item.tags.internaldns | default(item.private_dns_name)}}"
-    tags:
-      - create_inventory
-      - must
-  # AnsibleGroup tag can have several comma-separated values. Ex: activedirectories,windows
-  - add_host:
-      name: "{{item.tags.internaldns | default(item.private_dns_name)}}"
-      groups: "{{item.tags.AnsibleGroup}}"
-    with_items: "{{ec2_facts['instances']}}"
-    loop_control:
-      label: "{{item.tags.internaldns | default(item.private_dns_name)}}"
-    tags:
-      - create_inventory
-      - must
-  - name: debug hostvars
-    debug:
-      var: hostvars
-      verbosity: 2
-  - name: debug groups
-    debug:
-      var: groups
-      verbosity: 2
BREAKS HERE
-# file: primogen/tests/vagrant.yml
BREAKS HERE
-               - { name: db_recovery_file_dest_size, value: 4G, state: absent, scope: both } 
-               - { name: db_recovery_file_dest, value: '+FRA', state: absent, scope: both } 
-               - { name: pga_aggregate_target, value: 400M, state: absent, scope: both } 
-               - { name: _parallel_statement_queuing, value: "false", state: absent, scope: both } 
-               - { name: sga_target, value: 2G, state: absent, scope: spfile } 
-               - { name: sga_max_size, value: 2G, state: absent, scope: spfile } 
-               - { name: open_cursors, value: 400, state: absent, scope: both } 
-               - { name: processes, value: 700, state: absent, scope: spfile } 
-               - { name: newtbs, datafile: '+DATA', size: 5M, autoextend: "true", next: 5M, maxsize: 500M, content: permanent, state: absent } 
-               - { name: newundo, datafile: '+DATA', size: 5M, autoextend: "true", next: 5M, maxsize: 500M, content: undo, state: absent } 
-               - { name: newtemp, datafile: '+DATA', size: 5M, autoextend: "true", next: 5M, maxsize: 500M, content: temp, state: absent } 
-             state: absent
-             state: absent
-             state: absent
-             state: absent
-             state: absent
BREAKS HERE
-- include: designate_init.yml
BREAKS HERE
-  hosts: osbs-orchestrator-prod-masters[0]
BREAKS HERE
-    cron: '0 0 21 * * ?'
-
BREAKS HERE
-    with_items: "{{ ceph_osd_docker_devices }}"
-    with_items: "{{ ceph_osd_docker_devices }}"
-    with_items: "{{ ceph_osd_docker_devices }}"
-      - "{{ ceph_osd_docker_devices }}"
-      - "{{ ceph_osd_docker_devices }}"
BREAKS HERE
-- include: teardown.yml
-# The [provision.yml](provision.yml.html) playbook is responsible for
BREAKS HERE
-
-    - debug: msg={{ item }}
-      with_items:
-        - "Playbook complete. Remaining tasks:"
-        - "- Clone and set up private dotfiles"
-        - "- Import desired ssh keys to ~/.ssh/keys"
BREAKS HERE
-      command: "sudo scripts/gate-check-commit.sh {{ scenario }} {{ action }}"
-      environment: '{{ zuul | zuul_legacy_vars }}'
BREAKS HERE
-      instance_type: t2.large
BREAKS HERE
-  post_tasks:
-    # FIXME:
-    # PyYAML 5.1 gets installed as a dependency to ansible or the oslo.config
-    # package, but this version has a bug that prevents proper rendering of
-    # lists when generating docker-compose files.
-    # Till it gets fixed, install specific version of PyYAML required for the
-    # install_contrail.yml playbook. Can be removed after its fixed upstream
-    - name: Install required PyYAML version
-      pip:
-        name: pyyaml==3.13
BREAKS HERE
-            nics_subfolder: "{{ (install.version|default(undercloud_version)|openstack_release > 10) | ternary('osp11', 'legacy') }}"
BREAKS HERE
-    - name: Do nothing when Logical Interconnect Group is absent
BREAKS HERE
-        value: httpd:latest
-        value: httpd:2.4
BREAKS HERE
-    - scaleio_common
-  tags: scaleio_common
-    - scaleio_mdm
-  tags: scaleio_mdm
-    - scaleio_tb
-  tags: scaleio_tb
-#    - scaleio_gateway
-#  tags: scaleio_gateway
-    - scaleio_sds
-  tags: scaleio_sds
-    - scaleio_lia
-  tags: scaleio_lia
-    - scaleio_sdc
-  tags: scaleio_sdc
-    - scaleio_gui
-  tags: scaleio_gui
BREAKS HERE
-    - name: Print upgrade var
-      debug:
-        msg: "{{ splunk.upgrade }}"
-      ignore_errors: true
-
BREAKS HERE
-# Small playbook with logic for when to build images or not.
-# The logic there can be completely overridden by setting the
-# to_build variable.
-- include: to-build-or-not-to-build.yml
-
-
-    - { role: repo-setup, repo_inject_image_path: "$HOME/ironic-python-agent.initramfs", repo_run_live: false, initramfs_image: true, when: not to_build|bool }
-    - { role: install-built-repo, ib_repo_image_path: "$HOME/ironic-python-agent.initramfs", initramfs_image: true, when: compressed_gating_repo is defined and not to_build|bool }
BREAKS HERE
-    - "{{ iscsi_gw_group_name|default('iscsi-gw') }}"
BREAKS HERE
-          filter: "storageVolumeUri='/rest/storage-volumes/E5B84BC8-75CF-4305-8DB5-7585A2979351'"
-        serverProfileName: "sp-bdd"
-        storageVolumeUri: "/rest/storage-volumes/89118052-A367-47B6-9F60-F26073D1D85E"
-        serverProfileName: "sp-bdd"
-        storageVolumeName: "volume-attachment-demo"
-        serverProfileName: "sp-bdd"
-        storageVolumeName: "volume-attachment-demo"
BREAKS HERE
-- include: openstack_kernel_check.yml
-  tags:
-    - openstack_hosts-install
-
-- include: openstack_proxy_settings.yml
-  tags:
-    - openstack_hosts-install
-    - openstack_hosts-config
-
-- include: openstack_host_install.yml
-  tags:
-    - openstack_hosts-install
-
-- include: openstack_sysstat.yml
-    - openstack_hosts-config
-- include: openstack_update_hosts_file.yml
-  static: no
-  when: openstack_host_manage_hosts_file | bool
-    - openstack_hosts-install
-- include: openstack_kernel_modules.yml
-- include: openstack_kernel_tuning.yml
-    - openstack_hosts-config
-- include: openstack_release.yml
-  tags:
-    - openstack_hosts-install
BREAKS HERE
-- name: "Group for workaround of RHBZ#1262357 - ovs-vsctl hangs"
-  hosts: openstack_nodes
-  gather_facts: no
-  sudo: no
-  tasks:
-      - group_by: key=workaround_bz1262357
-        when: workarounds['bz1262357']['enabled'] is defined and workarounds['bz1262357']['enabled'] | bool
-
-- name: "Workaround RHBZ#1262357 new ovs-vsctl hangs"
-  hosts: workaround_bz1262357
-  gather_facts: no
-  sudo: yes
-  tasks:
-      - copy:
-          dest: /tmp/bz1262357.te
-          content: |
-              module bz1262357 1.0;
-
-              require {
-                  type modules_dep_t;
-                  type openvswitch_t;
-                  class file { read getattr open };
-              }
-
-              #============= openvswitch_t ==============
-              allow openvswitch_t modules_dep_t:file { read getattr open };
-        register: bz1262357_te_file
-      - command: checkmodule -M -m -o /tmp/bz1262357.mod /tmp/bz1262357.te
-        when: bz1262357_te_file|changed
-      - command: semodule_package -o /tmp/bz1262357.pp -m /tmp/bz1262357.mod
-        when: bz1262357_te_file|changed
-      - command: semodule -i /tmp/bz1262357.pp
-        when: bz1262357_te_file|changed
-
BREAKS HERE
-  until: container_extra_commands | success
BREAKS HERE
-    {{ item.size | default('') }}
BREAKS HERE
-
-# Deploy the undercloud
-- name:  Install undercloud
-  hosts: undercloud
-  gather_facts: no
-  roles:
-    - tripleo/undercloud
BREAKS HERE
-    with_items: "{{ docs_instances.results | map(attribute='tagged_instances') | list }}"
-    with_items: "{{ workstation_instances.results | map(attribute='tagged_instances') | list }}"
BREAKS HERE
-        test "[""$(ceph -s -f json | python -c 'import sys, json; print(json.load(sys.stdin)["pgmap"]["num_pgs"])')""]" = "$(ceph -s -f json | python -c 'import sys, json; print [ i["count"] for i in json.load(sys.stdin)["pgmap"]["pgs_by_state"] if i["state_name"] == "active+clean"]')"
BREAKS HERE
-- hosts: [ceph-mon, ceph-osd]
-    - { role: ceph, tags: ceph, when: enable_ceph | bool }
-- hosts: [haproxy, mariadb, rabbitmq, cinder-api, glance-api, keystone, murano-api, nova-api, neutron-server, swift-proxy-server]
-    - { role: haproxy, tags: haproxy, when: enable_haproxy | bool }
-    - { role: memcached, tags: memcache, memcached, when: enable_memcached | bool }
-    - { role: mariadb, tags: mariadb, when: enable_mariadb | bool }
-    - { role: rabbitmq, tags: rabbitmq, when: enable_rabbitmq | bool }
-    - { role: keystone, tags: keystone, when: enable_keystone | bool }
-- hosts: [swift-proxy-server, swift-account-server, swift-object-server, swift-container-server ]
-    - { role: swift, tags: swift, when: enable_swift | bool }
-- hosts: [glance-api, glance-registry, ceph-mon]
-    - { role: glance, tags: glance, when: enable_glance | bool }
-- hosts: [nova-api, nova-conductor, nova-consoleauth, nova-novncproxy, nova-scheduler, compute, ceph-mon, glance-api]
-    - { role: nova, tags: nova, when: enable_nova | bool }
-- hosts: [neutron-server, neutron-agents, compute]
-    - { role: neutron, tags: neutron, when: enable_neutron | bool }
-- hosts: [cinder-api, cinder-backup, cinder-scheduler, cinder-volume, ceph-mon]
-    - { role: cinder, tags: cinder, when: enable_cinder | bool }
-- hosts: [heat-api, heat-api-cfn, heat-engine]
-    - { role: heat, tags: heat, when: enable_heat | bool }
-    - { role: horizon, tags: horizon, when: enable_horizon | bool }
-- hosts: [murano-api, murano-engine]
-    - { role: murano, tags: murano, when: enable_murano | bool }
-- hosts: [ironic-api, ironic-conductor, ironic-discoverd, ironic-pxe]
-    - {role: ironic, tags: ironic, when: enable_ironic | bool }
BREAKS HERE
-# ansible-playbook -e ireallymeanit=yes|no shrink-cluster.yml
BREAKS HERE
-    target: https://alt.stg.fedoraproject.org/cloud/
-    target: https://alt.stg.fedoraproject.org/cloud/
-    target: https://alt.fedoraproject.org/cloud/
-    target: https://alt.fedoraproject.org/cloud/
BREAKS HERE
-      Subsystem: "sftp /usr/lib/openssh/sftp-server"
BREAKS HERE
-  hosts: virthost:bvirthost:buildvmhost:virthost-comm:colo-virt:communnishift
BREAKS HERE
-    server_aliases:
-    - src.stg.fedoraproject.org
BREAKS HERE
-
-    - name: Job Template to launch a Job Template with update on launch inventory set
-      uri:
-        url: "https://{{ ansible_tower_ip }}/api/v1/job_templates/{{ job_template_id }}/launch/"
-        method: POST
-        user: "{{tower_admin}}"
-        password: "{{tower_admin_password}}"
-        body:
-          extra_vars:
-            guid: "{{guid}}"
-            ipa_host_password: "{{ipa_host_password}}"
-        body_format: json
-        validate_certs: False
-        HEADER_Content-Type: "application/json"
-        status_code: 200, 201
-      when: tower_run == 'true'
-- name: create our own inventory 
-  hosts: localhost                                                                                                                                                                                                                    
-  connection: local                                                                                                                                                                                                                   
-  gather_facts: false                                                                                                                                                                                                                 
-  become: false                                                                                                                                                                                                                       
-  vars_files:                                                                                                                                                                                                                         
-    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"                                                                                                                                                                   
-    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_secret_vars.yml"                                                                                                                                                            
-  tags:                                                                                                                                                                                                                               
-    - step001                                                                                                                                                                                                                         
-    - pre_infrastructure                                                                                                                                                                                                              
-    - generate_env_keys                                                                                                                                                                                                               
-                                                                                                                                                                                                                                      
-  - name: get rid of ll hostvars
-    ec2_remote_facts:
-        "tag:Workshop": "{{guid}}"
-  - name: print our new instances                                                                                                                                                                                               
-     msg: "instance short name is: {{item.tags.short_name}}"
-#     var: item
-    with_items: "{{ec2_facts['instances']}}"
-  - name: print our new instances                                                                                                                                                                                               
BREAKS HERE
-# Configure host groups based on inventory results
-- include: plays/configure_host_groups.yml
-
BREAKS HERE
-  tags: dovecot, debug
BREAKS HERE
-      User-Agent: [python-requests/2.18.4]
-    uri: https://or.deploy4.dev.atix/katello/api/v2/ping
-        H4sIAAAAAAAAA43NPQqAMAyA0d1TSGYX/1C8TAm2YlHb0qQu0rtr3ZzqFBLeR8ryAmLkQDCB3aAC
-        Uv7Us3r2C1zYXZpfIYNH1taII52GAWL1SoGB1xzv+sRnNHJXTpscb9oP//WiGVOzWK8ONIKRNsol
-        9ZPEWNxeGq+sDgEAAA==
-      Apipie-Checksum: [f9d0632466d4f19857657a25e9d550ba]
-      Content-Length: ['127']
-      Date: ['Tue, 05 Jun 2018 14:02:45 GMT']
-      ETag: [W/"6551cfc5e297e81b4612a35f56da82b4-gzip"]
-      Foreman_version: [1.16.0]
-      Set-Cookie: [_session_id=2a617cdeb343d0a4a7843c9d87634a71; path=/; secure; HttpOnly]
-      Strict-Transport-Security: [max-age=631152000; includeSubdomains]
-      X-Powered-By: [Phusion Passenger 4.0.53]
-      X-Request-Id: [5b7f2801-47af-4f67-8aa5-1062bb700e27]
-      X-Runtime: ['0.250515']
-      User-Agent: [python-requests/2.18.4]
-    uri: https://or.deploy4.dev.atix/api/v2/job_templates
-        H4sIAAAAAAAAA32OsW6EMBBEe74Cbe2G04UIpPRpUl15FyFjFuLIeK21HeWE+PcscEq6dDtvNDuz
-        FGUJiZJ20JbVk9pkzP0v2UHQE/4J5O4Bzqfm3NTPp6Y+cqjZfAgHr2d8uYEhP9opM5ajy9+loTmg
-        jzoR3wCOCHGSwCK3qP4ut8/OqUMTD8gPJGTdI4wxuxQFXxewA7RVXam9Edp/GkHBJ/Wd0QknYimC
-        NxsNOqc9Uo5iB6YvK4Vduoft1+XyKjR6GwLKyFG7iAoGjIZtSJZ8NxLPWqxt3/perMUPdz6l304B
-        AAA=
-      Apipie-Checksum: [f9d0632466d4f19857657a25e9d550ba]
-      Content-Length: ['230']
-      Date: ['Tue, 05 Jun 2018 14:02:45 GMT']
-      ETag: [W/"09436e4b8afda2ebb479d305a834e670-gzip"]
-      Foreman_version: [1.16.0]
-      Set-Cookie: [_session_id=a05a39fb9e172209bb83d21c393c7a38; path=/; secure; HttpOnly]
-      Strict-Transport-Security: [max-age=631152000; includeSubdomains]
-      X-Powered-By: [Phusion Passenger 4.0.53]
-      X-Request-Id: [336b757c-3187-452c-ba8e-7e524d16f318]
-      X-Runtime: ['0.077921']
-      User-Agent: [python-requests/2.18.4]
-    uri: https://or.deploy4.dev.atix/api/v2/locations
-        H4sIAAAAAAAAA42PMU/EMAyF9/6KyHNPakMotBITK+MxcajKpRZUCknluAOq+t9x0zshEAOb3+f3
-        4pelUAo4svXQKV1uKs3nK6gzmOwbfguk/gKMbk3b3Om22XNoyb0Lh2A/8OEER0ysnqKzPMZwAthd
-        kVg8i8yizp8yh9n7cteRBqQLErLmCGGaPSfBLwvY4ORZklyOSTnCwP04/AJbhytyhJZx6K1cBl3V
-        94eqOVS3qjZdpbsbo56Pj1DCPA3/sW2ntClhv/Dzl7Lmkf1ffMDkaJyyyr3W12ItvgAdg8oSgAEA
-        AA==
-      Apipie-Checksum: [f9d0632466d4f19857657a25e9d550ba]
-      Content-Length: ['229']
-      Date: ['Tue, 05 Jun 2018 14:02:45 GMT']
-      ETag: [W/"c7031fede74e3572a3cd49063ac95796-gzip"]
-      Foreman_version: [1.16.0]
-      Set-Cookie: [_session_id=924bce266f3087a57b8ab3bd4cd417cb; path=/; secure; HttpOnly]
-      Strict-Transport-Security: [max-age=631152000; includeSubdomains]
-      X-Powered-By: [Phusion Passenger 4.0.53]
-      X-Request-Id: [01d86d9e-26c7-4c98-aba8-c4d81e433d63]
-      X-Runtime: ['0.077619']
-      User-Agent: [python-requests/2.18.4]
-    uri: https://or.deploy4.dev.atix/katello/api/v2/organizations
-        H4sIAAAAAAAAA42PwUrEMBCG732KMOcutLFWG/DkA3hZT66UaTPsBmJbkulBS9/d2bSiCIK3+b78
-        w59ZMqWAR0YPRun8SnHuvkSZxIRn+gYK7S4q3VRNfaebetsjDP1FPAz4Rg8nOFJk9RTOOLgPZDcO
-        J4AtOQaW3CKzUPcu8zB7n288BkthV2LWtBIozp6j6JcFPHYk30sF7c8CyKEPhEy2RWkAXZT3h6I+
-        FLeqrEyhja7V8/FRYvNk/xNzFoy+ydNFe6H6VciO/V9vlmIf3JTIXO9ZX7M1+wTyZfr/dAEAAA==
-      Apipie-Checksum: [f9d0632466d4f19857657a25e9d550ba]
-      Date: ['Tue, 05 Jun 2018 14:02:45 GMT']
-      ETag: [W/"db5fe9cf6dba48654fd4161759ccecae-gzip"]
-      Foreman_version: [1.16.0]
-      Set-Cookie: [_session_id=f6cf7798c4784900dfd0bf0c07b80f8e; path=/; secure; HttpOnly]
-      Strict-Transport-Security: [max-age=631152000; includeSubdomains]
-      X-Powered-By: [Phusion Passenger 4.0.53]
-      X-Request-Id: [327d953b-1d14-4e61-b8c2-baa306fdd7a5]
-      X-Runtime: ['0.076924']
-      User-Agent: [python-requests/2.18.4]
-    uri: https://or.deploy4.dev.atix/api/v2/job_templates/161
-        H4sIAAAAAAAAA32Qu3LCMBBFf0WzNTA2JjZxly5NKlKl0SjSmizoNXo4MJn8e6QQoEt75uy9u/sF
-        pGBs+3YBVhiEEaSzE+1zQDbpfGLSGY82iuQCLCCh8Vqk6j2xg3tnV8A+KX0wYW+gyNGS95hgnISO
-        eB/mR7KK116btV6ADFio4qKosG7a7bLpl83Da7sZm/XYDatt9/hW8rJX/3ibbjX0Q/W0k0dUt1qF
-        k8j6vsaMVpVj/spd5JMwpM9XUK7isvTsXSgMXihK1FpYdDmWcB/cTAoDT2df37DbPUPtiDKQT+Qs
-        n1wwdcVLHJ5Q5l+eyJSMxMkmDLPQF+P7B/CvrxCCAQAA
-      Apipie-Checksum: [f9d0632466d4f19857657a25e9d550ba]
-      Content-Length: ['261']
-      Date: ['Tue, 05 Jun 2018 14:02:45 GMT']
-      ETag: [W/"eb1c443a7b01963846a80c39e8bb0b33-gzip"]
-      Foreman_version: [1.16.0]
-      Set-Cookie: [request_method=DELETE; path=/, _session_id=b061f755bbcd8661810fc3884d70dfbd;
-          path=/; secure; HttpOnly]
-      Strict-Transport-Security: [max-age=631152000; includeSubdomains]
-      X-Powered-By: [Phusion Passenger 4.0.53]
-      X-Request-Id: [f9e65903-6c64-44a6-8bd9-ec2a4ab43b6c]
-      X-Runtime: ['0.107711']
BREAKS HERE
-        dest: "{{ tmpdir }}/data/{{ inventory_hostname }"
BREAKS HERE
-      include_tasks: "/opt/plexguide/containers/_core.yml"
BREAKS HERE
-  - { role: koji_builder, when: env == "staging" and inventory_hostname.startswith('s390') }}
BREAKS HERE
-      line: "space_left = {{ [rhel7stig_auditd_space_left, 51] | min }}"
BREAKS HERE
-      shell: ping -i 5 -c 6 8.8.8.8
BREAKS HERE
-  run_once: yes
BREAKS HERE
-run_once: Boolean that will bypass the host loop, forcing the task to execute on the first host available and will also apply any facts to all active hosts.
BREAKS HERE
-  when: >
-    inventory_hostname == groups['nova_api_os_compute'][0]
-  when: >
-    inventory_hostname == groups['nova_api_os_compute'][0]
-  when: >
-    inventory_hostname in groups['nova_compute']
BREAKS HERE
-- name: Playbook for creating containers
BREAKS HERE
-    - glance-registry
BREAKS HERE
-      pause:
-        seconds: 300 
BREAKS HERE
-  when: "{{ openstack_host_distro_packages | length > 0 }}"
BREAKS HERE
-  powervm:
-    nova_compute_driver: powervm.driver.PowerVMDriver
-    nova_reserved_host_memory_mb: 8192
-    nova_scheduler_tracks_instance_changes: True
-  - powervm
-nova_compute_powervm_pip_packages:
-  - nova-powervm
-  - pyasn1-modules
-
BREAKS HERE
-    until: add_service|success
-    until: add_service|success
-    until: add_service|success
-    until: add_service|success
BREAKS HERE
-    - "inventory_hostname == (groups[neutron_services['neutron-server']['group']] | intersect(ansible_play_hosts))[0]"
BREAKS HERE
-        zuul_changes: "{{ zuul_changes|regex_replace(':[a-z]*:refs/changes/\\d{2}/\\d{6}/\\d{1}','') }}"
BREAKS HERE
-# 'undercloud_version' has been setted as host's fact for undercloud nodes.
-- include: tasks/ironic_pxe_ipmitool.yml
-  when: "install.version|default(hostvars[groups['undercloud'][0]]['undercloud_version']) | openstack_release >= 11"
-      - name: Retrieve public key from private key to localhost
-        command: "ssh-keygen -y -f ~/.ssh/id_rsa"
-        register: uc_pubkey_result
-
-      - name: insert the public key to the known hosts in hypervisor
-        authorized_key:
-            user: "{{ ansible_ssh_user }}"
-            key: "{{ uc_pubkey_result.stdout }}"
-        delegate_to: hypervisor
-
BREAKS HERE
-        - "dbsync_contract | succeeded"
BREAKS HERE
-  yaml: {key: defaults.cow_selection}
-  yaml: {key: defaults.cow_whitelist}
-  description: This options forces color mode even when running without a TTY
-  yaml: {key: defaults.force_color}
-  yaml: {key: defaults.nocolor}
-  yaml: {key: defaults.nocows}
-  description: 'TODO: write it'
-  description: Sets the default value for the any_errors_fatal keyword
-  yaml: {key: errors.anyerrors_fatal}
-  description: This setting controls if become is skipped when remote user and become user are the same.
-  description: Chooses which cache plugin to use
-  yaml: {key: defaults.fact_caching}
-  default:
-  yaml: {key: defaults.fact_caching_connection}
-  yaml: {key: defaults.fact_caching_prefix}
-  yaml: {key: defaults.fact_caching_timeout}
-  description: Defines the color to use on 'Changed' status
-  yaml: {key: colors.debug}
-  yaml: {key: colors.deprecate}
-  yaml: {key: colors.diff_add}
-  yaml: {key: colors.diff_lines}
-  yaml: {key: colors.diff_remove}
-  description: 'TODO: write it'
-  yaml: {key: colors.highlight}
-  description: Defines the color to use when showing 'OK' status
-  yaml: {key: colors.ok}
-  description: Defines the color to use when showing 'Skipped' status
-  yaml: {key: colors.skip}
-  yaml: {key: colors.unreachable}
-  description: Defines the color to use when emitting verbose messages
-  yaml: {key: colors.verbose}
-  yaml: {key: colors.warn}
-  description: 'TODO: write it'
-  yaml: {key: defaults.command_warnings}
-  type: pathlist
-  yaml: {key: defaults.action_plugins}
-  description: 'TODO: write it'
-  yaml: {key: defaults.allow_unsafe_lookups}
-  description: 'TODO: write it'
-  description: 'TODO: write it'
-  description: 'TODO: write it'
-  description: 'TODO: write it'
-  yaml: {key: defaults.ask_vault_pass}
-  yaml: {key: privilege_escalation.become}
-  yaml: {key: privilege_escalation.become_ask_pass}
-  yaml: {key: privilege_escalation.become_method}
-  yaml: {key: privilege_escalation.become_exe}
-  yaml: {key: privilege_escalation.become_flags}
-  description: User your become when using privilege escalation, most systems will use 'root' when no user is specified.
-  yaml: {key: privilege_escalation.become_user}
-  description: 'TODO: write it'
-  type: pathlist
-  yaml: {key: defaults.cache_plugins}
-  description: 'TODO: write it'
-  yaml: {key: defaults.callable_whitelist}
-  description: 'TODO: write it'
-  type: pathlist
-  yaml: {key: defaults.callback_plugins}
-  description: 'TODO: write it'
-  yaml: {key: defaults.callback_whitelist}
-  description: 'TODO: write it'
-  type: pathlist
-  yaml: {key: defaults.connection_plugins}
-  yaml: {key: defaults.debug}
-  description: 'TODO: write it'
-  yaml: {key: defaults.executable}
-  default:
-  description: 'TODO: write it'
-  yaml: {key: defaults.fact_path}
-  description: 'TODO: write it'
-  type: pathlist
-  yaml: {key: defaults.filter_plugins}
-  description: 'TODO: write it'
-  yaml: {key: defaults.force_handlers}
-  yaml: {key: defaults.forks}
-    default: 'implicit'
-    description: 'TODO: write it'
-    env: [{name: ANSIBLE_GATHERING}]
-    ini:
-      - key: gathering
-        section: defaults
-    yaml: {key: defaults.gathering}
-    default: 'all'
-    description: 'TODO: write it'
-    env: [{name: ANSIBLE_GATHER_SUBSET}]
-    ini:
-      - key: gather_subset
-        section: defaults
-    yaml: {key: defaults.gather_subset}
-  description: 'TODO: write it'
-  description: 'TODO: write it'
-  yaml: {key: defaults.handler_includes_static}
-  description: 'TODO: write it'
-  yaml: {key: defaults.hash_behaviour}
-  description: Location of the Ansible inventory source.
-  env: [{name: ANSIBLE_INVENTORY}]
-  - {key: inventory, section: defaults}
-  description: 'TODO: write it'
-  yaml: {key: defaults.internal_poll_interval}
-  description: 'TODO: write it'
-  type: pathlist
-  yaml: {key: defaults.inventory_plugins}
-  default:
-  description: 'TODO: write it'
-  yaml: {key: defaults.jinja2_extensions}
-  yaml: {key: defaults.keep_remote_files}
-  description: 'TODO: write it'
-  yaml: {key: selinux.libvirt_lxc_noseclabel}
-  description: 'TODO: write it'
-  yaml: {key: defaults.bin_ansible_callbacks}
-  yaml: {key: defaults.local_tmp}
-  yaml: {key: defaults.log_path}
-  description: 'TODO: write it'
-  type: pathlist
-  default: Ansible managed
-  description: Sets the macro for the 'ansible_managed' variable available for 'tempalte' tasks.
-  description: 'TODO: write it'
-  yaml: {key: defaults.module_args}
-  description: 'TODO: write it'
-  yaml: {key: defaults.module_compression}
-  # TODO: allow setting to function: os.getenv('LANG', 'en_US.UTF-8')
-  yaml: {key: defaults.module_lang}
-  description: Module to use with the `ansible` AdHoc command, if none is specified.
-  yaml: {key: defaults.module_name}
-  description: 'TODO: write it'
-  type: pathlist
-  yaml: {key: defaults.library}
-  yaml: {key: defaults.module_set_locale}
-  description: 'TODO: write it'
-  type: pathlist
-  yaml: {key: defaults.module_utils}
-  yaml: {key: defaults.no_log}
-  default:
-  description: 'TODO: write it'
-  yaml: {key: defaults.null_representation}
-  description: 'TODO: write it'
-  yaml: {key: defaults.poll_interval}
-  default:
-  description: 'TODO: write it'
-  yaml: {key: defaults.private_key_file}
-  description: 'TODO: write it'
-  default:
-  yaml: {key: defaults.remote_tmp}
-  yaml: {key: defaults.remote_user}
-  description: 'TODO: write it'
-  type: pathlist
-  yaml: {key: ssh_connection.scp_if_ssh}
-
-  yaml: {key: selinux.special_context_filesystems}
-  yaml: {key: defaults.squash_actions}
-  yaml: {key: ssh_connection.transfer_method}
-  yaml: {key: defaults.stdout_callback}
-  yaml: {key: defaults.strategy}
-  description: 'TODO: write it'
-  type: pathlist
-  yaml: {key: defaults.strategy_plugins}
-  description: 'TODO: write it'
-  yaml: {key: defaults.syslog_facility}
-  description: 'TODO: write it'
-  yaml: {key: defaults.task_includes_static}
-  description: 'TODO: write it'
-  type: pathlist
-  yaml: {key: defaults.test_plugins}
-  description: 'TODO: write it'
-  yaml: {key: defaults.timeout}
-  yaml: {key: defaults.transport}
-  description: 'TODO: write it'
-  yaml: {key: defaults.error_on_undefined_vars}
-  description: 'TODO: write it'
-  type: pathlist
-  yaml: {key: defaults.vars_plugins}
-DEFAULT_VAR_COMPRESSION_LEVEL:
-  default: 0
-  description: 'TODO: write it'
-  env: [{name: ANSIBLE_VAR_COMPRESSION_LEVEL}]
-  ini:
-  - {key: var_compression_level, section: defaults}
-  type: integer
-  yaml: {key: defaults.var_compression_level}
-  description: 'TODO: write it'
-  yaml: {key: defaults.verbosity}
-  yaml: {key: defaults.deprecation_warnings}
-  description: 'TODO: write it'
-  yaml: {key: diff.always}
-  description: 'TODO: write it'
-  yaml: {key: diff.context}
-  description: 'TODO: write it'
-  yaml: {key: defaults.display_args_to_stdout}
-  description: "Toggle to control displaying skipped host entries in a task in the default callback"
-  yaml: {key: defaults.display_skipped_hosts}
-  yaml: {key: defaults.error_on_missing_handler}
-  description: 'TODO: write it'
-  yaml: {key: galaxy.ignore_certs}
-  description: 'TODO: write it'
-  yaml: {key: galaxy.role_skeleton}
-  description: 'TODO: write it'
-  yaml: {key: galaxy.role_skeleton_ignore}
-GALAXY_SCMS:
-  default: git, hg
-  description: 'TODO: write it'
-  env: [{name: ANSIBLE_GALAXY_SCMS}]
-  ini:
-  - {key: scms, section: galaxy}
-  type: list
-  yaml: {key: galaxy.scms}
-  type: boolean
-  type: integer
-  yaml: {key: defaults.max_diff_size}
-  description: 'TODO: write it'
-  type: boolean
-  yaml: {key: defaults.merge_multiple_cli_tags}
-ONLY_NAMESPACE_FACTS:
-  default: False
-  description:
-    - Facts normally get injected as top level variables, this setting prevents that.
-    - Facts are still available in the `ansible_facts` variable w/o the `ansible_` prefix.
-  env: [{name: ANSIBLE_RESTRICT_FACTS}]
-  ini:
-  - {key: restrict_facts_namespace, section: defaults}
-  type: boolean
-  yaml: {key: defaults.restrict_facts_namespace}
-  version_added: "2.4"
-  yaml: {key: paramiko_connection.host_key_auto_add}
-  yaml: {key: paramiko_connection.look_for_keys}
-  yaml: {key: paramiko_connection.proxy_command}
-  yaml: {key: paramiko_connection.pty}
-  yaml: {key: paramiko_connection.record_host_keys}
-  description: 'TODO: write it'
-  yaml: {key: persistent_connection.control_path_dir}
-  yaml: {key: persistent_connection.connect_timeout}
-  yaml: {key: persistent_connection.connect_retry_timeout}
-  type: integer
-  yaml: {key: persistent_connection.command_timeout}
-  type: boolean
-  yaml: {key: errors.retry.enabled}
-  yaml: {key: errors.retry.path}
-  type: boolean
-  yaml: {key: defaults.show_custom_stats}
-  yaml: {key: jinja2.dont_type_filters}
-  description: 'TODO: write it'
-  yaml: {key: defaults.system_warnings}
-  description: 'TODO: write it'
-  yaml: {key: defaults.use_persistent_connections}
-  default: [all_inventory, groups_inventory, all_plugins_inventory, all_plugins_play, groups_plugins_inventory, groups_plugins_play]
-  description: 'TODO: write it'
-  yaml: {key: defaults.precedence}
-  yaml: {key: defaults.yaml_valid_extensions}
BREAKS HERE
-- include_tasks: keystone_service_setup.yml
-    - keystone_service_setup | bool
-  run_once: yes
BREAKS HERE
-# Rules for auditd are enabled if 'yes', disabled if 'no'. See the
-# documentation for each STIG control before enabling or disabling any rules.
-security_rhel7_audit_account_access: yes                     # RHEL-07-030492
-security_rhel7_audit_passwd_command: yes                     # RHEL-07-030510
-security_rhel7_audit_unix_chkpwd: yes                        # RHEL-07-030511
-security_rhel7_audit_gpasswd: yes                            # RHEL-07-030512
-security_rhel7_audit_chage: yes                              # RHEL-07-030513
-security_rhel7_audit_userhelper: yes                         # RHEL-07-030514
-security_rhel7_audit_su: yes                                 # RHEL-07-030521
-security_rhel7_audit_sudo: yes                               # RHEL-07-030522
-security_rhel7_audit_sudo_config_changes: yes                # RHEL-07-030523
-security_rhel7_audit_newgrp: yes                             # RHEL-07-030524
-security_rhel7_audit_sudoedit: yes                           # RHEL-07-030526
-security_rhel7_audit_umount: yes                             # RHEL-07-030531
-security_rhel7_audit_ssh_keysign: yes                        # RHEL-07-030550
-security_rhel7_audit_crontab: yes                            # RHEL-07-030561
-security_rhel7_audit_pam_timestamp_check: yes                # RHEL-07-030630
-security_rhel7_audit_init_module: yes                        # RHEL-07-030670
-security_rhel7_audit_delete_module: yes                      # RHEL-07-030671
-security_rhel7_audit_insmod: yes                             # RHEL-07-030672
-security_rhel7_audit_rmmod: yes                              # RHEL-07-030673
-security_rhel7_audit_modprobe: yes                           # RHEL-07-030674
-security_rhel7_audit_account_actions: yes                    # RHEL-07-030710
BREAKS HERE
-    with_items: "{{ groups[mon_group_name] | default('mons') }}"
BREAKS HERE
-    - name: Prepare ceph disks
-      script: "{{ kolla_ansible_full_src_dir }}/tests/setup_ceph_disks.sh"
-      when: scenario == "ceph"
-      become: true
-
BREAKS HERE
-        ROLE_REPO_PATH="{{ lookup('env', 'ANSIBLE_ROLE_DIR') }}/{{ role['name'] | default(role['src'] | basename) }}"
-        dest: "{{ lookup('env', 'ANSIBLE_ROLE_DIR') }}"
-            dest: "{{ lookup('env', 'ANSIBLE_ROLE_DIR') }}/{{ item['name'] | default(item['src'] | basename) }}"
-            dest: "{{ lookup('env', 'ANSIBLE_ROLE_DIR') }}/{{ item['name'] | default(item['src'] | basename) }}"
-                  dest: '{{ lookup("env", "ANSIBLE_ROLE_DIR") }}/\1'
-            dest: "{{ lookup('env', 'ANSIBLE_ROLE_DIR') }}/{{ item | regex_replace('openstack/openstack-ansible-', '') }}"
-            dest: "{{ lookup('env', 'ANSIBLE_ROLE_DIR') }}/{{ item['name'] | default(item['src'] | basename) }}"
BREAKS HERE
-        curl -s {{ _lxc_hosts_container_image_url_base }} | grep -P -o '{{ ansible_architecture }}.*?lxc.*?Buildlp.*?xz' | head -n 1
BREAKS HERE
-            postgresql_admin_user: "{{ qe_quay_database_admin_user }}"
BREAKS HERE
-nova_rpc_backend: nova.openstack.common.rpc.impl_kombu
BREAKS HERE
-  - yum: package=koji-hub state=latest
-  - yum: package=koji-builder state=latest
BREAKS HERE
-    - splunk.role == "splunk_search_head_captain"
-  until: task_result.rc == 0 or task_result.rc == 22
-
-- name: Register with deployer
-  command: "{{ splunk.exec }} set deploy-poll 'https://{{ groups['splunk_deployer'][0] }}:{{ splunk.svc_port }}' -auth 'admin:{{ splunk.password }}'"
-  become: yes
-  become_user: "{{ splunk.user }}"
-  register: task_result
-  until: task_result.rc == 0
-  retries: "{{ retry_num }}"
-  delay: 3
-  notify:
-    - Restart the splunkd service
-  no_log: "{{ hide_password }}"
BREAKS HERE
-        ib_server=192.168.1.1
BREAKS HERE
-        _state: "{{ (httpdebug_bind|default('') == 'absent')|ternary('absent', 'started') }}"
-        _bind_dirty: "{{httpdebug_bind|default(_bind_default)}}"
-        _bind: "{{ _bind_dirty if ':' in _bind_dirty else _bind_default }}"
-      tags: ['fakeshop', 'platform', 'pservice', 'docker-container', 'full']
BREAKS HERE
-        delegate_to: localhost
BREAKS HERE
-        {{base_dir}}/khaleesi-settings/beakerCheckOut.sh --arch={{ provisioner.beaker_arch }} --family={{ provisioner.beaker_family }} --distro={{ provisioner.beaker_distro }} --variant={{ provisioner.beaker_variant }} --hostrequire=hostlabcontroller={{ provisioner.host_lab_controller }} --username={{ provisioner.beaker_user_name }} --password=$BEAKER_PASSWORD --task=/CoreOS/rhsm/Install/automatjon-keys --keyvalue=HVM=1  --ks_meta="ksdevice=link" --whiteboard={{ provisioner.whiteboard_message }} --job-group={{ provisioner.beaker_group }} --machine={{ lookup('env', 'BEAKER_MACHINE') }} --timeout=720 >/dev/null 2>&1
-        {{base_dir}}/khaleesi-settings/beakerCheckOut.sh --arch={{ provisioner.beaker_arch }} --family={{ provisioner.beaker_family }} --distro={{ provisioner.beaker_distro }} --variant={{ provisioner.beaker_variant }} --hostrequire=hostlabcontroller={{ provisioner.host_lab_controller }} --task=/CoreOS/rhsm/Install/automatjon-keys --keyvalue=HVM=1 --ks_meta=ksdevice=link --whiteboard={{ provisioner.whiteboard_message }} --job-group={{ provisioner.beaker_group }} --machine={{ lookup('env', 'BEAKER_MACHINE') }} --timeout=720;
BREAKS HERE
-        openshift_deployment_type: "origin"
-        openshift_deployment_type: "origin"
BREAKS HERE
-    #globals
-    galaxy_data: "/galaxy_data"
-    galaxy_database: "/galaxy_database"
-    galaxy_user: "galaxy"
-    galaxy_port: "8080"
-    ftp_port: "21"
-    galaxy_host: "0.0.0.0"
-    galaxy_admin: "artimed@gmail.com"
-    galaxy_dir: "/home/{{ galaxy_user }}/{{ galaxy_user }}"
-    galaxy_api: 379ab2a47d714a74b5cee4081703368e
-    galaxy_db: "postgresql:///{{ galaxy_user }}?host=/var/run/postgresql"
-    galaxy_config_file: "{{ galaxy_config_dir }}/galaxy.ini"
-    - role: postgresql.movedb
-      galaxy_user_name: "{{ galaxy_user }}"      
-      supervisor_slurm_config_dir: "{{ galaxy_data }}" # Do not work?
BREAKS HERE
-    - "repo_server"
BREAKS HERE
-    - name: secondary.fedoraproject.org
BREAKS HERE
-  - postconf -e 'smtpd_tls_cert_file = /etc/letsencrypt/live/{{ server_name }}/fullchain.pem'
-  - postconf -e 'smtpd_tls_key_file = /etc/letsencrypt/live/{{ server_name }}/privkey.pem'
-- name: cron.daily job
-  copy: src="cron.daily/libravatar" dest="/etc/cron.daily/libravatar" mode=755
BREAKS HERE
-          with_items: "{{ async_ips }}"
BREAKS HERE
-                'mac': '`echo "$NODE_XML" | grep data -B 1 | grep mac | cut -d\' -f2`',
BREAKS HERE
-      # The $HOME directory is mocked to work with tox
-      #  by defining the 'ansible_env' hash. This should
-      #  NEVER be done outside of testing.
-      ansible_env:  ## NEVER DO THIS OUTSIDE OF TESTING
-        HOME: "/tmp"
-        path: /tmp/.ssh
BREAKS HERE
-  when: keystone_apache_mod_wsgi_enabled | bool
-  when: not keystone_apache_mod_wsgi_enabled | bool
-  when:
-    - not keystone_apache_mod_wsgi_enabled | bool
BREAKS HERE
-    - set_fact:
-      when: ceph_docker_image_tag | search("centos") or ceph_docker_image | search("rhceph") or ceph_docker_image_tag | search("fedora")
BREAKS HERE
-    shell: ceph-disk zap "{{ item }}"
BREAKS HERE
-
BREAKS HERE
-- name: run smoke tests when we can't run the full suite
-  include_tasks: smoke.yml
-- block:
-  - name: run non-destructive tests
-  
-  - name: run destructive tests if flag is set
-    include_tasks: destructive.yml
-    when: test_win_dsc_run_desctructive == True
-  - name: remove test feature
-    win_feature:
-      name: '{{test_win_dsc_feature_name}}'
-      state: absent
-  
-  - name: remove test folder
-    win_file:
-      path: '{{test_win_dsc_folder}}'
-  
-  - name: remove custom DSC resource folder
-    win_file:
-      path: C:\Program Files\WindowsPowerShell\Modules\xTestDsc
-      state: absent
-
-  when: powershell_version.stdout_lines[0]|int >= 5
BREAKS HERE
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      vars:
-        user_name: "{{ barbican_galera_user }}"
-        password: "{{ barbican_galera_password }}"
-        login_host: "{{ barbican_galera_address }}"
-        db_name: "{{ barbican_galera_database }}"
-      when: inventory_hostname == groups['barbican_all'][0]
-
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - barbican
BREAKS HERE
-      command: "cat /var/log/ironic/* | grep -v ERROR_FOR_DIVISION_BY_ZERO | grep ERROR"
BREAKS HERE
-- name: Prepare MQ services
-  hosts: glance_all
-  gather_facts: no
-  user: root
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - glance
-  tasks:
-    - name: Configure oslo messaging rpc vhost/user
-      include: common-tasks/oslomsg-rpc-vhost-user.yml
-      static: no
-      vars:
-        rpc_user: "{{ glance_oslomsg_rpc_userid }}"
-        rpc_password: "{{ glance_oslomsg_rpc_password }}"
-        rpc_vhost: "{{ glance_oslomsg_rpc_vhost }}"
-      when:
-        - groups[glance_oslomsg_rpc_host_group] | length > 0
-
-    - name: Configure oslo.messaging notify vhost/user
-      include: common-tasks/oslomsg-notify-vhost-user.yml
-      static: no
-      vars:
-        notify_user: "{{ glance_oslomsg_notify_userid }}"
-        notify_password: "{{ glance_oslomsg_notify_password }}"
-        notify_vhost: "{{ glance_oslomsg_notify_vhost }}"
-      when:
-        - glance_ceilometer_enabled | bool
-        - groups[glance_oslomsg_notify_host_group] | length > 0
-
-
-
BREAKS HERE
-        state: present
BREAKS HERE
-    template_options: "{{ lxc_cache_download_template_options }}"
-  register: cache_download
-  until: cache_download|success
BREAKS HERE
-    nsx_sgement_id_pool:
BREAKS HERE
-  connection: local
BREAKS HERE
-    when: env != 'staging'
-
-  - role: httpd/reverseproxy
-    website: apps.fedoraproject.org
-    destname: blockerbugs
-    remotepath: /blockerbugs
-    localpath: /blockerbugs
-    proxyurl: http://localhost:10022
-    when: env == 'staging'
BREAKS HERE
-- name: gather facts and check init system
-  - name: detect init system
-    command: ceph-detect-init
-    register: init_system
-    when: init_system.stdout == 'systemd'
-    when: init_system.stdout == 'sysvinit'
-    when: init_system.stdout == 'upstart'
-    when: init_system.stdout == 'systemd'
-    when: init_system.stdout == 'sysvinit'
-    when: init_system.stdout == 'upstart'
-    when: init_system.stdout == 'systemd'
-    when: init_system.stdout == 'upstart'
-    when: init_system.stdout == 'systemd'
-    when: init_system.stdout == 'sysvinit'
-    when: init_system.stdout == 'upstart'
-    when: init_system.stdout == 'systemd'
-    when: init_system.stdout == 'sysvinit'
-    when: init_system.stdout == 'upstart'
-    when: init_system.stdout == 'systemd'
-    when: init_system.stdout == 'sysvinit'
-    when: init_system.stdout == 'upstart'
-    when: init_system.stdout == 'sysvinit'
-    when: init_system.stdout == 'upstart'
BREAKS HERE
-      file: "{{ item.filename }}"
BREAKS HERE
-    cert_src: "{{private}}/files/docker-registry/{{env}}/pki/issued/containerstable.pem",
BREAKS HERE
-  gather_facts: false
BREAKS HERE
-    SSLCertificateChainFile: wildcard-2017.fedorahosted.org.intermediate.cert
BREAKS HERE
-  - name: Ensure the EPEL repository is present
-    yum: name=epel-release state=present
-    when: ansible_os_family == "RedHat"
-  - name: Ensure the EPEL repository GPG key is imported
-    rpm_key: key=/etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 state=present
-  - name: Ensure MongoDB Server is present
-    yum: name={{ item }} state=present
-      - mongodb
-      - mongodb-server
-    when: rocket_chat_include_mongodb and (ansible_os_family == "RedHat")
-    tags: mongodb
-  - name: "Configure /etc/hosts as detailed in Rocket.Chat's docs"
-  - name: Ensure MongoDB replication has been set correctly
-    lineinfile:
-      line: "replSet=001-rs"
-      dest: /etc/mongod.conf
-    when: rocket_chat_include_mongodb
-    tags: mongodb
-
-  - name: Ensure the MongoDB service is started/enabled
-    service: name=mongod state=started enabled=yes
-    when: rocket_chat_include_mongodb
-    tags: mongodb
-
-  - name: Ensure the MongoDB shell replSet script has been deployed
-    template:
-      src: mongo_rs_initiate.js.j2
-      dest: /var/lib/mongodb/mongo_rs_initiate.js
-    when: rocket_chat_include_mongodb
-    tags: mongodb
-
-  - name: Ensure the MongoDB replSets have been initiated
-    shell: >-
-      mongo mongo_rs_initiate.js &&
-      mongo --eval 'rs.initiate()' &&
-      touch .mongo_rs_initialised
-    args:
-      chdir: /var/lib/mongodb
-      creates: /var/lib/mongodb/.mongo_rs_initialised
-    when: rocket_chat_include_mongodb
-    tags: mongodb
-
BREAKS HERE
-  when:
-    - not swift_force_change_hashes | bool
-  when:
-    - swift_do_sync | bool
-  when:
-    - swift_do_sync | bool
-  when:
-    - swift_do_sync | bool
BREAKS HERE
-    - name: Create instance of Lab
-    - name: Create instance of DevTest Lab Policy
-    - name: Create instance of DevTest Lab Schedule
-    - name: Create instance of DevTest Labs virtual network
-        description: My DevTest Lab
-    - name: Create instance of DevTest Labs artifacts source
-    - name: Create instance of DTL Virtual Machine
-    - name: List all artifact sources
-    - name: List ARM Template facts
-    - name: Get ARM Template facts
-    - name: Create instance of DevTest Lab Environment
-    - name: Create instance of DevTest Lab Image
-    - name: Delete instance of Lab
BREAKS HERE
-- hosts: kube-node
BREAKS HERE
-        - name: designate.yml.aio
BREAKS HERE
-
-- name: show freshmaker app config on stage (for temp troubleshoot)
-  hosts: freshmaker-stg
-
-  tasks:
-  - name: Slurp hosts file
-    slurp:
-      src: /etc/freshmaker/config.py
-    register: slurpfile
-
-  - debug: msg="{{ slurpfile['content'] | b64decode }}"
BREAKS HERE
-    - passwords.yml
BREAKS HERE
-    - defaults/repo_packages/openstack_other.yml
BREAKS HERE
-      static: no
-      when: (gnocchi_storage_driver == "file") or (gnocchi_storage_driver is not defined)
BREAKS HERE
-    - include: common-tasks/set-upper-constraints.yml
-    - include: common-tasks/set-pip-upstream-url.yml
BREAKS HERE
-    - name: Install ansible
-      yum:
-       name: ansible
-       state: latest
BREAKS HERE
-#      include_tasks: tasks/create-and-remove-interface.yml type=tap interface=tap1298
BREAKS HERE
-- name: Check init system
-  command: cat /proc/1/comm
-  changed_when: false
-  register: _pid1_name
-  tags:
-    - always
-
-- name: Set the name of pid1
-  set_fact:
-    pid1_name: "{{ _pid1_name.stdout }}"
-  tags:
-    - always
-
BREAKS HERE
-- include_tasks: mq_setup.yml
-  with_items:
-    - oslomsg_setup_host: "{{ swift_oslomsg_notify_setup_host }}"
-      oslomsg_userid: "{{ swift_oslomsg_notify_userid }}"
-      oslomsg_password: "{{ swift_oslomsg_notify_password }}"
-      oslomsg_vhost: "{{ swift_oslomsg_notify_vhost }}"
-      oslomsg_transport: "{{ swift_oslomsg_notify_transport }}"
-  no_log: true
BREAKS HERE
-    proxyurl: "{{ varnish_url }}"
BREAKS HERE
-    - name: VM Servers group
-            port: 22
-            port: 22
BREAKS HERE
-        - rabbit-ssh-wait
BREAKS HERE
-# create a new taskotron dev server
-  - debug:
-      msg: "|||FEDORA|||| {{ ansible_distribution_major_version }}"
BREAKS HERE
-          value=15b3:1004,8086:10c9,8086:10ca,8086:1520
BREAKS HERE
-          line: "127.0.0.1   {{ inventory_hostname }}.redhat.local {{ inventory_hostname }}"
BREAKS HERE
-      configure_priv: "((a|z)mq\\.topic|amqp_to_zmq|amqp_bridge_verify_missing)"
BREAKS HERE
-# vim: set ft=ansible:
-- name: Step 003 - Prepare Ansible Host Files
-  hosts: localhost
-  connection: local
-  become: False
-
-
-
-  tags:
-    - generate_ansible_hosts_file
-  tasks:
-    - name: generate ansible hosts file
-      template:
-        src: "../configs/{{ env_type }}/files/hosts_template.j2"
-        dest: "{{output_dir}}/hosts-{{ env_type }}-{{ guid }}"
-    - name: generate ansible tower hosts file
-      template:
-        src: "../configs/{{ env_type }}/files/tower_hosts_template.j2"
-        dest: "{{output_dir}}/tower_hosts-{{ env_type }}-{{ guid }}"
-
-- name: Prepare bastion for Tower Deployment
-  hosts: bastions
-  gather_facts: False
-
-
-
-  vars:
-    tower_inventory_path: /root/tower_hosts
-
-  tags:
-    - install_tower
-    - name: Ensure directory /opt/tower
-      file:
-        path: /opt/tower
-        state: directory
-
-    - name: Copy over ansible hosts file
-        src: "{{output_dir}}/hosts-{{ env_type }}-{{ guid }}"
-        dest: /etc/ansible/hosts
-    - name: Copy over ansible tower hosts file
-      copy:
-        src: "{{output_dir}}/tower_hosts-{{ env_type }}-{{ guid }}"
-        dest: "{{tower_inventory_path}}"
-    - name: unarchive the latest tower software
-        src: "https://releases.ansible.com/ansible-tower/setup/ansible-tower-setup-{{ tower_setup_version | default('latest') }}.tar.gz"
-        dest: /opt/tower
-
-    - name: list current repository files
-      shell: "ls /opt/tower/ansible-tower-setup-*/setup.sh"
-      register: tower_installer_path
-
-    - name: print out tower Installer
-      debug:
-        var: tower_installer_path
-
-    - name: Add log path to Ansible configuration
-      lineinfile:
-        regexp: "^#log_path"
-        dest: "/etc/ansible/ansible.cfg"
-        line: "log_path = /root/ansible.log"
-        state: present
-        - ansible_log_enable
-    - name: print out tower Installer
-      debug:
-        msg: "{{tower_installer_path.stdout}} -i {{tower_inventory_path}}"
-    - name: run the tower installer
-      shell: "{{tower_installer_path.stdout}} -i {{tower_inventory_path}}"
-
-
-
-- name: Prepare bastion for Tower Deployment
-  hosts: towers[0]
-  become: true
-  gather_facts: False
-
-
-
-  vars:
-    tower_inventory_path: /root/tower_hosts
-
-  tags:
-    - install_tower_post
-  tasks:
-    - name: install pip via yum
-      yum:
-       name: python-pip
-    - name: Install Tower cli
-       name: 'ansible-tower-cli'
-    #   version: '3.1.8'
-
-    - name: Configure the tower cli file
-      blockinfile:
-        path: "~/.tower_cli.cfg"
-        create: true
-        block: |
-          [general]
-          host = tower1.{{guid}}.example.opentlc.com
-          username = admin
-          password = {{tower_admin_password}}
-        dest: "~/.tower_cli.cfg"
-        mode: 0640
-    - name: Download Licence file
-      get_url:
-        url: "{{license_file_url}}"
-        dest: /root/license.txt
-    - name: Append file
-      lineinfile:
-        path: /root/license.txt
-        insertbefore: '"company_name.*"'
-        line: '"eula_accepted" : "true",'
-      command: |
-        tower-cli setting modify LICENSE @/root/license.txt {{ tower_cli_insecure_flag | d() }}
BREAKS HERE
-# TODO: Remove hotfix
-# https://bugzilla.redhat.com/show_bug.cgi?id=1288076#c4
-#- name: digicert cert hotfix
-#  copy: src=digicert-intermediate.pem dest=/etc/pki/ca-trust/source/anchors/digicert-intermediate.pem
-#  tags: digicert_hotfix
-
-- name: Remove digicert cert hotfix
-  file: path=/etc/pki/ca-trust/source/anchors/digicert-intermediate.pem state=absent
-  tags: digicert_hotfix
-
-- name: update ca trust
-  shell: /usr/bin/update-ca-trust
-  tags: digicert_hotfix
-
BREAKS HERE
-  copy: src="/srv/data/run-in-venv.sh" dest="/usr/local/bin/run-in-venv.sh" mode=755
BREAKS HERE
-            ironic node-set-provision-state {{ item }} provide
-        with_items: failed_introspection_nodes.stdout_lines
-        when: failed_introspection_nodes.stdout_lines.0 is defined  and hw_env.env_type != 'ovb_host_cloud'
BREAKS HERE
-# This playbook deploys an application
-  - name: deploy new project
-    debug: msg="Deploy customer {{ customer }} on {{ env_type }}"
BREAKS HERE
-    - fail: msg="Installed ansible version {{ ansible_version.full }}, but ansible version >= 2.4 required"
-      when: ansible_version.full | version_compare('2.4.0.0', '<')
BREAKS HERE
-              - pytest
BREAKS HERE
-  tasks: []
BREAKS HERE
-# - name: Setup Host routes for ansible control node and host1 when in networking mode
-#   hosts: "managed_nodes:control_nodes"
-#   become: yes
-#   gather_facts: no
-#   roles:
-#     - { role: network_hostroutes, when: networking is defined and networking }
BREAKS HERE
-        shell: rhos-release -p {{ distro.version.major }} {{ distro.name }}-{{ distro.latest_version }}
BREAKS HERE
-  when: "'python34-fedmsg' in group_names"
BREAKS HERE
-        systemctl list-units | grep "loaded active" | grep -Eo 'ceph-osd@[0-9]{1,2}.service|ceph-disk@dev-[a-z]{3,4}[0-9]{1}.service'
BREAKS HERE
-    # - import_role:
-    #     name: alco-openvpn-server
-    #   vars:
-    #     ovpn_keys_dir: '{{covpn_keys_dir}}'
-    #     ovpn_docker_net_name: '{{docker_net_name}}'
-    #     ovpn_docker_network: '{{docker_network}}'
-    #     ovpn_docker_netmask: '{{docker_netmask}}'
-    #     ovpn_port: '{{ports.openvpn.0}}'
-    #   tags: ['ovpn']
BREAKS HERE
-      path: "/var/lib/libvirt/images/{{item.name}}.qcow2"
BREAKS HERE
-- include: /srv/web/infra/ansible/playbooks/include/proxies-reverseproxy.yml
BREAKS HERE
-      /etc/neutron/* r,
-      /var/log/neutron/* rw,
BREAKS HERE
-- import_tasks: mq_setup.yml
-  when:
-    - "neutron_services['neutron-server']['group'] in group_names"
-    - "inventory_hostname == ((groups[neutron_services['neutron-server']['group']]| intersect(ansible_play_hosts)) | list)[0]"
-  vars:
-    _oslomsg_rpc_setup_host: "{{ neutron_oslomsg_rpc_setup_host }}"
-    _oslomsg_rpc_userid: "{{ neutron_oslomsg_rpc_userid }}"
-    _oslomsg_rpc_password: "{{ neutron_oslomsg_rpc_password }}"
-    _oslomsg_rpc_vhost: "{{ neutron_oslomsg_rpc_vhost }}"
-    _oslomsg_rpc_transport: "{{ neutron_oslomsg_rpc_transport }}"
-    _oslomsg_notify_setup_host: "{{ neutron_oslomsg_notify_setup_host }}"
-    _oslomsg_notify_userid: "{{ neutron_oslomsg_notify_userid }}"
-    _oslomsg_notify_password: "{{ neutron_oslomsg_notify_password }}"
-    _oslomsg_notify_vhost: "{{ neutron_oslomsg_notify_vhost }}"
-    _oslomsg_notify_transport: "{{ neutron_oslomsg_notify_transport }}"
-  tags:
-    - common-mq
-    - neutron-config
-
BREAKS HERE
-            auth_file_path: "{{ overcloudrc_file }}"
BREAKS HERE
-# Set the volume size in gigabytes for the machine image caches.
-lxc_host_machine_volume_size: "64G"
BREAKS HERE
-    - role: "{{ role_name | default('../../RocketChat.Server') }}"
BREAKS HERE
----
-- name: Group hosts by provisioner
-  hosts: localhost
-  sudo: no
-  tasks:
-    - group_by: key={{ provisioner.type }}
-
-- name: provisioner skip the provisioning step
-  hosts: localhost
-  sudo: no
-  tasks:
-    - group_by: key={{ provisioner.skip }}
-
-- name: check if beakerCheckOut.sh script exists
-  hosts: localhost:!skip_provision
-  tasks:
-    - stat: path="{{ jenkins.workspace }}/{{ jenkins.job_name }}/beakerCheckOut.sh"
-      register: beakerCheckOut_present
-
-- name: remove beakerCheckOut.sh script if it exists
-  hosts: localhost:!skip_provision
-  tasks:
-    - command: rm -rf {{ jenkins.workspace }}/{{ jenkins.job_name }}/beakerCheckOut.sh
-      when: beakerCheckOut_present.stat.exists == True
-
-- name: get beakerCheckOut.sh file from khaleesi-settings
-  hosts: localhost:!skip_provision
-  tasks:
-    shell: >
-      chdir={{ jenkins.workspace }}/{{ jenkins.job_name }}
-      cp {{ jenkins.workspace }}/{{ jenkins.job_name }}/{{ provisioner.beaker_checkout_script }} .
-
-- name: check if beakerCheckOut.sh script exists
-  hosts: localhost:!skip_provision
-  tasks:
-    - stat: path="{{ jenkins.workspace }}/{{ jenkins.job_name }}/beakerCheckOut.sh"
-      register: beakerCheckOut_downloaded
-
-- name: Fail when can't get checkout script
-  hosts: localhost:!skip_provision
-  tasks:
-    - fail: msg="Cannot get {{ provisioner.beaker_checkout_script }}. Exiting with failure."
-      when: beakerCheckOut_downloaded.stat.exists == False
-
-- name: chmod if file exists
-  hosts: localhost:!skip_provision
-  tasks:
-    - shell: >
-        chdir={{ jenkins.workspace }}/{{ jenkins.job_name }}
-        chmod +x beakerCheckOut.sh
-      when: beakerCheckOut_downloaded.stat.exists == True
-
-- name: return the Beaker machine if it is checked out
-  hosts: localhost:!skip_provision
-  tasks:
-    - shell: ssh -o 'StrictHostKeyChecking=no' root@{{ job.host }} "return2beaker.sh"
-      ignore_errors: yes
-      async: 60
-
-- name: provision Beaker machine with password
-  hosts: localhost:!skip_provision
-  tasks:
-    - shell: >
-        chdir={{ jenkins.workspace }}/{{ jenkins.job_name }}
-        {{ jenkins.workspace }}/{{ jenkins.job_name }}/beakerCheckOut.sh --arch={{ provisioner.beaker_arch }} --family={{ provisioner.beaker_family }} --distro={{ provisioner.beaker_distro }} --variant={{ provisioner.beaker_variant }} --hostrequire=hostlabcontroller={{ provisioner.host_lab_controller }} --username={{ provisioner.beaker_user_name }} --password=$BEAKER_PASSWORD --task=/CoreOS/rhsm/Install/automatjon-keys --keyvalue=HVM=1  --ks_meta="ksdevice=link" --whiteboard={{ provisioner.whiteboard_message }} --job-group={{ provisioner.beaker_group }} --machine={{ job.host }} --timeout=720 >/dev/null 2>&1
-      register: beaker_job_status
-      async: 7200
-      poll: 180
-      no_log: true
-      when: provisioner.beaker_password is defined
-
-- name: provision Beaker machine with kerberos auth
-  hosts: localhost:!skip_provision
-  tasks:
-    - shell: >
-        chdir={{ jenkins.workspace }}/{{ jenkins.job_name }}
-        {{ jenkins.workspace }}/{{ jenkins.job_name }}/beakerCheckOut.sh --arch={{ provisioner.beaker_arch }} --family={{ provisioner.beaker_family }} --distro={{ provisioner.beaker_distro }} --variant={{ provisioner.beaker_variant }} --hostrequire=hostlabcontroller={{ provisioner.host_lab_controller }} --task=/CoreOS/rhsm/Install/automatjon-keys --keyvalue=HVM=1 --ks_meta=ksdevice=link --whiteboard={{ provisioner.whiteboard_message }} --job-group={{ provisioner.beaker_group }} --machine={{ job.host }} --timeout=720;
-      register: beaker_job_status
-      async: 7200
-      poll: 180
-      when: provisioner.beaker_password is not defined
BREAKS HERE
-    - name: collect running osds
-        systemctl list-units | grep "loaded active" | grep -Eo 'ceph-osd@[0-9]{1,2}.service'
-    - name: stop non-containerized ceph osd(s)
BREAKS HERE
-      - name: Port forwarding for 8443
-      - name: Port forwarding for 8443
-      - name: Port forwarding for 8443
BREAKS HERE
-          shell ip n | grep "`virsh dumpxml master01 | grep "mac address" | sed "s/.*'\(.*\)'.*/\1/g"`" | awk '{ gsub(/[\(\)]/,"",$1); print $1" master01" }'
-          shell ip n | grep "`virsh dumpxml infranode01 | grep "mac address" | sed "s/.*'\(.*\)'.*/\1/g"`" | awk '{ gsub(/[\(\)]/,"",$1); print $1" infranode01" }'
BREAKS HERE
-          when_failed: $result
-          when_failed: $result
-          when_failed: $result
BREAKS HERE
-    - ovirt-infra
BREAKS HERE
-- include: ovs_install.yml
-  when:
-    - neutron_services['neutron-openvswitch-agent']['group'] in group_names
-    - inventory_hostname in groups[neutron_services['neutron-openvswitch-agent']['group']]
-    - neutron_services['neutron-openvswitch-agent'].service_en | bool
-
BREAKS HERE
-  when:
-    - ansible_pkg_mgr == 'apt'
BREAKS HERE
-      command: lsblk --nodeps -no pkname "{{ item.stdout }}"
-      with_items:
-        --name ceph-osd-zap-{{ ansible_hostname }}-{{ item.stdout }} \
-        -e OSD_DEVICE=/dev/{{ item.stdout }} \
-      with_items:
BREAKS HERE
-    creates: /tmp/random_exporter
-      src: random
BREAKS HERE
-nova_service_publicurl: "{{ nova_service_publicuri }}/v2.1/%(tenant_id)s"
-nova_service_adminurl: "{{ nova_service_adminuri }}/v2.1/%(tenant_id)s"
-nova_service_internalurl: "{{ nova_service_internaluri }}/v2.1/%(tenant_id)s"
BREAKS HERE
-      - name: Update hosts user
-        add_host:
-            name: "{{ item }}"
-            ansible_ssh_user: "{{ provision.topology.username }}"
-        when: hostvars[item].create_user|changed
-        with_items: "{{ ansible_play_hosts }}"
-
BREAKS HERE
-      until: _install_host_packages | success
BREAKS HERE
-
BREAKS HERE
-        value: {'inputs': [], 'outputs': [], 'topology_ouputs': [], 'start': '', 'end': '', 'rc': 0, 'action': '', 'inventory_path': ''}
BREAKS HERE
-    lineinfile:
-      path: /etc/fstab
-      line: "/dev/vdb   /exports  ext4    defaults    0 0"
-  - name: Refresh mounts
-    command: mount -a
-
-
BREAKS HERE
-   - /srv/web/infra/ansible/vars/{{ ansible_distribution }}.yml
-   - /srv/web/infra/ansible/vars/{{ ansible_distribution }}.yml
BREAKS HERE
-    - "inventory_hostname == ((groups['keystone_all'] | intersect(ansible_play_hosts)) | list)[-1]"
BREAKS HERE
-- name: set correct fcontext for logstash launcher, note: there is equivalency between /usr/lib and /usr/lib64
BREAKS HERE
-# things like ansible_user_id and friends.
BREAKS HERE
-    register: result
-    failed_when:
-      - result | failed
-      - result.rc != 257
BREAKS HERE
-
BREAKS HERE
-      sudo: yes
BREAKS HERE
-    - name: insert user-vols to exports file
-      shell: "echo '/srv/nfs/user-vols *(rw,root_squash)' >> /etc/exports.d/user-vols.exports"
-    - name: restart nfs Server
-      shell: "systemctl restart nfs"
BREAKS HERE
-  - { role: ansible-role-adauth, tags: [ 'auth' ] }
-  - { role: ansible-role-pam, tags: [ 'auth' ] }
BREAKS HERE
-  when: nomad_enable_docker == "true"
BREAKS HERE
-            - install.version|openstack_release == 13
BREAKS HERE
-- name: Initialise the master user password parameters
-  tags: impersonate
-  set_fact:
-    master_password_params: "{{ playbook_dir }}/../../backup/{{ network.domain }}/ldap/master.pwd {{ policies.system.password }}"
-
-- name: Create the master user account password
-  when: mail.impersonate.active
-  tags: impersonate
-  set_fact:
-    master_password: '{{ lookup("password", master_password_params) }}'
-
-- name: Store the master user account password
-  when: mail.impersonate.active
-  tags: impersonate
-  htpasswd:
-    path: /etc/dovecot/master-users
-    name: '{{ mail.impersonate.master }}'
-    password: '{{ master_password }}'
-    owner: dovecot
-    group: dovecot
-    mode: '0400'
-    crypt_scheme: ldap_sha1
-
BREAKS HERE
-- hosts: all
-    node_version: 6.2.2
BREAKS HERE
-  - name: yum update bugyou packages from main repo
-    yum: name="bugyou*" state=latest
BREAKS HERE
-        --quiet
BREAKS HERE
-- name: CREATE FORTIMANAGER ADDRESSES FOR UNIT TESTES
BREAKS HERE
-      until: ssh_wait_check|success
BREAKS HERE
-nexus_backup_dir: '/tmp/nexus-backup'
BREAKS HERE
-    path: /etc/audit/auditd.conf
-    regexp: "space_left =.*"
-    line: "space_left = {{ rhel7stig_auditd_space_left }}"
BREAKS HERE
-    - name: restart VMs
-      command: "virsh reboot {{ item.value.name }}"
-      with_dict: "{{ vms_info }}"
-
BREAKS HERE
-- hosts: all
-        basename $(grep --exclude '*.bak' -R fsid /etc/ceph/ | egrep -o '^[^.]*')
-        src: "roles/ceph-common/templates/ceph.conf.j2"
BREAKS HERE
-        # FIXME(psedlak): this task likey does not have access to randkey.stdout variable
-        # NOTE(psedlak): possibly for even younger than 13, but afaik no issue there
-      - name: Setup fixed key class for encrypted volumes for cinder
BREAKS HERE
-            base_role_name: Controller
-            base_role_name: Novacontrol
-            base_role_name: Compute
-            base_role_name: CephStorage
-            base_role_name: ObjectStorage
-          - base_role_name: Database
-            base_role_name: Messaging
-            base_role_name: Networker
BREAKS HERE
-security_reset_perm_ownership: yes                           # RHEL-07-010010
BREAKS HERE
-    checksum: "sha1:{{ lookup('url', neutron_venv_download_url | replace('tgz', 'checksum')) }}"
BREAKS HERE
-- include_playbook: /srv/web/infra/ansible/playbooks/include/proxies-certificates.yml
-- include_playbook: /srv/web/infra/ansible/playbooks/include/proxies-websites.yml
-- include_playbook: /srv/web/infra/ansible/playbooks/include/proxies-fedorahosted.yml
-- include_playbook: /srv/web/infra/ansible/playbooks/include/proxies-fedora-web.yml
-- include_playbook: /srv/web/infra/ansible/playbooks/include/proxies-reverseproxy.yml
-- include_playbook: /srv/web/infra/ansible/playbooks/include/proxies-rewrites.yml
-- include_playbook: /srv/web/infra/ansible/playbooks/include/proxies-redirects.yml
-- include_playbook: /srv/web/infra/ansible/playbooks/include/proxies-haproxy.yml
-- include_playbook: /srv/web/infra/ansible/playbooks/include/proxies-miscellaneous.yml
BREAKS HERE
-    # deploy the same stack on other regions
-    - loop: "{{ target_regions[1:] }}"
-      include_tasks: deploy_stack.yml
-        target_region: "{{ _region.region }}"
-        target_host_suffix: "{{ _region.host_suffix }}"
BREAKS HERE
-# Detect whether the init system is upstart of systemd.
-- name: Check init system
-  command: cat /proc/1/comm
-  changed_when: false
-  register: _pid1_name
-  tags:
-    - always
-
-- name: Set the name of pid1
-  set_fact:
-    pid1_name: "{{ _pid1_name.stdout }}"
-  tags:
-    - always
-
BREAKS HERE
-    horizon_self_signed == true or
-    horizon_self_signed == "True"
-  when: >
-    horizon_self_signed == false or
-    horizon_self_signed == "False"
BREAKS HERE
-      when: createuser_results|failed and not createuser_results.stderr|search("role .* already exists")
-      when: createdb_results|failed and not createdb_results.stderr|search("database .* already exists")
-      when: createschema_results|failed and not createschema_results.stderr|search("schema .* already exists")
-      when: hiveserver_hosts is defined and hiveserver_hosts|length > 0 and createuser_results|failed and not createuser_results.stderr|search("role .* already exists")
-      when: hiveserver_hosts is defined and hiveserver_hosts|length > 0 and createdb_results|failed and not createdb_results.stderr|search("database .* already exists")
-      when: oozie_hosts is defined and oozie_hosts|length > 0 and createuser_results|failed and not createuser_results.stderr|search("role .* already exists")
-      when: oozie_hosts is defined and oozie_hosts|length > 0 and createdb_results|failed and not createdb_results.stderr|search("database .* already exists")
-      when: rangeradmin_hosts is defined and rangeradmin_hosts|length > 0 and createuser_results|failed and not createuser_results.stderr|search("role .* already exists")
-      when: rangeradmin_hosts is defined and rangeradmin_hosts|length > 0 and createdb_results|failed and not createdb_results.stderr|search("database .* already exists")
-      when: registry_hosts is defined and registry_hosts|length > 0 and createuser_results|failed and not createuser_results.stderr|search("role .* already exists")
-      when: registry_hosts is defined and registry_hosts|length > 0 and createdb_results|failed and not createdb_results.stderr|search("database .* already exists")
-      when: streamline_hosts is defined and streamline_hosts|length > 0 and createuser_results|failed and not createuser_results.stderr|search("role .* already exists")
-      when: streamline_hosts is defined and streamline_hosts|length > 0 and createdb_results|failed and not createdb_results.stderr|search("database .* already exists")
BREAKS HERE
-  hosts: gnocchi_all
-  user: root
-      gnocchi_api: "http://localhost:{{ gnocchi_service_port }}"
-    - name: Install openstackclient
-      pip:
-        name: "python-openstackclient"
-        extra_args: >-
-          {{ gnocchi_developer_mode | ternary(pip_install_developer_constraints | default('--constraint /opt/developer-pip-constraints.txt'), '') }}
-          {{ pip_install_options | default('') }}
-
-        url: "{{ gnocchi_api }}"
-        url: "{{ gnocchi_api }}/v1/status"
-    - name: Get auth token
-      shell: >
-        . /root/openrc && openstack token issue --format yaml | awk '/^id\:/ {print $2}'
-      register: get_keystone_token
-      changed_when: false
-
-    - name: set token
-      set_fact:
-        keystone_token: "{{ get_keystone_token.stdout }}"
-        url: "{{ gnocchi_api }}/v1/metric"
-          X-Auth-Token: "{{ keystone_token }}"
-        url: "{{ gnocchi_api }}/v1/metric/{{ metric_create.json.id }}/measures"
-          X-Auth-Token: "{{ keystone_token }}"
-        url: "{{ gnocchi_api }}/v1/metric/{{ metric_create.json.id }}/measures?refresh=true"
-          X-Auth-Token: "{{ keystone_token }}"
-        url: "{{ gnocchi_api }}/v1/archive_policy"
-          X-Auth-Token: "{{ keystone_token }}"
BREAKS HERE
-- include_tasks: tasks/recover/add_nfs_domain.yml
-  with_items:
-    - "{{ storage }}"
-  when: "storage.dr_domain_type == 'nfs'"
-  loop_control:
-      loop_var: nfs_storage
-- include_tasks: tasks/recover/add_glusterfs_domain.yml
-  with_items:
-    - "{{ storage }}"
-  when: "storage.dr_domain_type == 'gluster'"
-  loop_control:
-      loop_var: gluster_storage
-- include_tasks: tasks/recover/add_posixfs_domain.yml
-  with_items:
-    - "{{ storage }}"
-  when: "storage.dr_domain_type == 'posixfs'"
-  loop_control:
-      loop_var: posix_storage
-- include_tasks: tasks/recover/add_iscsi_domain.yml
-  with_items:
-    - "{{ storage }}"
-  when: "storage.dr_domain_type == 'iscsi'"
-  loop_control:
-      loop_var: iscsi_storage
-
-- include_tasks: tasks/recover/add_fcp_domain.yml
-  with_items:
-    - "{{ storage }}"
-  when: "storage.dr_domain_type == 'fcp'"
-  loop_control:
-      loop_var: fcp_storage
BREAKS HERE
-#    - ui-items
BREAKS HERE
-
-    queue_name: greenwave
BREAKS HERE
-  - set_fact: real_ips="{{ ips | regex_replace(':[0-9][0-9][0-9][0-9]\/[0-9][0-9][0-9][0-9]', '') }}"
BREAKS HERE
-      register: devices
-      with_items: "{{ devices.stdout_lines }}"
-        - "{{ devices.stdout_lines }}"
-    - set_fact: devices={{ devices.stdout_lines }}
-
BREAKS HERE
-        command: "mkdir -p {{ dirs }}"
-        command: "chown {{_uid}}:{{_gid}} {{ dirs }}"
-        dirs: "{{ create_dirs|flatten|join(' ') }}"
-      - name: Fix permissions
-
BREAKS HERE
-          - { src: '/etc/apt/apt.conf.d/99unauthenticated', dest: '/etc/apt/apt.conf.d/99unauthenticated' }
-  roles:
-    - role: "lxc_hosts"
BREAKS HERE
-  sudo: yes
-      sudo: yes
-      sudo: yes
BREAKS HERE
-  when: "httpd_install_new_version == true"
BREAKS HERE
-lxc_image_cache_server: https://images.linuxcontainers.org
-lxc_image_cache_server_path: "{{ lxc_image_cache_server }}/images/{{ lxc_cache_map.distro }}/{{ lxc_cache_map.release }}/{{ lxc_cache_map.arch }}/{{ lxc_cache_default_variant }}/"
BREAKS HERE
-      # The $HOME directory is mocked to work with tox
-      #  by defining the 'ansible_env' hash. This should
-      #  NEVER be done outside of testing.
-      ansible_env:  ## NEVER DO THIS OUTSIDE OF TESTING
-        HOME: "/tmp"
-        path: /tmp/.cache/pip
-        path: /tmp/.cache/pip/selfcheck.json
BREAKS HERE
-      changed_when: rhel_07_040180_audit.rc == 1
-- name: "MEDIUM | RHEL-07-041002 | PATCH | The operating system must implement multifactor authentication for access to privileged accounts via pluggable authentication modules (PAM)."
-      - name: "MEDIUM | RHEL-07-041002 | PATCH | Check if pam service is configured in sssd file"
-        shell: 'grep -E "^\s*services\s*=.*pam" /etc/sssd/sssd.conf'
-        changed_when: no
-        failed_when: no
BREAKS HERE
-        autostart: no
-    - name: List virt network(s)
-        command: list_nets
-      register: vm_networks
-        command: define
-      when: "item.value.iface not in vm_networks.list_nets"
-    - name: Create virt network(s)
-        command: create
-        autostart: true
-      when: "item.value.iface not in vm_networks.list_nets"
BREAKS HERE
-nova_s3_service_publicuri: "{{ nova_s3_service_proto }}://{{ external_lb_vip_address }}:{{ nova_s3_service_port }}"
-nova_s3_service_adminuri: "{{ nova_s3_service_proto }}://{{ internal_lb_vip_address }}:{{ nova_s3_service_port }}"
-nova_s3_service_internaluri: "{{ nova_s3_service_proto }}://{{ internal_lb_vip_address }}:{{ nova_s3_service_port }}"
-nova_v3_service_publicuri: "{{ nova_v3_service_proto }}://{{ external_lb_vip_address }}:{{ nova_v3_service_port }}"
-nova_v3_service_adminuri: "{{ nova_v3_service_proto }}://{{ internal_lb_vip_address }}:{{ nova_v3_service_port }}"
-nova_v3_service_internaluri: "{{ nova_v3_service_proto }}://{{ internal_lb_vip_address }}:{{ nova_v3_service_port }}"
-nova_v21_service_publicuri: "{{ nova_v21_service_proto }}://{{ external_lb_vip_address }}:{{ nova_v21_service_port }}"
-nova_v21_service_adminuri: "{{ nova_v21_service_proto }}://{{ internal_lb_vip_address }}:{{ nova_v21_service_port }}"
-nova_v21_service_internaluri: "{{ nova_v21_service_proto }}://{{ internal_lb_vip_address }}:{{ nova_v21_service_port }}"
-nova_service_publicuri: "{{ nova_service_proto }}://{{ external_lb_vip_address }}:{{ nova_service_port }}"
-nova_service_adminuri: "{{ nova_service_proto }}://{{ internal_lb_vip_address }}:{{ nova_service_port }}"
-nova_service_internaluri: "{{ nova_service_proto }}://{{ internal_lb_vip_address }}:{{ nova_service_port }}"
-nova_ec2_service_publicuri: "{{ nova_ec2_service_proto }}://{{ external_lb_vip_address }}:{{ nova_ec2_service_port }}"
-nova_ec2_service_adminuri: "{{ nova_ec2_service_proto }}://{{ internal_lb_vip_address }}:{{ nova_ec2_service_port }}"
-nova_ec2_service_internaluri: "{{ nova_ec2_service_proto }}://{{ internal_lb_vip_address }}:{{ nova_ec2_service_port }}"
BREAKS HERE
-      command: lvresize -l +100%FREE /dev/atomicos/root
-      when: is_atomic
-
-    - name: increasing the size of the fs for root partition
-      command: xfs_growfs /
BREAKS HERE
-        when: install.version|openstack_distribution == 'OSP'
BREAKS HERE
-      shortname: "{{item['tags']['Name'] | default('default_value')}}"
-      internaldns: "{{item['tags']['internaldns'] | default('default_value')}}"
BREAKS HERE
-    - swift-storage-hosts
BREAKS HERE
-  debug: msg="Will use ADVERTISE_IP with plex container http://{{public_ip}}:{{plex.port}}/"
-      ADVERTISE_IP: "http://{{public_ip}}:{{plex.port}}/"
-      ADVERTISE_IP: "http://{{public_ip}}:{{plex.port}}/"
BREAKS HERE
-  - name: Install docker-compose (also always the newest version)
BREAKS HERE
-    website: admin.fedoraproject.org
-    destname: mirrormanager
-    localpath: /mirrormanager
-    remotepath: /mirrormanager
-    proxyurl: http://localhost:10039
-    when: env == "staging"
-
-  - role: httpd/reverseproxy
BREAKS HERE
-      join_repospanner_node: repospanner01.ansible.fedoraproject.org
BREAKS HERE
-  vars_files:
-    - "login_creds.vault"
-          - "device_type is defined and device_type"
-      include_tasks: "tasks/{{ device_type }}.yml"
BREAKS HERE
-        state: present
BREAKS HERE
-    # Remove when 29147 lands on 2.4.1
-    - name: Check xenial image exists
-      os_image_facts:
-        cloud: ansible-cloud
-        image: ubuntu-xenial
-    - name: Create xenial image if needed
-      shell: OS_CLOUD=ansible-cloud openstack image create --file /home/centos/mnt/xenial-server-cloudimg-amd64-disk1.img --disk-format qcow2 ubuntu-xenial
-      when: not openstack_image
-    # Remove when 29147 lands on 2.4.1
-    - name: Check xenial image exists
-      os_image_facts:
-        cloud: calfonso-cloud
-        image: ubuntu-xenial
-    - name: Create xenial image if needed
-      shell: OS_CLOUD=calfonso-cloud openstack image create --file /home/centos/mnt/xenial-server-cloudimg-amd64-disk1.img --disk-format qcow2 ubuntu-xenial
-      when: not openstack_image
-    # Remove when 29147 lands on 2.4.1
-    - name: Check vyos image exists
-      os_image_facts:
-        cloud: ansible-cloud
-        image: vyos-1.1.7
-    - name: Create vyos image if needed
-      shell: OS_CLOUD=ansible-cloud openstack image create --file /home/centos/mnt/vyos-1.1.7.qcow2 --disk-format qcow2 vyos-1.1.7
-      when: not openstack_image
-    # Remove when 29147 lands on 2.4.1
-    - name: Check vyos image exists
-      os_image_facts:
-        cloud: calfonso-cloud
-        image: vyos-1.1.7
-    - name: Create vyos image if needed
-      shell: OS_CLOUD=calfonso-cloud openstack image create --file /home/centos/mnt/vyos-1.1.7.qcow2 --disk-format qcow2 vyos-1.1.7
-      when: not openstack_image
-    # Remove when 29147 lands on 2.4.1
-    - name: Check eos image exists
-      os_image_facts:
-        cloud: ansible-cloud
-        image: eos-4.18.1F
-    - name: Create eos image if needed
-      shell: OS_CLOUD=ansible-cloud openstack image create --file /home/centos/mnt/eos-4.18.1F.qcow2 --disk-format qcow2 --property hw_disk_bus=ide eos-4.18.1F
-      when: not openstack_image
-    - name: Check eos image exists
-      os_image_facts:
-        cloud: calfonso-cloud
-        image: eos-4.18.1F
-    - name: Create eos image if needed
-      shell: OS_CLOUD=calfonso-cloud openstack image create --file /home/centos/mnt/eos-4.18.1F.qcow2 --disk-format qcow2 --property hw_disk_bus=ide eos-4.18.1F
-      when: not openstack_image
-    # Remove when 29147 lands on 2.4.1
-    - name: Check nxos image exists
-      os_image_facts:
-        cloud: ansible-cloud
-        image: nxos-7.0.3.I5.1
-    - name: Create nxos image if needed
-      shell: OS_CLOUD=ansible-cloud openstack image create --file /home/centos/mnt/nxos-7.0.3.I5.1.qcow2 --disk-format qcow2 nxos-7.0.3.I5.1
-      when: not openstack_image
-    # Remove when 29147 lands on 2.4.1
-    - name: Check nxos image exists
-      os_image_facts:
-        cloud: calfonso-cloud
-        image: nxos-7.0.3.I5.1
-    - name: Create nxos image if needed
-      shell: OS_CLOUD=calfonso-cloud openstack image create --file /home/centos/mnt/nxos-7.0.3.I5.1.qcow2 --disk-format qcow2 nxos-7.0.3.I5.1
-      when: not openstack_image
-    # Remove when 29147 lands on 2.4.1
-    - name: Check junos vqfx image exists
-      os_image_facts:
-        cloud: ansible-cloud
-        image: junos-vqfx-15.1X53-D60.4
-    - name: Create junos vqfx image if needed
-      shell: OS_CLOUD=ansible-cloud openstack image create --file /home/centos/mnt/junos-vqfx-15.1X53-D60.4.qcow2 --disk-format qcow2 --property hw_disk_bus=ide junos-vqfx-15.1X53-D60.4
-      when: not openstack_image
-    # Remove when 29147 lands on 2.4.1
-    - name: Check junos vqfx image exists
-      os_image_facts:
-        cloud: calfonso-cloud
-        image: junos-vqfx-15.1X53-D60.4
-    - name: Create junos vqfx image if needed
-      shell: OS_CLOUD=calfonso-cloud openstack image create --file /home/centos/mnt/junos-vqfx-15.1X53-D60.4.qcow2 --disk-format qcow2 --property hw_disk_bus=ide junos-vqfx-15.1X53-D60.4
-      when: not openstack_image
-    # Remove when 29147 lands on 2.4.1
-    - name: Check ios image exists
-      os_image_facts:
-        cloud: ansible-cloud
-        image: ios-15.6.2.T
-    - name: Create ios image if needed
-      shell: OS_CLOUD=ansible-cloud openstack image create --file /home/centos/mnt/ios-15.6.2.T.qcow2 --disk-format qcow2 ios-15.6.2.T
-      when: not openstack_image
-    # Remove when 29147 lands on 2.4.1
-    - name: Check ios image exists
-      os_image_facts:
-        cloud: calfonso-cloud
-        image: ios-15.6.2.T
-    - name: Create ios image if needed
-      shell: OS_CLOUD=calfonso-cloud openstack image create --file /home/centos/mnt/ios-15.6.2.T.qcow2 --disk-format qcow2 ios-15.6.2.T
-      when: not openstack_image
-    # Remove when 29147 lands on 2.4.1
-    - name: Check ios-xe image exists
-      os_image_facts:
-        cloud: ansible-cloud
-        image: csr1000v-universalk9.16.03.05
-    - name: Create ios-xe image if needed
-      shell: OS_CLOUD=ansible-cloud openstack image create --file /home/centos/mnt/csr1000v-universalk9.16.03.05.qcow2 --disk-format qcow2 csr1000v-universalk9.16.03.05
-      when: not openstack_image
-    # Remove when 29147 lands on 2.4.1
-    - name: Check ios-xe image exists
-      os_image_facts:
-        cloud: calfonso-cloud
-        image: csr1000v-universalk9.16.03.05
-    - name: Create ios-xe image if needed
-      shell: OS_CLOUD=calfonso-cloud openstack image create --file /home/centos/mnt/csr1000v-universalk9.16.03.05.qcow2 --disk-format qcow2 csr1000v-universalk9.16.03.05
-      when: not openstack_image
-    # Remove when 29147 lands on 2.4.1
-    - name: Check ios-xr image exists
-      os_image_facts:
-        cloud: calfonso-cloud
-        image: iosxrv-k9-demo-6.1.2
-    - name: Create ios-xr image if needed
-      shell: OS_CLOUD=calfonso-cloud openstack image create --file /home/centos/mnt/iosxrv-k9-demo-6.1.2.qcow2 --disk-format qcow2 iosxrv-k9-demo-6.1.2
-      when: not openstack_image
BREAKS HERE
-    when_string: ${inventory_hostname} not in ${result.list_vms}
BREAKS HERE
-    when: env == "staging"
-    when: env == "staging"
-    when: env == "staging"
-    when: env == "staging"
BREAKS HERE
-  serial: 1
BREAKS HERE
-- name: Get undercloud version
-  hosts: undercloud
-  gather_facts: yes
-  any_errors_fatal: true
-  tags: images
-  tasks:
-    - name: get undercloud version
-      find:
-          use_regex: yes
-          patterns: 'rhos-release-[0-9]+.*'
-          paths:
-            - '/etc/yum.repos.d/'
-      register: result
-
-    - set_fact:
-          undercloud_version: "{{ result.files[0]['path'] | basename | regex_replace('^rhos-release-([0-9]+).*$', '\\1') }}"
-      images_dest: ~/base_image.qcow2
BREAKS HERE
-# test code for the win_package module
-# (c) 2014, Chris Church <chris@ninemoreminutes.com>
-# This file is part of Ansible
-#
-# Ansible is free software: you can redistribute it and/or modify
-# it under the terms of the GNU General Public License as published by
-# the Free Software Foundation, either version 3 of the License, or
-# (at your option) any later version.
-#
-# Ansible is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-# GNU General Public License for more details.
-#
-# You should have received a copy of the GNU General Public License
-# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
-
-- name: use win_get_url module to download msi
-    url: "{{msi_url}}"
-    dest: "{{msi_download_path}}"
-  register: win_get_url_result
-
-- name: make sure msi is uninstalled
-  win_package:
-    path: "{{msi_download_path}}"
-    product_id: "{{msi_product_code}}"
-    state: absent
-
-- name: install msi
-  win_package:
-    path: "{{msi_download_path}}"
-    product_id: "{{msi_product_code}}"
-    state: present
-  register: win_package_install_result
-
-- name: check win_package install result
-  assert:
-    that:
-      - "not win_package_install_result|failed"
-      - "win_package_install_result|changed"
-
-- name: install msi again (check for no change)
-    path: "{{msi_download_path}}"
-    product_id: "{{msi_product_code}}"
-    state: present
-  register: win_package_install_again_result
-
-- name: check win_package install again result
-  assert: 
-    that:
-      - "not win_package_install_again_result|failed"
-      - "not win_package_install_again_result|changed"
-
-- name: uninstall msi
-  win_package:
-    path: "{{msi_download_path}}"
-    product_id: "{{msi_product_code}}"
-    state: absent
-  register: win_package_uninstall_result
-
-- name: check win_package uninstall result
-  assert: 
-    that:
-      - "not win_package_uninstall_result|failed"
-      - "win_package_uninstall_result|changed"
-
-- name: uninstall msi again (check for no change)
-  win_package:
-    path: "{{msi_download_path}}"
-    product_id: "{{msi_product_code}}"
-  register: win_package_uninstall_again_result
-
-- name: check win_package uninstall result
-  assert: 
-    that:
-      - "not win_package_uninstall_result|failed"
-      - "not win_package_uninstall_again_result|changed"
BREAKS HERE
-## Common Override to enable Ceilometer for each service.
-## By default all service are *not* enabled.
-# swift_ceilometer_enabled: True
-# heat_ceilometer_enabled: True
-# cinder_ceilometer_enabled: True
-# glance_ceilometer_enabled: True
-# nova_ceilometer_enabled: True
-# neutron_ceilometer_enabled: True
-# keystone_ceilometer_enabled: True
-
BREAKS HERE
-    - name: ensure az is installed
-    - name: fail if az not available
-    # AZURE_CONFIG_DIR: create a specific config dir for this stack to allow concurrent access
-    - name: Get ssh pub key
-      set_fact:
-        ssh_key: "{{ANSIBLE_REPO_PATH}}/workdir/{{env_authorized_key}}"
-        ssh_key_data: "{{lookup('file', '{{ANSIBLE_REPO_PATH}}/workdir/{{env_authorized_key}}.pub')}}"
-      when: use_own_key|bool
-
-    - name: Get ssh pub key
-      when: not use_own_key|bool
-    - name: validate arm template
-        - validate_azure_template
-        - validate_azure_template
-      register: hosted_subzone_ns
-        - validate_azure_template
-        var: hosted_subzone_ns
-        - validate_azure_template
-        resource_group: dns
-        records: "{{hosted_subzone_ns | json_query('ansible_facts.azure_dnsrecordset[0].properties.NSRecords[*].{entry: nsdname}')}}"
-        - validate_azure_template
-    - name: get list of VMs
-    - set_fact:
-    - add_host:
BREAKS HERE
-            openstack flavor set --property 'cpu_arch'='x86_64' --property 'capabilities:boot_option'='local' --property 'capabilities:profile'='{{ item.cmd.split() | last }}' {{ item.cmd.split() | last }}
-            ironic node-update {{ item[0].stdout }} add properties/capabilities='profile:{{ item[1].cmd.split() | last }},boot_option:local'
-            openstack baremetal node set --property capabilities='profile:{{ item[1].cmd.split() | last }},boot_option:local' {{ item[0].stdout }}
BREAKS HERE
-  - name: Ensure influxdb bave a place to store data
-      state: present
BREAKS HERE
-- hosts: cluster_nodes
BREAKS HERE
-    - name: Use the unconfined aa profile
-      lxc_container:
-        name: "{{ container_name }}"
-        container_config:
-          - "lxc.aa_profile=unconfined"
-      delegate_to: "{{ physical_host }}"
-
-    - name: Set mount path for kernel modules (Ubuntu)
-      set_fact:
-        kernel_module_path: "lib/modules"
-      when: ansible_pkg_mgr == 'apt'
-
-    - name: Set mount path for kernel modules (CentOS)
-      set_fact:
-        kernel_module_path: "usr/lib/modules"
-      when: ansible_pkg_mgr == 'yum'
-          [[ ! -d "/{{ kernel_module_path }}" ]] && mkdir -p "/{{ kernel_module_path }}"
-          - "lxc.mount.entry=/{{ kernel_module_path }} {{ kernel_module_path }} none bind 0 0"
-    - name: Add iptables rule for communication w/ metadata agent
-      command: /sbin/iptables -t mangle -A POSTROUTING -p tcp --sport 80 -j CHECKSUM --checksum-fill
-- name: Deploy neutron
-  pre_tasks:
-    # NOTE: These are typically installed in the repo server where we build the
-    #       neutron wheel
-    - name: Install packages required to build neutron python package (Ubuntu)
-        name: "{{ item }}"
-      with_items:
-        - libffi-dev
-      when:
-        - inventory_hostname in groups['neutron_all']
-        - ansible_pkg_mgr == 'apt'
-    - name: Install packages required to build neutron python package (CentOS)
-        name: "{{ item }}"
-      with_items:
-        - libffi-devel
-      when:
-        - inventory_hostname in groups['neutron_all']
-        - ansible_pkg_mgr == 'yum'
-    - role: "{{ neutron_rolename | default('os_neutron') }}"
BREAKS HERE
-      docker_image_facts:
-      when: not item.pre_build_image | default(false)
BREAKS HERE
-    - name: Set fact for upper constraints dictionary
-        upper_constraints_dict: >
-          {%- set constraints = {} %}
-          {%- for package in slurp_upper_constraints.content | b64decode | splitlines %}
-          {%-    set name = package | regex_replace('===.*$','') %}
-          {%-    set version = package | regex_replace('^.*===','') %}
-          {%-    set _ = constraints.update({name: version}) %}
-          {%-    set _ = override_packages.append(name) %}
-    - name: Ensure that requirements constraints is applying upper constraints
-       that: requirements_constraints_content.find( "{{ item.key }}<={{ item.value }}" ) != -1
-      with_dict: upper_constraints_dict
-        - item.key not in upper_constraints_override_list
-        - requirements_constraints_content.find(item.key) != -1
-       that: requirements_constraints_content.find( "{{ item }}" ) != -1
BREAKS HERE
-      shell: >-
-        rm -rf /var/tmp/.guestfs
BREAKS HERE
-- name: Install libvirt packages and configure networks
-    - environment/setup
BREAKS HERE
-  virt_pool:
-    command: define
-    state: present
-    name: "{{ libvirt_volume_pool }}"
-    xml: '{{ lookup("template", "volume_pool.xml.j2") }}'
-    uri: "{{ libvirt_uri }}"
BREAKS HERE
-  vars:
-    local_home: "{{ lookup('env','HOME') }}"
-    - azure_infra
-  vars:
-    container_runtime_docker_storage_type: overlay2
-    container_runtime_docker_storage_setup_device: /dev/sdd
-  
-# HACK: fails to connect to grafana svc the first time so run it twice
-- name: install Grafana
-  import_playbook: /usr/share/ansible/openshift-ansible/playbooks/openshift-grafana/config.yml
-
-
-        dest: "{{ local_home }}/.kube/"
BREAKS HERE
-- name: Create route 'Test-Four'
-  panos_static_route:
-    ip_address: '{{ fw_ip_address }}'
-    username: '{{ fw_username }}'
-    password: '{{ fw_password }}'
-    name: 'Test-Four'
-    destination: '4.4.4.0/24'
-    nexthop: '10.0.0.1'
-    virtual_router: 'VR-Two'
BREAKS HERE
-# defaults file for natefoo.galaxy
BREAKS HERE
-      shell: "{{ postgres_initdb_command }}"
BREAKS HERE
-- name: Fix Elasticsearch for 3.10.14 on NFS
-  hosts: masters
-  run_once: True
-  gather_facts: False
-  become: yes
-  vars_files:
-    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
-  tasks:
-    - when:
-      - osrelease is version_compare("3.10.14", "==")
-      - install_nfs|bool
-      block:
-      - name: Get name of Elasticsearch Deployment Config
-        shell: "oc get dc -n openshift-logging -o jsonpath='{range .items[*].metadata}{.name}{\"\n\"}{end}'|grep -v curator|grep -v kibana"
-        register: result
-        ignore_errors: true
-      - name: Redeploy Elasticsearch Deployment Config
-        command: >
-          oc rollout latest {{ result.stdout }}
-        ignore_errors: true
-
BREAKS HERE
-        src: "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}y6t/files/userpvs.j2"
BREAKS HERE
-      copy: src=roles/okapi-demo/files/vagrant-tidy.sh dest=/root/vagrant-tidy.sh mode=0700
-      copy: src=roles/okapi-demo/files/vagrant-tidy.sh dest=/root/vagrant-tidy.sh mode=0700
-      copy: src=roles/okapi-demo/files/vagrant-tidy.sh dest=/root/vagrant-tidy.sh mode=0700
-      copy: src=roles/okapi-demo/files/vagrant-tidy.sh dest=/root/vagrant-tidy.sh mode=0700
BREAKS HERE
-- name: "Add nodes to required groups"
-  hosts: localhost
-  connection: local
-  gather_facts: False
-  tasks:
-     add_host:
-        name: "{{ hostvars[item].inventory_hostname }}"
-        ansible_host: "{{ hostvars[item].ansible_host|default(hostvars[item].ansible_ssh_host) }}"
-        ansible_user: "{{ hostvars[item].ansible_user|default('root') }}"
-        ansible_ssh_pass: "{{ hostvars[item].ansible_ssh_pass|default('') }}"
-        ansible_become_user: root
-        ansible_become_pass: "{{ hostvars[item].ansible_ssh_pass|default('') }}"
-        groups: ambari-node
-      with_items: "{{ groups['admin-nodes']|sort|last }}"
-
BREAKS HERE
-      user: cloud-user
-    local_action: shell ssh-keyscan {{ inventory_hostname }} >> {{user_ssh_dir}}/.ssh/known_hosts
BREAKS HERE
-  group_by: "key=os_{{ ansible_os_family }}"
-  group_by: "key={{ vault_node_role }}"
BREAKS HERE
-    - debug: msg="WARNING - upgrading a ceph cluster with only one monitor node ({{ inventory_hostname }})"
-      when: mon_host_count | int == 1
-
-    - name: fail when single containerized monitor
-        msg: "Upgrades of a single monitor are not supported, also running 1 monitor is not recommended always use 3."
-        - containerized_deployment
-        - mon_host_count | int == 1
-    - name: select a running monitor if multiple monitors
-        mon_host: "{{ groups[mon_group_name] | difference([inventory_hostname]) | last if mon_host_count > 1 else groups[mon_group_name] | last }}"
BREAKS HERE
-        - name: Deactivates the default virtualhost
-          action: command a2dissite default
-
BREAKS HERE
-        name: "ceph-osd@{{ item.0.stdout[:-1] | regex_replace('/dev/', '') }}"
BREAKS HERE
-      command: aria2c --input-file=/tmp/aria2c.input {{ aria2c_parameters }}
BREAKS HERE
-    - name: set osd flags
-      command: ceph --cluster {{ cluster }} osd set {{ item }}
-      with_items:
-        - noout
-        - noscrub
-        - nodeep-scrub
-      delegate_to: "{{ groups[mon_group_name][0] }}"
-      when: not containerized_deployment
-
-    - name: set containerized osd flags
-      command: |
-        docker exec ceph-mon-{{ hostvars[groups[mon_group_name][0]]['ansible_hostname'] }} ceph --cluster {{ cluster }} osd set {{ item }}
-      with_items:
-        - noout
-        - noscrub
-        - nodeep-scrub
-      delegate_to: "{{ groups[mon_group_name][0] }}"
-      when: containerized_deployment
-
BREAKS HERE
-        module: cloudera/cloudera_init
-        name: "{{ cluster_name }}"
-        fullVersion: 5.7.0
-        admin_password: admin
-        cm_host: "{{ groups['cm-node'][0] }}"
-        hosts: "{{ groups['hadoop-cluster']|join(',') }}"
-        state: present
-#    - name: Cloudera setup manager
-#      local_action:
-#        module: cloudera_deploy_manager
-#        name: "{{ cluster_name }}"
-#        fullVersion: 5.6.0
-#        admin_password: admin
-#        cm_host: "{{ groups['cm-node'][0] }}"
-#        hosts: "{{ groups['hadoop-cluster']|join(',') }}"
-#        service_host: "{{ groups['cm-node'][0] }}"
-#        service_pass: temp
-#        state: present
-#      register: my_mngr
-#
-#    - debug: var=my_mngr
-#
BREAKS HERE
-            --setopt 'rhelosp-{{ install.version }}.0-devtools-puddle.includepkgs=openstack-packstack*' \
BREAKS HERE
-      with_file:
BREAKS HERE
-
-- name: upgrade ceph osds cluster
-
-  vars:
-    health_osd_check_retries: 40
-    health_osd_check_delay: 30
-    upgrade_ceph_packages: True
-
-  hosts:
-    - "{{ osd_group_name|default('osds') }}"
-
-  serial: 1
-  become: True
-
-  pre_tasks:
BREAKS HERE
-          - "lxc.mount.entry = udev dev devtmpfs defaults 0 0"
-      when: (is_metal == false or is_metal == "False") and inventory_hostname in groups['cinder_volume']
BREAKS HERE
-- hosts: common
-
-- hosts: java
-  roles:
-
-- hosts: tomcat
-  roles:
-
-- hosts: cassandra
-  roles:
-    - { role: cassandra }
-
-- hosts: mysql
-  roles:
-
-- hosts: apache
-  roles:
-
-- hosts: nameindex
-  roles:
-
-- hosts: solr
-  roles:
-
-- hosts: biocache-db
-  roles:
-
-- hosts: biocache-service
-  roles:
-
-- hosts: biocache-hub
-  roles:
-
-- hosts: collectory-db
-  roles:
-
-- hosts: collectory
-  roles:
-
-- hosts: datacheck
-  roles:
-
-- hosts: demo-apache
-  roles:
-
-- hosts: cli
-  roles:
-    - cli
BREAKS HERE
-        httpd_bin: "/usr/sbin/apache2"
BREAKS HERE
-  - { role: openshift/object, app: greenwave, file: deploymentconfig.yml }
BREAKS HERE
-
BREAKS HERE
-    - name: Increase priority for Ceph repos
-           --setopt="Ceph.priority=50"
-           --setopt="Ceph-noarch.priority=50"
-    - name: Increase priority for Ceph repos
-           --setopt="Ceph.priority=50"
-           --setopt="Ceph-noarch.priority=50"
BREAKS HERE
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      vars:
-        user_name: "{{ sahara_galera_user }}"
-        password: "{{ sahara_container_mysql_password }}"
-        login_host: "{{ sahara_galera_address }}"
-        db_name: "{{ sahara_galera_database }}"
-      when: inventory_hostname == groups['sahara_all'][0]
-
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - sahara
BREAKS HERE
-### Default configurations for openstack-ansible-security #####################
-# All of the configuration items below are documented in the developer notes
-# found here:
BREAKS HERE
-    - { role: ../../galaxy/openshift-ansible-contrib/roles/docker, tags: 'docker' }
BREAKS HERE
-            EC-2 Instance Tags:        Name={{ instance_name }}
-            EC-2 Instance ID:          {{ ec2_instances.tagged_instances[0].id }}
BREAKS HERE
-      - name: update system packages
-        package:
-            state: latest
-            name: "*"
-        register: update
-
-      - name: reboot the undercloud
-        shell: "sleep 2 && shutdown -r now"
-        async: 1
-        poll: 0
-        ignore_errors: true
-        when: update is changed
-
-      - name: wait for undercloud to go down
-        become: no
-        command: ping -c1 {{ ansible_host|default(ansible_ssh_host) }}
-        register: node_down
-        until: node_down.rc != 0
-        retries: 100
-        delay: 3
-        ignore_errors: true
-        delegate_to: localhost
-        when:
-          - update is changed
-          - "'hypervisor' not in groups"
-
-      - name: wait for undercloud to go down
-        command: ping -c1 {{ ansible_host|default(ansible_ssh_host) }}
-        register: node_down
-        until: node_down.rc != 0
-        retries: 100
-        delay: 3
-        ignore_errors: true
-        delegate_to: hypervisor
-        when:
-          - update is changed
-          - "'hypervisor' in groups"
-
-      # Ansible bug around delegate+vars+wait
-      # https://github.com/ansible/ansible/issues/11705
-      # http://www.elmund.io/configuration%20management/2015/07/23/ansible-delegate_to-and-variables/
-      - name: waiting for the undercloud to be available
-        become: no
-        wait_for:
-            port: 22
-            host: "{{ ansible_host }}"
-            search_regex: OpenSSH
-            timeout: 30
-        register: uc_reachable
-        delegate_to: localhost
-        when: "'hypervisor' not in groups"
-        retries: 21
-        until: uc_reachable|succeeded
-
-      - name: waiting for the undercloud to be available
-        become: no
-        wait_for:
-            port: 22
-            host: "{{ ansible_host }}"
-            search_regex: OpenSSH
-            delay: 10
-            sleep: 3
-        delegate_to: hypervisor
-        when: "'hypervisor' in groups"
-        retries: 6
-        delay: 30
BREAKS HERE
-      raw_device=$(echo "{{ item }}" | egrep -o '/dev/([hsv]d[a-z]{1,2}|cciss/c[0-9]d[0-9]p|nvme[0-9]n[0-9]p){1,2}')
BREAKS HERE
-          - "{{ image_files.results }}"
BREAKS HERE
-        - '"Could not find imported module support code for test_failure.  Looked for either foo or zebra" == result["msg"]'
BREAKS HERE
-nova_rabbitmq_servers: 127.0.0.1
-nova_rabbitmq_port: 5672
-nova_rabbitmq_userid: nova
-nova_rabbitmq_vhost: /nova
-nova_rabbitmq_use_ssl: False
-
-## Configuration for notifications communication, i.e. [oslo_messaging_notifications]
-nova_rabbitmq_telemetry_userid: "{{ nova_rabbitmq_userid }}"
-nova_rabbitmq_telemetry_password: "{{ nova_rabbitmq_password }}"
-nova_rabbitmq_telemetry_vhost: "{{ nova_rabbitmq_vhost }}"
-nova_rabbitmq_telemetry_port: "{{ nova_rabbitmq_port }}"
-nova_rabbitmq_telemetry_servers: "{{ nova_rabbitmq_servers }}"
-nova_rabbitmq_telemetry_use_ssl: "{{ nova_rabbitmq_use_ssl }}"
-
-
BREAKS HERE
-- name: kill a dummy container that created pool(s)/key(s)
-  command: docker rm -f ceph-create-keys
-  changed_when: false
-  when:
-    - containerized_deployment
-    - inventory_hostname == groups.get(client_group_name) | first
-
BREAKS HERE
-- name: Power off overcloud nodes VMs so they can be properly registered
-  hosts: hypervisor
-  gather_facts: no
-  any_errors_fatal: true
-  tasks:
BREAKS HERE
-lxc_host_machine_volume_size: 8
BREAKS HERE
-            shell: ssh -i ~/backup_server_auth_key {{ install.dest.address }} md5sum {{install.dest.path}}/{{ undercloud_image_file }}
-                msg: "The copy wasn't successfuly completed"
BREAKS HERE
-  - debug: msg="hello world"
BREAKS HERE
-    - java
-    - tomcat
BREAKS HERE
-      local_action:
-        module: wait_for
BREAKS HERE
-    - name: Posttask 2
BREAKS HERE
-    firmware_name: "" # provide your firwmare name here
BREAKS HERE
-    name: "{{ item.packages }}"
-  with_items:
-    - packages: "{{ neutron_optional_bgp_pip_packages }}"
-      enabled: "{{ neutron_bgp | bool }}"
-    - packages: "{{ neutron_optional_fwaas_pip_packages }}"
-      enabled: "{{ neutron_fwaas | bool }}"
-    - packages: "{{ neutron_optional_lbaas_pip_packages }}"
-      enabled: "{{ neutron_lbaasv2 | bool }}"
-    - packages: "{{ neutron_optional_vpnaas_pip_packages }}"
-      enabled: "{{ neutron_vpnaas | bool }}"
-  when: "{{ item.enabled }}"
BREAKS HERE
-- name: Start neutron-server
BREAKS HERE
-- include_tasks: neutron_check.yml
BREAKS HERE
-            dpkg-query -s python $> /dev/null
BREAKS HERE
-    - {
-      role: osbs-namespace,
-        osbs_namespace: "{{ osbs_worker_namespace }}",
-        osbs_service_accounts: "{{ osbs_worker_service_accounts }}",
-        osbs_nodeselector: "{{ osbs_worker_default_nodeselector|default('') }}",
-        osbs_authoritative_registry: "{{ source_registry }}",
-        osbs_sources_command: "{{ osbs_conf_sources_command }}",
-        osbs_vendor: "{{ osbs_conf_vendor }}",
-    }
-      osbs_namespace: "{{ osbs_worker_namespace }}"
-      osbs_secret_name: "kojisecret"
-      osbs_secret_files:
-        - source: "/etc/pki/koji/fedora-builder.pem"
-          dest: cert
-      osbs_namespace: "{{ osbs_worker_namespace }}"
-      osbs_secret_name: "registry-secret"
-      osbs_secret_files:
-        - source: "{{private}}/files/koji/{{docker_cert_name}}.cert.pem"
-          dest: registry.crt
-        - source: "{{private}}/files/koji/{{docker_cert_name}}.key.pem"
-          dest: registry.key
BREAKS HERE
-  - "postgresql-server"
-  - "postgresql-contrib"
-  - "python3-psycopg2"
-  - "libselinux-python"
-  - "libsemanage-python"
-  - "python3-virtualenv"
-  - "community-mysql-devel"
-  - "openldap-devel"
-  - "python3-pip"
-  - "gcc"
-  - "git"
-  - "httpd"
-  - "python3-mod_wsgi"
-  - "policycoreutils-python"
-  - "postfix"
-  - "mod_ssl"
-  - "letsencrypt"
-- command: "{{ item }}"
BREAKS HERE
-  - notifs-frontend
-  - notifs-frontend-stg
-  hosts: notifs-frontend;notifs-frontend-stg
-  - notifs-frontend
-  - notifs-frontend-stg
BREAKS HERE
-   - "/srv/web/infra/ansible/vars/nagios.{{ inventory_hostname }}.yml"
BREAKS HERE
-  - role: GKS-slave_node
-    tags:
-      - configure_slurm_slave_node
BREAKS HERE
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
-  serial: '{{ "30%" if action == "upgrade" else "0" }}'
BREAKS HERE
-    
BREAKS HERE
-    copy: dest=/etc/pam.d/mock src="{{ files }}/common/mock
BREAKS HERE
-  run_once: true
-  when: containerized_deployment
BREAKS HERE
-  - name: figure out which node to delegate os uncordon to
-  - name: figure out which node to delegate os uncordon to
-  - name: figure out which node to delegate os uncordon to
BREAKS HERE
-# The value is specified in seconds, with the default being 10 minutes.
-lxc_cache_prep_timeout: 600
BREAKS HERE
-      shell: source /opt/stack/ansible/hacking/env-setup && ansible-playbook -vvvv -i inventory/localhost install.yaml -e network_interface={{network_interface}}
BREAKS HERE
-  when:
-    - lxc_container_fixed_mac | bool
-  when:
-    - lxc_container_fixed_mac | bool
-  when:
-    - lxc_container_fixed_mac | bool
-  when:
-    - lxc_container_fixed_mac | bool
BREAKS HERE
-    apt: "name={{item}} state=present"
-    with_items:
BREAKS HERE
-- hosts: localhost
BREAKS HERE
-    - name: Set glusterfs-storage class to default
-      when: install_glusterfs|bool
-      shell: "oc patch storageclass glusterfs-storage -p '{\"metadata\": {\"annotations\": \
-    {\"storageclass.kubernetes.io/is-default-class\": \"true\"}}}'"
-    - name: Remove default from glusterfs-storage-block class
-      when: install_glusterfs|bool
-      shell: "oc patch storageclass glusterfs-storage-block -p '{\"metadata\": {\"annotations\": \
-    {\"storageclass.kubernetes.io/is-default-class\": \"false\"}}}'"
BREAKS HERE
-  tags: ocp-pre
-
-  ignore_errors: true
-  tags: logging-metrics
-- name: install Grafana
-  import_playbook: /usr/share/ansible/openshift-ansible/playbooks/openshift-grafana/config.yml
-  ignore_errors: true
-  tags: logging-metrics
-  
-  vars:
-    admin_user: "{{ openshift_master_htpasswd_users.keys() }}"
-    - name: Install oc client
-      yum:
-        name: atomic-openshift-clients
-        state: latest
-      notify:
-        - add cluster-admin
-      tags: ocp-post
- 
-      notify:
-        - add cluster-admin
-  handlers:
-      shell: oc adm policy add-cluster-role-to-user cluster-admin "{{ admin_user }}"
- 
BREAKS HERE
-      when: env == "production"
-      when: env == "staging"
BREAKS HERE
-## Nova s3
-nova_s3_service_name: s3
-nova_s3_service_type: s3
-nova_s3_service_proto: http
-nova_s3_service_publicuri_proto: "{{ openstack_service_publicuri_proto | default(nova_s3_service_proto) }}"
-nova_s3_service_adminuri_proto: "{{ openstack_service_adminuri_proto | default(nova_s3_service_proto) }}"
-nova_s3_service_internaluri_proto: "{{ openstack_service_internaluri_proto | default(nova_s3_service_proto) }}"
-nova_s3_service_port: 3333
-nova_s3_service_description: "S3 Compatibility Layer"
-nova_s3_service_publicuri: "{{ nova_s3_service_publicuri_proto }}://{{ external_lb_vip_address }}:{{ nova_s3_service_port }}"
-nova_s3_service_publicurl: "{{ nova_s3_service_publicuri }}"
-nova_s3_service_adminuri: "{{ nova_s3_service_adminuri_proto }}://{{ internal_lb_vip_address }}:{{ nova_s3_service_port }}"
-nova_s3_service_adminurl: "{{ nova_s3_service_adminuri }}"
-nova_s3_service_internaluri: "{{ nova_s3_service_internaluri_proto }}://{{ internal_lb_vip_address }}:{{ nova_s3_service_port }}"
-nova_s3_service_internalurl: "{{ nova_s3_service_internaluri }}"
-nova_s3_program_name: nova-api-ec2
-nova_s3_deprecated_but_enabled: false
-
-## Nova v3
-nova_v3_service_name: novav3
-nova_v3_service_type: computev3
-nova_v3_service_proto: http
-nova_v3_service_publicuri_proto: "{{ openstack_service_publicuri_proto | default(nova_v3_service_proto) }}"
-nova_v3_service_adminuri_proto: "{{ openstack_service_adminuri_proto | default(nova_v3_service_proto) }}"
-nova_v3_service_internaluri_proto: "{{ openstack_service_internaluri_proto | default(nova_v3_service_proto) }}"
-nova_v3_service_port: 8774
-nova_v3_service_description: "Nova Compute Service V3"
-nova_v3_service_publicuri: "{{ nova_v3_service_publicuri_proto }}://{{ external_lb_vip_address }}:{{ nova_v3_service_port }}"
-nova_v3_service_publicurl: "{{ nova_v3_service_publicuri }}/v3"
-nova_v3_service_adminuri: "{{ nova_v3_service_adminuri_proto }}://{{ internal_lb_vip_address }}:{{ nova_v3_service_port }}"
-nova_v3_service_adminurl: "{{ nova_v3_service_adminuri }}/v3"
-nova_v3_service_internaluri: "{{ nova_v3_service_internaluri_proto }}://{{ internal_lb_vip_address }}:{{ nova_v3_service_port }}"
-nova_v3_service_internalurl: "{{ nova_v3_service_internaluri }}/v3"
-nova_v3_deprecated_but_enabled: false
-
-nova_v21_service_name: novav21
-nova_v21_service_type: computev21
-nova_v21_service_proto: http
-nova_v21_service_publicuri_proto: "{{ openstack_service_publicuri_proto | default(nova_v21_service_proto) }}"
-nova_v21_service_adminuri_proto: "{{ openstack_service_adminuri_proto | default(nova_v21_service_proto) }}"
-nova_v21_service_internaluri_proto: "{{ openstack_service_internaluri_proto | default(nova_v21_service_proto) }}"
-nova_v21_service_port: 8774
-nova_v21_service_description: "Nova Compute Service V2.1"
-nova_v21_service_publicuri: "{{ nova_v21_service_publicuri_proto }}://{{ external_lb_vip_address }}:{{ nova_v21_service_port }}"
-nova_v21_service_publicurl: "{{ nova_v21_service_publicuri }}/v2.1"
-nova_v21_service_adminuri: "{{ nova_v21_service_adminuri_proto }}://{{ internal_lb_vip_address }}:{{ nova_v21_service_port }}"
-nova_v21_service_adminurl: "{{ nova_v21_service_adminuri }}/v2.1"
-nova_v21_service_internaluri: "{{ nova_v21_service_internaluri_proto }}://{{ internal_lb_vip_address }}:{{ nova_v21_service_port }}"
-nova_v21_service_internalurl: "{{ nova_v21_service_internaluri }}/v2.1"
-nova_v21_enabled: true
-
-## Nova v2
-nova_service_publicurl: "{{ nova_service_publicuri }}/v2/%(tenant_id)s"
-nova_service_adminurl: "{{ nova_service_adminuri }}/v2/%(tenant_id)s"
-nova_service_internalurl: "{{ nova_service_internaluri }}/v2/%(tenant_id)s"
-## Nova ec2
-# WARNNING: The EC2 api in the nova tree has been deprecated. To consume this API you'll need to
-# uncomment the EC2 section found within the nova `api-paste.ini` file.
-nova_ec2_service_name: ec2
-nova_ec2_service_type: ec2
-nova_ec2_service_proto: http
-nova_ec2_service_publicuri_proto: "{{ openstack_service_publicuri_proto | default(nova_ec2_service_proto) }}"
-nova_ec2_service_adminuri_proto: "{{ openstack_service_adminuri_proto | default(nova_ec2_service_proto) }}"
-nova_ec2_service_internaluri_proto: "{{ openstack_service_internaluri_proto | default(nova_ec2_service_proto) }}"
-nova_ec2_service_port: 8773
-nova_ec2_service_description: "EC2 Compatibility Layer"
-nova_ec2_service_publicuri: "{{ nova_ec2_service_publicuri_proto }}://{{ external_lb_vip_address }}:{{ nova_ec2_service_port }}"
-nova_ec2_service_publicurl: "{{ nova_ec2_service_publicuri }}/services/Cloud"
-nova_ec2_service_adminuri: "{{ nova_ec2_service_adminuri_proto }}://{{ internal_lb_vip_address }}:{{ nova_ec2_service_port }}"
-nova_ec2_service_adminurl: "{{ nova_ec2_service_adminuri }}/services/Admin"
-nova_ec2_service_internaluri: "{{ nova_ec2_service_internaluri_proto }}://{{ internal_lb_vip_address }}:{{ nova_ec2_service_port }}"
-nova_ec2_service_internalurl: "{{ nova_ec2_service_internaluri }}/services/Cloud"
-nova_ec2_program_name: nova-api-ec2
-nova_ec2_deprecated_but_enabled: false
-
-# If ``nova_ec2_workers`` is unset the system will use half the number of available VCPUS to
-# compute the number of api workers to use.
-# nova_ec2_workers: 16
-
-  - "{{ nova_s3_program_name }}"
-  - "{{ nova_ec2_program_name }}"
BREAKS HERE
-  hosts: controller
BREAKS HERE
-      security_groups: "{{ topology_node.security_groups | default(omit) }}"
BREAKS HERE
-        PATH: "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games"
-    - name: Check for environment file
-      stat:
-        path: /etc/environment
-      register: environment_file
-          - "environment_file.stat.exists"
BREAKS HERE
-        state: firmware_installed
BREAKS HERE
-# create a new openqa worker server system
-# NOTE: should be used with --limit most of the time
-# NOTE: most of these vars_path come from group_vars/backup_server or from hostvars
-# This has an extra role that configures the virthost to be used with beaker for
-# virtual machine clients
-
-- name: basic configuration
-  vars_files: 
-  - base
-  - rkhunter
-  - { role: denyhosts, when: ansible_distribution_major_version|int != 7 }
-  - nagios_client
-  - hosts
-  - fas_client
-  - collectd/base
-  - { role: iscsi_client, when: datacenter == "phx2" }
-  - sudo
-  - { role: openvpn/client, when: datacenter != "phx2" }
-  - include: "{{ handlers }}/restart_services.yml"
-
-#- name: configure openqa workers
-#  hosts: openqa-workers:openqa-stg-workers
-#  user: root
-#  gather_facts: True
-#
-#  vars_files:
-#   - /srv/web/infra/ansible/vars/global.yml
-#   - "/srv/private/ansible/vars.yml"
-#   - /srv/web/infra/ansible/vars/{{ ansible_distribution }}.yml
-#
-#  roles:
-#   - { role: openqa/something, tags: ['something'] }
-#
-#  handlers:
-#   - include: "{{ handlers }}/restart_services.yml"
BREAKS HERE
-      galaxy_changeset_id: "{{ galaxy_git_branch }}"
-    #- role: galaxy.movedata
-    #  check_tool_data: yes
-    #  when: install_galaxy
-      galaxy_tools_galaxy_instance_url: "{{ galaxy_hostname }}:{{ galaxy_port }}"
-    - role: artimed_extras
-      restart_galaxy: true
-      when: install_dms
-      galaxy_tools_galaxy_instance_url: "{{ galaxy_hostname }}:{{ galaxy_port }}"
BREAKS HERE
-    - playbooks/test-vars.yml
BREAKS HERE
-- include: ironic_api_install.yml
-  when: inventory_hostname in groups['ironic_api']
-  tags:
-    - ironic-install
-
-- include: ironic_conductor_install.yml
-  when: inventory_hostname in groups['ironic_conductor']
-  tags:
-    - ironic-install
-
BREAKS HERE
-    - name: Clone git repos
BREAKS HERE
-      - raw_multi_journal is defined
-      - raw_multi_journal
-
BREAKS HERE
-  - { role: ansible-role-flowdock, tags: [ 'flowdock' ] }
BREAKS HERE
-        msg:
-          - "Variables are not defined"
-          - "Please, define the following variables - junos_ip and/or cisco_ip, switch_user, switch_pass, server_name, vlan_id"
-      when: item == ''
-      when: cisco_ip != ''
-      when: junos_ip != ''
BREAKS HERE
-        state: absent
BREAKS HERE
-- name: create container hostgroups for destroyer
-    - name: Generate container_host group
-      role: create_container_host_group
-    - name: Generate openstack host groups
-      role: create_openstack_host_group
-- name: Apply role destroy to contrail
-    - set_global_variables
-    - build_node_lists
BREAKS HERE
-- name: Check init system
-  command: cat /proc/1/comm
-  changed_when: false
-  register: _pid1_name
-  tags:
-    - always
-
-- name: Set the name of pid1
-  set_fact:
-    pid1_name: "{{ _pid1_name.stdout }}"
-  tags:
-    - always
-
-
BREAKS HERE
-  with_items: lxc_packages
BREAKS HERE
-- include: keystone_pre_install.yml
-- include: keystone_install.yml
-- include: keystone_post_install.yml
-- include: keystone_key_setup.yml
-- include: keystone_fernet.yml
-  static: no
-- include: keystone_credential.yml
-  static: no
-- include: keystone_federation_sp_setup.yml
-  static: no
-- include: keystone_db_setup.yml
-- include: keystone_token_cleanup.yml
-  static: no
-- include: keystone_ssl.yml
-- include: "keystone_{{ keystone_web_server }}.yml"
-- include: keystone_uwsgi.yml
-- include: keystone_service_setup.yml
-- include: keystone_ldap_setup.yml
-  static: no
-- include: keystone_federation_sp_idp_setup.yml
-- include: keystone_idp_setup.yml
-  static: no
BREAKS HERE
-- include: nova_init_common.yml
-- include: nova_compute_wait.yml
-  when:
-    - "'nova_compute' in group_names"
-    - nova_compute_restart | default(dict(changed=False)) | changed
-    - nova_discover_hosts_in_cells_interval | int < 1
-  tags:
-    - nova-config
-
BREAKS HERE
-  default: apk, apt, dnf, homebrew, openbsd_pkg, pacman, pkgng, yum, zypper
BREAKS HERE
-    - role: borgbackup
-      tags: ['backup']
BREAKS HERE
-                -o IdentitiesOnly=yes \
BREAKS HERE
-        repo: "https://github.com/Admin9705/PlexGuide-Core"
-        dest: "{{boxbranch.stdout}}"
-        version: "{{boxuser.stdout}}"
BREAKS HERE
-   - /srv/private/files/openstack/overcloudrc.yml
BREAKS HERE
-      - name: "Workaround BZ#1717469 - python-websocket-client needs WEBSOCKET_CLIENT_CA_BUNDLE env to be set"
-        shell: |
-           echo "export WEBSOCKET_CLIENT_CA_BUNDLE=/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem" >> ~/stackrc
-        when:
-            - install.version|openstack_distribution == 'OSP'
-            - install.version|openstack_release >= 10
-            - install.version|openstack_release <= 14
-
BREAKS HERE
-- name: "Add Vault user
BREAKS HERE
-    - /vars/config.yml
-        nginx_auth_basic_files: '{{nginx_auth_basic_files}}'
-
-          drdc_ports:
-            - "{{docker_host_ip}}:{{ports.chproxy}}:9090"
-
-        drdc_volumes:
-          - '{{data_dir}}:/opt/notebooks'
-        drdc_ports:
-          - "{{docker_host_ip}}:{{ports.jupyter}}:8888"
-          drdc_memory_limit: '500m'
BREAKS HERE
-  # TODO: Switch the default to True in either the Ansible 2.6 release or the 2.7 release, whichever happens after the Tower 3.3 release
-  default: False
BREAKS HERE
-        version: "v8.5.6"
BREAKS HERE
-  - name: debug hostvars
-    debug:
-      var: hostvars
BREAKS HERE
-# This playbook is used primarily for CMNT integration to include units that
-# do not have public CMNT IP space, and require NAT for IPsec NAT-T to
-# function with CMNT headend HAIPEs.
-    # Ask the router what NAT statements are already configured and store them
-    # in a list of current NATs.
-    - name: "IOS >> Get current NAT statements"
-      ios_command:
-        provider: "{{ login_creds }}"
-        commands: "show running-config | include ^ip nat name .*"
-      register: CUR_NATS
-
-    # Print the output of the current NATs for debugging, but it will be
-    # ugly and hard to read.
-    - name: "DEBUG >> Print current NAT statements"
-      debug:
-        msg: "{{ CUR_NATS.stdout[0] }}"
-        verbosity: 1
-
-    # The 'nat' template has the intelligence to determine when changes need
-    # to be made to the router. Toggle the 'save_when' key between never
-    # and always depending on if the NATs should be saved in the configured.
-    #
-    # Notify the "updates exist" handlers when changes occur.
-    - name: "IOS >> Manage state NAT statements"
-      ios_config:
-        provider: "{{ login_creds }}"
-        src: "templates/nat.j2"
-        save_when: never
-        # save_when: always
-        match: none
-      register: NAT_CONFIG
-      changed_when: "NAT_CONFIG.updates is defined"
-      notify: "updates exist"
-
-    # Get the NAT table to determine if the configuration was successful and
-    # the router installed state entries appropriately. Relying only on the
-    # configuration is not wise, so this extra verification ensures quality.
-    - name: "IOS >> Get NAT translation table"
-      ios_command:
-        provider: "{{ login_creds }}"
-        commands: "show ip nat translations{{ VRF_STR }}"
-      register: NAT_TABLE
BREAKS HERE
-    - yum:
-          list: libvirt
-      vars:
-          libvirt_state: "{{  libvirt_status.results|map(attribute='yumstate')|list }}"
-      when: "'installed' in libvirt_state"
BREAKS HERE
-        ansible_ssh_user: Administrator
BREAKS HERE
-- name: Fetch the overcloud images
-  hosts: undercloud
-  gather_facts: no
-  roles:
-   - fetch-images
-
-- name: Build test packages using DLRN
-    - {role: build-test-packages, when: build_test_packages|default(false)|bool }
-- name: Install the built package on the undercloud
-    - {role: install-built-repo, when: build_test_packages|default(false)|bool }
BREAKS HERE
-    - name: Set grubby
-      shell: |
-           kernel_path=$(grubby --default-kernel);
-           grub_params="intel_iommu=on igb.max_vfs=7";
-           grubby --update-kernel=${kernel_path} --args="${grub_params}";
BREAKS HERE
-    fail: msg="you CAN NOT delete the last member of etcd cluster!"
-    when: "groups['etcd']|length < 2 and NODE_TO_DEL in groups['etcd']"
-
-  - name: fail info2
-  task:
BREAKS HERE
-      with_items: utility_apt_packages | default([])
-      with_items: utility_pip_packages
BREAKS HERE
-    _oslomsg_configure_rpc: "{{ keystone_messaging_enabled | bool }}"
-    _oslomsg_configure_notify: "{{ keystone_ceilometer_enabled | bool }}"
BREAKS HERE
-    _oslomsg_configure_notify: "{{ (neutron_ceilometer_enabled | bool) or (neutron_designate_enabled | bool) }}"
BREAKS HERE
-  when: storage.data_centers is not defined
-  when: storage.data_centers is not defined
-    pattern: "status=up and storage={{ storage.name }}"
-  when: storage.data_centers is defined
-  set_fact: sp_uuid="{{ storage.data_centers[0].id }}"
-  #storage={{ storage }} host={{ ovirt_hosts[0] }} sp_uuid=sp_uuid
-  when: "force"
BREAKS HERE
-- name: Expose bind_address and node_role as facts
BREAKS HERE
-            vbmc_nodes: "{{ groups.get('overcloud_nodes', []) }}"
-        with_items: "{{ groups['overcloud_nodes']|sort }}"
-      - name: Read baremetal nodes got from user input on hybrid deployment
-        include_vars:
-            file: "{{ install.hybrid }}"
-            name: bm_nodes
-        when: install.hybrid is defined
-
-        with_items: "{{ groups['overcloud_nodes'] }}"
BREAKS HERE
-      {{ ((openstack_service_setup_host | default('localhost')) == 'localhost') | ternary(ansible_playbook_python, ansible_python['executable']) }}
BREAKS HERE
-    privatefile: fedmsg-certs/keys/messaging-bridges-stg.key
-    when: env == "staging"
-    privatefile: fedmsg-certs/keys/messaging-bridges-stg.crt
-    when: env == "staging"
-  - role: openshift/secret-file
-    app: messaging-bridges
-    secret_name: fedmsg-key
-    key: fedmsg-fedmsg-migration-tools.key
-    privatefile: fedmsg-certs/keys/messaging-bridges.key
-    when: env != "staging"
-  - role: openshift/secret-file
-    app: messaging-bridges
-    secret_name: fedmsg-cert
-    key: fedmsg-fedmsg-migration-tools.crt
-    privatefile: fedmsg-certs/keys/messaging-bridges.crt
-    when: env != "staging"
BREAKS HERE
-        dest: "{{ tower_base_path }}/tower-backup-latest.tar.gz"
-        
-      shell: "{{tower_installer_path.stdout}} -r {{ tower_base_path }}/tower-backup-latest.tar.gz"
-      
BREAKS HERE
-    command: rm -rf /var/lib/ceph/*
BREAKS HERE
-- name: Get local venv checksum
-  stat:
-    path: "/var/cache/{{ neutron_venv_download_url | basename }}"
-    get_md5: False
-  when:
-    - not neutron_developer_mode | bool
-  register: local_venv_stat
-
-- name: Get remote venv checksum
-  uri:
-    url: "{{ neutron_venv_download_url | replace('tgz', 'checksum') }}"
-    return_content: True
-  when:
-    - not neutron_developer_mode | bool
-  register: remote_venv_checksum
-
-# TODO: When project moves to ansible 2 we can pass this a sha256sum which will:
-#       a) allow us to remove force: yes
-#       b) allow the module to calculate the checksum of dest file which would
-#          result in file being downloaded only if provided and dest sha256sum
-#          checksums differ
-    force: yes
-  failed_when: false
-  register: get_venv
-  when:
-    - not neutron_developer_mode | bool
-    - (local_venv_stat.stat.exists == False or
-        {{ local_venv_stat.stat.checksum is defined and local_venv_stat.stat.checksum != remote_venv_checksum.content | trim }})
-
-- name: Set neutron get_venv fact
-  set_fact:
-    neutron_get_venv: "{{ get_venv }}"
-  when:
-    - neutron_get_venv | changed
-  when:
-    - not neutron_developer_mode | bool
-  when:
-    - neutron_get_venv | failed or neutron_developer_mode | bool
-    - neutron_get_venv | success
BREAKS HERE
-  - { role: openshift/object, app: greenwave, template: deploymentconfig.yml }
BREAKS HERE
-    - name: Gather variables for each operating system
-      include_vars: "{{ item }}"
-      with_first_found:
-        - "{{ playbook_dir }}/vars/{{ ansible_distribution | lower }}-{{ ansible_distribution_version | lower }}.yml"
-        - "{{ playbook_dir }}/vars/{{ ansible_distribution | lower }}-{{ ansible_distribution_major_version | lower }}.yml"
-        - "{{ playbook_dir }}/vars/{{ ansible_os_family | lower }}-{{ ansible_distribution_major_version | lower }}.yml"
-        - "{{ playbook_dir }}/vars/{{ ansible_distribution | lower }}.yml"
-        - "{{ playbook_dir }}/vars/{{ ansible_os_family | lower }}.yml"
-      tags:
-        - always
-
-    - name: Get info about the virt storage pools
-      tags:
-        - always
-    - name: Stop running VMs
-        name: "{{ hostvars[item]['server_hostname'] }}"
-        state: shutdown
-      when:
-        - hostvars[item]['server_vm'] | default(false) | bool
-      with_items: "{{ groups['pxe_servers'] }}"
-        if [[ -e {{ hostvars[item]['server_hostname'] }}.img ]]; then
-          if [[ -e {{ hostvars[item]['server_hostname'] }}-base.img ]]; then
-            qemu-img commit {{ hostvars[item]['server_hostname'] }}.img
-            qemu-img convert -O qcow2 -c {{ hostvars[item]['server_hostname'] }}.img {{ hostvars[item]['server_hostname'] }}-base.img
-            qemu-img create -f qcow2 -b {{ hostvars[item]['server_hostname'] }}-base.img {{ hostvars[item]['server_hostname'] }}.img
-      when:
-        - hostvars[item]['server_vm'] | default(false) | bool
-      with_items: "{{ groups['pxe_servers'] }}"
-        src: "/etc/libvirt/qemu/{{ hostvars[item]['server_hostname'] }}.xml"
-      when:
-        - hostvars[item]['server_vm'] | default(false) | bool
-      with_items: "{{ groups['pxe_servers'] }}"
BREAKS HERE
-swift_account_server_program_config_options: /etc/swift/account-server/account-server.conf
-swift_account_replicator_program_config_options: /etc/swift/account-server/account-server-replicator.conf
-
-swift_container_server_program_config_options: /etc/swift/container-server/container-server.conf
-swift_container_replicator_program_config_options: /etc/swift/container-server/container-server-replicator.conf
-swift_container_reconciler_program_config_options: /etc/swift/container-server/container-reconciler.conf
-
-swift_object_server_program_config_options: /etc/swift/object-server/object-server.conf
-swift_object_replicator_program_config_options: /etc/swift/object-server/object-server-replicator.conf
-swift_object_expirer_program_config_options: /etc/swift/object-server/object-expirer.conf
-
-swift_proxy_server_program_config_options: /etc/swift/proxy-server/proxy-server.conf
-
-swift_account_program_names:
-  - swift-account-server
-  - swift-account-auditor
-  - swift-account-replicator
-  - swift-account-reaper
-  - swift-account-replicator-server
-
-swift_container_program_names:
-  - swift-container-server
-  - swift-container-auditor
-  - swift-container-replicator
-  - swift-container-sync
-  - swift-container-updater
-  - swift-container-replicator-server
-  - swift-container-reconciler
-
-swift_object_program_names:
-  - swift-object-server
-  - swift-object-auditor
-  - swift-object-replicator
-  - swift-object-updater
-  - swift-object-replicator-server
-  - swift-object-expirer
-
-swift_proxy_program_names:
-  - swift-proxy-server
BREAKS HERE
-    roles:
-     - postgresql_server
BREAKS HERE
-  - name: Create the amqp-to-zmq bindings
-    rabbitmq_binding:
-      source: amq.topic
-      destination: amqp_to_zmq
-      destination_type: queue
-      vhost: /pubsub
-      login_user: admin
-      login_password: "{{ rabbitmq_admin_password_staging }}"
-    tags:
-    - config
-  - name: Create the verify-missing bindings
-    rabbitmq_binding:
-      source: "{{item}}"
-      destination: amqp_bridge_verify_missing
-      destination_type: queue
-      vhost: /pubsub
-      login_user: admin
-      login_password: "{{ rabbitmq_admin_password_staging }}"
-    with_items:
-    - amq.topic
-    - zmq.topic
-    tags:
-    - config
BREAKS HERE
-- name: Get the passwords generated in the accounts tasks
-  tags: dovecot
-  set_fact:
-    roPasswdParams: "{{ playbook_dir }}/../../backup/ldap/readonly.pwd length=16 chars=ascii_letters,digits"
-
BREAKS HERE
-    command: cat ./gitlab_servers.out
BREAKS HERE
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
-      - cat2
-      - medium
-      - patch
BREAKS HERE
-                    key="{{ lookup('file', '{{ private }}/files/releng/sshkeys/primary-s390x-sshfs' + '-staging.pub' if env == 'staging' else '{ private }}/files/releng/sshkeys/primary-s390x-sshfs.pub') }}"
BREAKS HERE
-    with_items: vmlist.list_vms
-  - name: expire-caches
-    command: dnf clean expire-cache
-    when: ansible_distribution_major_version|int > 21 and ansible_cmdline.ostree is not defined
-
-  - name: dnf -y {{ yumcommand }}
-    command: dnf -y {{ yumcommand }}
BREAKS HERE
-#security_password_remember_password: 5                       # RHEL-07-010240
BREAKS HERE
-  - MySQL-python
BREAKS HERE
-- name: Configure instance(s)
-  hosts: launched
-  user: ubuntu
-  become: True
-  vars:
-     ansible_ssh_port: 22
-  gather_facts: True
-  roles:
-    - preconf
-  tags: preconf
-  tasks:
-    - name: restart machine after setup
-      shell: shutdown -r 1
-
BREAKS HERE
-    - { role: "{{ ANSIBLE_REPO_PATH }}/roles/bastion-opentlc-ipa", when: 'install_ipa_client' }
BREAKS HERE
-
-
-    
BREAKS HERE
-      internal_network: "{{ prefix }}{{ topology_node.external_network }}"
BREAKS HERE
-    - name: restart ceph osds (upstart)
-        state: restarted
-    - name: restart ceph osds (sysvinit)
-        state: restarted
-    - name: restart ceph osds (systemd)
-        state: restarted
BREAKS HERE
-
-    command: mkfs.ext4 /dev/vdb
-  - { role: docker_setup, device: '/dev/vdb'}
BREAKS HERE
-  local_action: add_host hostname="{{ inst_res.instances[0].public_ip }}" groupname=tmp_just_created
BREAKS HERE
-           delay: "{{ provision.wait | int }}"
-           sleep: 3
-      when: provision.wait > 0
BREAKS HERE
-      security_groups: "{{ topology_node.security_groups | default(omit) }}"
BREAKS HERE
-    - name: ensure ara-report folder existence
-      file:
-        path: "{{ zuul.executor.log_root }}/{{ inventory_hostname }}/ara-report"
-        state: directory
-      delegate_to: localhost
-      run_once: true
-
-    - name: download ara sqlite
-      synchronize:
-        src: "{{ ansible_env.HOME }}/.ara/ansible.sqlite"
-        dest: "{{ zuul.executor.log_root }}/{{ inventory_hostname }}/ara-report/"
-        mode: pull
BREAKS HERE
-      when: network_role == 'network-multus' and deploy_sriov_plugin == true and apb_action == 'provision'
-    - { role: "skydive", when: deploy_skydive == true}
BREAKS HERE
-  when: baseiptables
-  when: baseiptables
-  when: baseiptables
-  when: baseiptables
BREAKS HERE
-             bricks='{% for host in hosts %}
-             {{ hostvars[host]['mountpoints'] }};
-             {% endfor %}'
BREAKS HERE
-    host: "updates{{ env_suffix }}.coreos.fedoraproject.org"
BREAKS HERE
-    when: inventory_hostname.startswith == 'download-ib02'
-    when: inventory_hostname.startswith == 'download-ib01'
BREAKS HERE
-#TODO(michaelgugino): Break out tasks into yum/apt install steps as
-#required.
-#TODO(evrardjp): Replace the next 2 tasks by a standard apt with cache
-#when https://github.com/ansible/ansible-modules-core/pull/1517 is merged
-#in 1.9.x or we move to 2.0 (if tested working)
-- name: Check apt last update file
-  stat:
-    path: /var/cache/apt
-  register: apt_cache_stat
-  tags:
-    - neutron-apt-packages
-
-- name: Update apt if needed
-  apt:
-    update_cache: yes
-  when: "ansible_date_time.epoch|float - apt_cache_stat.stat.mtime > {{cache_timeout}}"
-  tags:
-    - neutron-apt-packages
-
-- name: Install neutron apt dependencies
-  apt:
-    pkg: "{{ item }}"
-    state: latest
-  register: install_packages
-  until: install_packages|success
-  retries: 5
-  delay: 2
-  with_items: "{{ neutron_apt_packages }}"
-  tags:
-    - neutron-install
-    - neutron-apt-packages
-
-- name: Install apt packages for LBaaS
-  apt:
-    pkg: "{{ item }}"
-    state: latest
-  register: install_packages
-  until: install_packages|success
-  retries: 5
-  delay: 2
-  with_items: "{{ neutron_lbaas_apt_packages }}"
-    - inventory_hostname in groups[neutron_services['neutron-lbaas-agent']['group']]
-    - neutron_lbaas | bool or neutron_lbaasv2 | bool
-    - neutron-install
-    - neutron-apt-packages
-
-- name: Install apt packages for VPNaaS
-  apt:
-    pkg: "{{ item }}"
-    state: latest
-  register: install_packages
-  until: install_packages|success
-  retries: 5
-  delay: 2
-  with_items: "{{ neutron_vpnaas_apt_packages }}"
-  when:
-    - inventory_hostname in groups[neutron_services['neutron-vpnaas-agent']['group']]
-    - neutron_vpnaas | bool
-  tags:
-    - neutron-install
-    - neutron-apt-packages
-
-- name: remove specific apt packages
-  apt:
-    pkg: "{{ item }}"
-    state: absent
-  register: remove_packages
-  until: remove_packages|success
-  retries: 5
-  delay: 2
-  with_items: "{{ neutron_apt_remove_packages }}"
-  tags:
-    - neutron-install
-    - neutron-apt-packages
BREAKS HERE
-          - ansible
-          - ansible
BREAKS HERE
-- name: Install Zabbix Agent on all hosts
-  hosts: "{{ ('tag_Project_' ~ env_type ~ '_' ~ guid) | replace('-', '_') }}"
-  gather_facts: true
-  become: yes
-  vars_files:
-    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_vars.yml"
-    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/env_secret_vars.yml"
-    - "{{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/ssh_vars.yml"
-  run_once: true
-  roles:
-    - { role: "{{ ANSIBLE_REPO_PATH }}/roles/zabbix-client" }
-  tags:
-    - env-specific
-    - install_zabbix
-
-
BREAKS HERE
-      security_groups: "{{ secgroups | default([]) | map('regex_replace', '(.*)', prefix + '\\1') | list or omit }}"
BREAKS HERE
-    - name: Expand filesystem
-      shell: "raspi-config --expand-rootfs"
-      sudo: yes
-
-    - name: Reboot
-      shell: sleep 2 && shutdown -r now
-      async: 1
-      poll: 0
-      sudo: true
-      ignore_errors: true
-  
-    - name: Wait for reboot
-      local_action: wait_for host={{ inventory_hostname }} port=22 state=started delay=20
-      sudo: false
BREAKS HERE
-      create: true
BREAKS HERE
-    # verify database deployments in cicd project
-    - name: wait for database deployments in project {{ project_cicd }} to complete (succeed or fail)
-      shell: "oc get pods -n {{ project_cicd }} -l comp-type=database | grep '\\-deploy\\s' | grep 'Running'"
-      register: deployment_running
-      until: deployment_running|failed
-      retries: 20
-      delay: 30
-      ignore_errors: true
-      tags: verify
-
-    - include_tasks: verify_tasks.yml project_name={{ project_cicd }} selector="comp-type=database"
-      tags: verify
-
-    # verify database deployments in prod project
-    - name: wait for database deployments in project {{ project_prod }} to complete (succeed or fail)
-      shell: "oc get pods -n {{ project_prod }} -l comp-type=database | grep '\\-deploy\\s' | grep 'Running'"
-      register: deployment_running
-      until: deployment_running|failed
-      retries: 20
-      delay: 30
-      ignore_errors: true
-      tags: verify
-
-    - include_tasks: verify_tasks.yml project_name={{ project_prod }} selector="comp-type=database"
-
-    # verify database deployments in test project
-    - name: wait for database deployments in project {{ project_test }} to complete (succeed or fail)
-      shell: "oc get pods -n {{ project_test }} -l comp-type=database | grep '\\-deploy\\s' | grep 'Running'"
-      register: deployment_running
-      until: deployment_running|failed
-      retries: 20
-      delay: 30
-      ignore_errors: true
-      tags: verify
-
-    - include_tasks: verify_tasks.yml project_name={{ project_test }} selector="comp-type=database"
-
-    # verify database deployments in dev project
-    - name: wait for database deployments in project {{ project_dev }} to complete (succeed or fail)
-      shell: "oc get pods -n {{ project_dev }} -l comp-type=database | grep '\\-deploy\\s' | grep 'Running'"
-      register: deployment_running
-      until: deployment_running|failed
-      retries: 20
-      delay: 30
-      ignore_errors: true
-      tags: verify
-
-    - include_tasks: verify_tasks.yml project_name={{ project_dev }} selector="comp-type=database"
-      tags: verify
-
-    # verify other deployments in cicd project
-    - name: wait for other deployments in project {{ project_cicd }} to complete (succeed or fail)
-      shell: "oc get pods -n {{ project_cicd }} -l comp-type!=database | grep '\\-deploy\\s' | grep 'Running'"
-      register: deployment_running
-      until: deployment_running|failed
-      retries: 20
-      delay: 30
-      ignore_errors: true
-      tags: verify
-
-    - include_tasks: verify_tasks.yml project_name={{ project_cicd }} selector="comp-type!=database"
-      tags: verify
-
-
-    # verify other deployments in prod project
-    - name: wait for other deployments in project {{ project_prod }} to complete (succeed or fail)
-      shell: "oc get pods -n {{ project_prod }} -l comp-type!=database | grep '\\-deploy\\s' | grep 'Running'"
-      register: deployment_running
-      until: deployment_running|failed
-      retries: 20
-      delay: 30
-      ignore_errors: true
-      tags: verify
-
-    - include_tasks: verify_tasks.yml project_name={{ project_prod }} selector="comp-type!=database"
-      tags: verify
-
-    # verify other deployments in stage project
-    - name: wait for other deployments in project {{ project_stage }} to complete (succeed or fail)
-      shell: "oc get pods -n {{ project_stage }} -l comp-type!=database | grep '\\-deploy\\s' | grep 'Running'"
-      register: deployment_running
-      until: deployment_running|failed
-      retries: 20
-      delay: 30
-      ignore_errors: true
-      tags: verify
-
-    - include_tasks: verify_tasks.yml project_name={{ project_stage }} selector="comp-type!=database"
-      tags: verify
-
-    # verify other deployments in test project
-    - name: wait for other deployments in project {{ project_test }} to complete (succeed or fail)
-      shell: "oc get pods -n {{ project_test }} -l comp-type!=database | grep '\\-deploy\\s' | grep 'Running'"
-      register: deployment_running
-      until: deployment_running|failed
-      retries: 20
-      delay: 30
-      ignore_errors: true
-      tags: verify
-
-    - include_tasks: verify_tasks.yml project_name={{ project_test }} selector="comp-type!=database"
-      tags: verify
-
-    # verify other deployments in dev project
-    - name: wait for other deployments in project {{ project_dev }} to complete (succeed or fail)
-      shell: "oc get pods -n {{ project_dev }} -l comp-type!=database | grep '\\-deploy\\s' | grep 'Running'"
-      register: deployment_running
-      until: deployment_running|failed
-      retries: 20
-      delay: 30
-      ignore_errors: true
-      tags: verify
-
-    - include_tasks: verify_tasks.yml project_name={{ project_dev }} selector="comp-type!=database"
-      tags: verify
BREAKS HERE
-        name: "{{ ANSIBLE_REPO_PATH }}/roles/ansible-versionlock"
BREAKS HERE
-    balancer_members: ['taskotron01.qa.fedoraproject.org:80']
-    balancer_members: ['taskotron01.qa.fedoraproject.org:80']
-    balancer_members: ['resultsdb01.qa.fedoraproject.org:80']
-    balancer_members: ['resultsdb01.qa.fedoraproject.org:80']
-    balancer_members: ['resultsdb01.qa.fedoraproject.org:80']
-    balancer_members: ['resultsdb01.qa.fedoraproject.org:80']
BREAKS HERE
-  hosts: os-masters[0]:os-masters-stg[0]
-  hosts: os-masters[0]:os-masters-stg[0]
BREAKS HERE
-    name: gluster
-    name: gluster
-    name: gluster
-    name: gluster
BREAKS HERE
-  sudo: True
BREAKS HERE
-  until: install_packages | success
BREAKS HERE
-          master_api_key: "{{ galaxy_admin_api_key }}"
-    - role: galaxyprojectdotorg.galaxy-tools #The default tools are listed in the file files/artimed_tool_list.yaml
BREAKS HERE
-    - include: common/ensure-rabbitmq.yml
-      vhost_name: "{{ ceilometer_rabbitmq_vhost }}"
-      user_name: "{{ ceilometer_rabbitmq_userid }}"
-      user_password: "{{ ceilometer_rabbitmq_password }}"
BREAKS HERE
-      when: workarounds | bug('rhbz1278181')
-      when: workarounds | bug('rhbz1278181')
-      when: workarounds | bug('rhbz1272347')
BREAKS HERE
--- import_playbook: "/srv/web/infra/ansible/playbooks/include/virt-create.yml myhosts=bastion"
BREAKS HERE
-  - include: "{{ tasks }}/accelerate_prep.yml"
-  - rkhunter
BREAKS HERE
-   - { role: openvpn/client,
-       when: deployment_type == "prod", tags: ['openvpn_client'] }
BREAKS HERE
-        dest: "{{ local_bedrock_dir }}/database_backup"
BREAKS HERE
-nova_spice_console_keymap: "{{ nova_console_keymap }}"
-nova_novncproxy_vnc_keymap: "{{ nova_console_keymap }}"
-nova_console_keymap: en-us
BREAKS HERE
-- name: "MEDIUM | RHEL-07-041010 | PATCH | Wireless network adapters must be disabled."
-  command: nmcli radio wifi off
-  changed_when: no
BREAKS HERE
-          {%       set _ = systemd_network_networks.append({'interface': interface_name, 'address': (interface.ip_addr | default('10.1.0.1')), 'netmask': (interface.netmask | default('255.255.255.0'))}) %}
BREAKS HERE
-    - name: call restart splunk from role splunk_common
BREAKS HERE
-    - solr5
-    - { role: bie-index,        bie_index_version: "1.4.2.2" }
-    - demo
BREAKS HERE
-    - name: Create first resource group
-        name: "{{ resource_group }}"
-        location: eastus
-    - name: Create second resource group
-      azure_rm_resourcegroup:
-        name: "{{ resource_group_secondary }}"
-        - name: Delete a resource group
-          azure_rm_resourcegroup:
-            name: "{{ resource_group }}"
-            state: absent
-            force: True
-          async: 5000
-          poll: 0
-        - name: Delete second resource group
-            name: "{{ resource_group_secondary }}"
BREAKS HERE
-      template_base: "{{ ansible_user_dir }}/{{ source_dir | basename }}"
BREAKS HERE
-        playbooks_dir: /usr/local/loopabull-playbooks/,
BREAKS HERE
-    - ceph-common
BREAKS HERE
-      when: not is_metal | bool
BREAKS HERE
-  tasks:
-    - include_vars: roles/ceph-defaults/defaults/main.yml
-    - include_vars: group_vars/all.yml
BREAKS HERE
-    - nginxinc.nginx
BREAKS HERE
-- name: Gather facts from hosts
-  hosts: all:!localhost:!ovb
-  any_errors_fatal: true
-  gather_facts: no
-  tasks:
-      - block:
-          - name: Gather facts
-            include_vars:
-                file: "{{ inventory_dir }}/{{ inventory_hostname }}"
-        rescue:
-            - name: Reactivate unreachable hosts
-              meta: clear_host_errors
-
BREAKS HERE
-    postgresql_locale: 'en_US.UTF-8'
BREAKS HERE
-security_ntp_server_options: offline maxpoll 10 minpoll 8
BREAKS HERE
-    - role: databases/beanstalkd
-    - role: databases/mysql
-    - role: databases/mongodb
-    - role: databases/redis
-    - role: webservers/apache2
-    - role: webservers/nginx
-    - role: webservers/nodejs
-    - role: webservers/php5
BREAKS HERE
- - name: stop tor, if two servers are up, but only one should act as HS (for example jabber servers)
-   service:
-       name: tor
-       state: stopped
-   notify: stop tor
-   when: not hidden_service_active
BREAKS HERE
-### Extra options when configuring swift as a glance back-end.
-### By default it will use the local swift install
-### Set these when using a remote swift as a glance backend
BREAKS HERE
-    tags: release-montoring
BREAKS HERE
-    with_items: "{{ ceph_journal_partition_to_erase_path.stdout_lines }}"
BREAKS HERE
-        openshift_ansible_version: "openshift-ansible-3.27-1",
BREAKS HERE
-    block:
-      - name: Determine captaincy
-        set_fact:
-          splunk_search_head_captain: true
-        when:
-          - ansible_hostname == lookup('env', 'SPLUNK_SEARCH_HEAD_CAPTAIN_URL') or splunk.role == "splunk_search_head_captain"
-      - name: Execute pre-setup playbooks
-        include_tasks: execute_adhoc_plays.yml
-        vars:
-          playbook: "{{ ansible_pre_tasks }}"
-        when:
-          - ansible_pre_tasks is defined
-          - ansible_pre_tasks is not none
-          - ansible_pre_tasks is match("^(http|https|file)://.*")
-      - name: Provision role
-        include_role:
-          name: "{{ splunk.role }}"
-        when:
-          - splunk.role is defined
-      - name: Execute post-setup playbooks
-        include_tasks: execute_adhoc_plays.yml
-        vars:
-          playbook: "{{ ansible_post_tasks }}"
-        when:
-          - ansible_post_tasks is defined
-          - ansible_post_tasks is not none
-          - ansible_post_tasks is match("^(http|https|file)://.*")
-    become: yes
-    become_method: "{% if ansible_system is match('CYGWIN*|Win32NT') %}runas{% else %}sudo{% endif %}"
-    become_user: splunk
BREAKS HERE
-        lvol: lv={{ item.name }} size={{ item.size }} vg="{{ vg}}"
BREAKS HERE
-          'user': {{ }},
-          'identity_file': "{{ }}",
BREAKS HERE
-hosts: db-datanommer02.phx2.fedoraproject.org:db-qa01.qa.fedoraproject.org:db-koji01.phx2.fedoraproject.org:db-fas01.stg.phx2.fedoraproject.org:db-fas01.phx2.fedoraproject.org:db01.phx2.fedoraproject.org:db01.stg.phx2.fedoraproject.org:db-s390-koji01.qa.fedoraproject.org:db-arm-koji01.qa.fedoraproject.org:db-ppc-koji01.ppc.fedoraproject.org:db-qa-stg01.qa.fedoraproject.org
BREAKS HERE
-- debug: msg="{{ public_html_ls}}"
BREAKS HERE
-nova_requirements_git_repo: https://git.openstack.org/openstack/requirements
-nova_requirements_git_install_branch: master
-nova_lxd_requirements_git_repo: https://git.openstack.org/openstack/requirements
-nova_lxd_requirements_git_install_branch: master
BREAKS HERE
-        src: "./templates/reports/wan/iwan-documentation.j2"
-        dest: './reports/wan/devices/{{inventory_hostname}}-intent.md'
-        src: ./reports/wan/devices/
-        dest: './reports/wan/README.md'
BREAKS HERE
-    - name: create remote unprivileged remote user
-    - name: 'duplicate authorized_keys'
-    - name: remote unprivileged remote user
BREAKS HERE
-    pause: seconds=15 prompt="Waiting for BGP to start"
BREAKS HERE
-        when: enable_elk | bool }
-        when: enable_elk | bool }
BREAKS HERE
-          nic: "{{ ansible_default_ipv4 }}"
-          bridges: "{{ networks.values() | selectattr('forward', 'defined') | selectattr('forward', 'equalto', 'bridge') | list }}"
-          bridge_name: "{{ bridges[0].name if bridges else '' }}"
-      when: bridges|length
BREAKS HERE
-- name: list existing pool(s)
-  command: >
-    {{ docker_exec_cmd | default('') }} ceph --cluster {{ cluster }}
-    osd pool get {{ item.name }} size
-  with_items: "{{ pools }}"
-  register: created_pools
-  failed_when: false
-  delegate_to: "{{ delegated_node }}"
-- name: create ceph pool(s)
-  command: >
-    {{ docker_exec_cmd | default('') }} ceph --cluster {{ cluster }}
-    osd pool create {{ item.0.name }}
-    {{ item.0.pg_num }}
-    {{ item.0.pgp_num }}
-    {{ 'replicated_rule' if item.0.rule_name | default('replicated_rule') == '' else item.0.rule_name | default('replicated_rule') }}
-    {{ 1 if item.0.type|default(1) == 'replicated' else 3 if item.0.type|default(1) == 'erasure' else item.0.type|default(1) }}
-    {%- if (item.0.type | default("1") == '3' or item.0.type | default("1") == 'erasure') and item.0.erasure_profile != '' %}
-    {{ item.0.erasure_profile }}
-    {%- endif %}
-    {{ item.0.expected_num_objects | default('') }}
-  with_together:
-    - "{{ pools }}"
-    - "{{ created_pools.results }}"
-  changed_when: false
-  delegate_to: "{{ delegated_node }}"
-  when:
-    - pools | length > 0
-    - condition_copy_admin_key
-    - inventory_hostname in groups.get('_filtered_clients') | first
-    - item.1.rc != 0
BREAKS HERE
-        when: install['overcloud']['update']['kernel']
BREAKS HERE
-  hosts: blockerbugs:blockerbugs-stg
BREAKS HERE
-  - { role: nfs/client, when: datacenter == 'staging', mnt_dir: '/mnt/koji',  nfs_src_dir: '/mnt/koji' }
BREAKS HERE
-  tags: deployment-host 
-    - debug: msg="Manually copy over new config by running 'cd /etc/openstack_deploy/ && cp openstack_user_config.yml.example openstack_user_config.yml'" 
BREAKS HERE
-
-      - libselinux-python
-      - firewalld
-    - name: install packages
-    - name: stop firewalld
-      service:
-        name: firewalld
-        state: stopped
-      copy:
-        src: files/index.html
-        dest: /var/www/html/
-      notify: restart apache
-    - name: restart apache
BREAKS HERE
-      - "{{ ansible_ssh_user }}"
BREAKS HERE
-      shell: |
-        set -o pipefail
-        php -v | grep -F '{{ php_version }}'
BREAKS HERE
-  - name: install special fedorahosted-redirects.conf with fedorahosted redirects
-    copy: src={{ files }}/httpd/fedorahosted-redirects.conf dest=/etc/httpd/conf.d/fedorahosted.org/fedorahosted-redirects.conf
-  - name: install special git.fedorahosted-redirects.conf with letsencrypt info
-    copy: src={{ files }}/httpd/git.fedorahosted-redirects.conf dest=/etc/httpd/conf.d/git.fedorahosted.org/fedorahosted-redirects.conf
BREAKS HERE
-        api_version: v1
-        kind: Project
-        name: "{{ project_name }}"
-        resource_definition:
-          description: "{{ project_description|default(project_display_name)|default(project_name) }}"
-          display_name: "{{ project_display_name|default(project_name) }}"
BREAKS HERE
-   - { role: nfs/client, mnt_dir: '/srv/',  nfs_src_dir: 'fedora_taskotron_stg', when: deployment_type == 'stg' }
BREAKS HERE
-    - { role: php, use_php56: true, use_php5_fpm: false, use_xdebug: true, activate_xdebug: false }
BREAKS HERE
-    - name: Fail wrong python
-      tags: ['never', 'os', 'base']
-      tags: ['never', 'docker', 'base']
-      tags: ['never', 'firewall', 'base']
-      tags: ['never', 'ovpn', 'base']
-      tags: ['never', 'ssl', 'http']
-      tags: ['never', 'ssl', 'wildcard', 'http']
-
-      tags: ['never', 'nginx', 'system']
-      tags: ['never', 'netdata', 'system']
-      tags: ['never', 'clickhouse', 'system']
-      tags: ['never', 'chproxy', 'system']
-      tags: ['never', 'redis', 'system', 'docker', 'docker-container']
-      tags: ['never', 'logspout', 'system', 'docker', 'docker-container']
-      tags: ['never', 'anaconda', 'system', 'platform', 'docker', 'docker-container']
-      tags: ['never', 'grafana', 'platform', 'docker', 'docker-container']
-
-
-    ##### ##### ##### ##### #####    Splash screen    ##### ##### ##### ##### ##### 
-    - block:
-      - name: Including Static service role for Splash screen
-        include_role:
-          name: dr.static-service
-        vars:
-          dr_name: splash
-          dr_dir: '{{dirs.splash}}'
-          dr_repo: https://github.com/rockstat/splash.git
-      tags: ['splash', 'platform']
-      - name: Including Static service role for Dashboart
-      tags: ['dashboard', 'platform']
-      tags: ['band', 'platform', 'docker', 'docker-container']
-      tags: ['theia', 'platform', 'band', 'docker', 'docker-container']
-      tags: ['frontier', 'platform', 'rockme', 'docker', 'docker-container']
-      tags: ['chwriter', 'chmigrate', 'platform', 'rockme', 'docker', 'docker-container']
-      tags: ['heavyload', 'platform', 'docker', 'docker-container']
BREAKS HERE
-        grep -lE '\[global\]|fsid' /etc/ceph/*.conf
BREAKS HERE
-    - ceph-config
-    - { role: ceph-mgr, when: "ceph_release_num[ceph_release] >= ceph_release_num.luminous" }
BREAKS HERE
-  hosts: controller
BREAKS HERE
-      until: get_pip | success
-        - get_pip | failed
-      until: get_pip_fallback | success
BREAKS HERE
-          - Student Login  : http://{{ hostvars['docs'].ansible_host }}/users/
-          - Lab Slide Deck : http://{{ hostvars['docs'].ansible_host }}/decks/
BREAKS HERE
-# Deffered to 2.5
BREAKS HERE
-DEFAULT_ASK_SUDO_PASS:
-  name: Ask for the sudo password
-  default: False
-  deprecated:
-    why: In favor of Ansible Become, which is a generic framework. See become_ask_pass.
-    version: "2.9"
-    alternatives: become
-  description:
-    - This controls whether an Ansible playbook should prompt for a sudo password.
-  env: [{name: ANSIBLE_ASK_SUDO_PASS}]
-  ini:
-  - {key: ask_sudo_pass, section: defaults}
-  type: boolean
-DEFAULT_ASK_SU_PASS:
-  name: Ask for the su password
-  default: False
-  deprecated:
-    why: In favor of Ansible Become, which is a generic framework. See become_ask_pass.
-    version: "2.9"
-    alternatives: become
-  description:
-    - This controls whether an Ansible playbook should prompt for a su password.
-  env: [{name: ANSIBLE_ASK_SU_PASS}]
-  ini:
-  - {key: ask_su_pass, section: defaults}
-  type: boolean
-DEFAULT_MODULE_LANG:
-  name: Target language environment
-  default: "{{ CONTROLLER_LANG }}"
-  description:
-    - "Language locale setting to use for modules when they execute on the target."
-    - "If empty it tries to set itself to the LANG environment variable on the controller."
-    - "This is only used if DEFAULT_MODULE_SET_LOCALE is set to true"
-  env: [{name: ANSIBLE_MODULE_LANG}]
-  ini:
-  - {key: module_lang, section: defaults}
-  deprecated:
-    why: Modules are coded to set their own locale if needed for screenscraping
-    version: "2.9"
-DEFAULT_MODULE_SET_LOCALE:
-  name: Target locale
-  default: False
-  description:
-    - Controls if we set locale for modules when executing on the target.
-  env: [{name: ANSIBLE_MODULE_SET_LOCALE}]
-  ini:
-  - {key: module_set_locale, section: defaults}
-  type: boolean
-  deprecated:
-    why: Modules are coded to set their own locale if needed for screenscraping
-    version: "2.9"
-DEFAULT_SU_EXE:
-  name: su executable
-  default: su
-  deprecated:
-    why: In favor of Ansible Become, which is a generic framework. See become_exe.
-    version: "2.9"
-    alternatives: become
-  description: 'specify an "su" executable, otherwise it relies on PATH.'
-  env: [{name: ANSIBLE_SU_EXE}]
-  ini:
-  - {key: su_exe, section: defaults}
-DEFAULT_SU_FLAGS:
-  name: su flags
-  default: ''
-  deprecated:
-    why: In favor of Ansible Become, which is a generic framework. See become_flags.
-    version: "2.9"
-    alternatives: become
-  description: 'Flags to pass to su'
-  env: [{name: ANSIBLE_SU_FLAGS}]
-  ini:
-  - {key: su_flags, section: defaults}
-DEFAULT_SU_USER:
-  name: su user
-  default:
-  description: 'User you become when using "su", leaving it blank will use the default configured on the target (normally root)'
-  env: [{name: ANSIBLE_SU_USER}]
-  ini:
-  - {key: su_user, section: defaults}
-  deprecated:
-    why: In favor of Ansible Become, which is a generic framework. See become_user.
-    version: "2.9"
-    alternatives: become
BREAKS HERE
-- name: Ensure apt cache is up to date
-  apt:
-    update_cache: yes
-    cache_valid_time: "{{ cache_timeout }}"
-  when:
-    - ansible_pkg_mgr == 'apt'
-
BREAKS HERE
-          tool_dependency_dir: "{{ galaxy_data }}/lib/tool_shed/dependencies"
BREAKS HERE
-    images_upload: false
BREAKS HERE
-- name: Update aptitude cache
-    - apt: name=python-software-properties update_cache=yes
BREAKS HERE
-    - role: "openstack-ansible-security"
BREAKS HERE
-# TODO: whap sql script in a transaction
-    shell: psql koji < /var/lib/pgsql/koji-reset-staging.sql
BREAKS HERE
-# Current supported choice: qemu or kvm or ironic or powervm
BREAKS HERE
-        include_tasks: tasks/pre/main.yml
-        include_tasks: "tasks/setup/main.yml"
-        include_tasks: "tasks/configure/main.yml"
-        include_tasks: 'tasks/collect-ansible-facts.yml'
BREAKS HERE
-        - cloudformation_out|succeeded
-      when: not cloudformation_out|succeeded
-      when: not cloudformation_out|succeeded
-      when: rwait|failed
-      when: rwait|failed
-      until: rping | succeeded
-      when: rwait|failed
-      when: rwait|failed
BREAKS HERE
-- debug: var=shell_jenkins_config_misc
BREAKS HERE
-      when: env == "staging" or short_hostname == "proxy02" }
BREAKS HERE
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      vars:
-        user_name: "{{ designate_pool_manager_galera_user }}"
-        password: "{{ designate_pool_manager_galera_password }}"
-        login_host: "{{ designate_pool_manager_galera_address }}"
-        db_name: "{{ designate_pool_manager_galera_database_name }}"
-      when: inventory_hostname == groups['designate_all'][0]
BREAKS HERE
-
-  - debug: msg="{{ result.msg }}"
BREAKS HERE
-  - { role: ansible-role-sshd-host-keys, tags: [ 'sshd', 'ssh', 'host-keys' ] }
BREAKS HERE
-#- name: grab facts from production nodes
-#  hosts: production
-#  tasks: [ ]
-  hosts: io-nfs
-    #  - { role: ansible-role-nis, tags: [ 'nis' ] }
BREAKS HERE
-      register: ceph_conf
-    - name: stat ceph.conf
-        path: "{{ ceph_conf.stdout }}"
-        dest: "{{ ceph_conf.stdout }}"
BREAKS HERE
-   - { role: fedmsg/hub, tags: ['fedmsg_hub'] }
-# for now just stg while we're testing
-- name: set up openQA server data NFS mounts
-
-- name: set up openQA server data NFS mounts
-  hosts: openqa-stg
-  roles:
-# set up prod temp mount
-- name: set up openQA server data NFS mounts
-    mnt_dir: '/mnt/temp'
-    nfs_src_dir: 'fedora_openqa'
BREAKS HERE
-- name: ensure data directory exists
-    - demo
BREAKS HERE
-        - state: args_present
-          args:
-              - "sha512"
-        - state: args_absent
-          args:
-              - "md5"
-              - "bigcrypt"
-              - "sha256"
-              - "blowfish"
-              - use_authtok
-              - remember={{ rhel7stig_pam_pwhistory.remember|default(5) }}
-              - retry={{ rhel7stig_pam_pwhistory.retries|default(3) }}
-              # Might be an AWS RHEL instance, try it...
BREAKS HERE
-    - dovecot-solr
-- name: Create indexes and control directory
-  file:
-    path: "/var/vmail/{{ dir }}"
-    state: directory
-    owner: dovecot
-    group: users
-    mode: 0775
-  with_items:
-    - indexes
-    - control
-  loop_control:
-    loop_var: dir
-- name: Copy configuration template
-  register: config
-  template:
-    src: "{{ file }}"
-    dest: "/etc/dovecot/{{ file }}"
-  with_items:
-    - dovecot.conf
-    - dovecot-dict-auth.conf.ext
-    - dovecot-dict-sql.conf.ext
-    - dovecot-ldap.conf.ext
-    - dovecot-sql.conf.ext
-  loop_control:
-    loop_var: file
-- name: Copy configuration template
-  register: config
-  template:
-    src: "conf.d/{{ file }}"
-    dest: "/etc/dovecot/conf.d/{{ file }}"
-  with_items:
-    - 10-auth.conf
-    - 10-director.conf
-    - 10-logging.conf
-    - 10-logging.conf.ucf-dist
-    - 10-mail.conf
-    - 10-mail.conf.ucf-dist
-    - 10-master.conf
-    - 10-ssl.conf
-    - 10-tcpwrapper.conf
-    - 15-lda.conf
-    - 15-mailboxes.conf
-    - 15-mailboxes.conf.ucf-dist
-    - 20-imap.conf
-    - 20-imap.conf.ucf-dist
-    - 20-lmtp.conf
-    - 20-lmtp.conf.ucf-dist
-    - 20-managesieve.conf
-    - 20-pop3.conf
-    - 20-pop3.conf.ucf-dist
-    - 90-acl.conf
-    - 90-plugin.conf
-    - 90-quota.conf
-    - 90-sieve.conf
-    - 90-sieve.conf.ucf-dist
-    - 90-sieve-extprograms.conf
-    - auth-checkpassword.conf.ext
-    - auth-deny.conf.ext
-    - auth-dict.conf.ext
-    - auth-ldap.conf.ext
-    - auth-master.conf.ext
-    - auth-passwdfile.conf.ext
-    - auth-sql.conf.ext
-    - auth-static.conf.ext
-    - auth-system.conf.ext
-    - auth-vpopmail.conf.ext
-  loop_control:
-    loop_var: file
-- name: Restart dovecot
-  when: config.changed
-  service:
-    name: dovecot
-    state: restarted
BREAKS HERE
-- name: "Perform control machine setup"
-  hosts: localhost
-      when: "log"
-# This play encompasses all NAT routers and makes changes to the NAT config
-# on each device (assumine ci_test is false).
-- name: "Manage IOS NAT statements"
-  hosts: nat_routers
-  vars_files:
-    - "login_creds.vault"
-
-  # Perform some preliminary error checking on the static_nats list to
-  # ensure it is defined and non-empty. Perform the same checks on the vrf
-  # string.
-  tasks:
-          - "vrf == true or vrf == false"
-
-    # Referencing long variable names makes everything more complicated,
-    # so it is worth the memory tax to copy these into each host.
-    - name: "SYS >> Copy the vars to each host to simplify playbook"
-      listen: "updates exist"
-      set_fact:
-        LOG_PATH: "{{ hostvars.localhost.LOG_PATH }}"
-        DTG: "{{ hostvars.localhost.DTG }}"
-
-        newline_sequence: "\r\n"
BREAKS HERE
-    - name: restart ceph mons with upstart
-        state: restarted
-    - name: restart ceph mons with sysvinit
-        state: restarted
-    - name: restart ceph mons with systemd
-        state: restarted
BREAKS HERE
-  hosts: localhost
BREAKS HERE
-    - role: "{{ rolename | basename }}"
BREAKS HERE
-- name: Install dependencies for domain join 
-  package:
-    name:
-      - oddjob
-      - oddjob-mkhomedir
-      - openldap-clients
-      - realmd
-      - sssd
-      - samba-common-tools 
-      - authconfig
-    state: present
-
-### Add something to check if it is already in a domain? 
-
-- name: Install pexpect using pip
-  pip:
-    name: pexpect
-
-- name: Check if machine is bound
-  shell: "realm list | grep sssd"
-  register: realmd_bound
-  changed_when: false
-  ignore_errors: true
-
-- name: Join the Domain 
-  expect: 
-    command: "realm join -U Admin@{{ dns_domain_name }} {{ dns_domain_name | upper }}"
-    responses:
-      Password for *: "{{ domain_admin_password }}"
-  register: realm_join_cmd_result
-  #no_log: true         # Uncomment this when in prod... (clear text)
-  #changed_when: realm_join_cmd_result.rc != 100
-  #failed_when: realm_join_cmd_result.rc not in [0, 100]
-  when: realmd_bound is failed
-
-- name: Add ad group to sudoers
-  lineinfile:
-    dest: /etc/sudoers
-    line: "%Ansible\\ Users	ALL=(ALL)	NOPASSWD: ALL"
-    insertafter: "^%wheel"
-  when: realmd_bound is failed
-
-- name: Get Kerberos Ticket
-  command: echo -n "{{ domain_admin_password }}" | kinit Admin@{{ dns_domain_name | upper }}
-  #command: kinit -k -t /etc/krb5.keytab {{ ansible_hostname | upper }}$\@{{ adauth_realm }}
-  args:
-    creates: /tmp/krb5cc_0
-
-- name: Check if authconfig ran before
-  command: "/bin/egrep '^auth.*sufficient.*pam_sss.so' /etc/pam.d/system-auth"
-  register: authconfig_run
-  failed_when: False
-  changed_when: False
-
-- name: Configure Server to use SSSD for auth
-  command: /usr/sbin/authconfig {{ authconfig_options | join(" ") }}
-  when: authconfig_run.rc != 0
-
-- name: Configure SSSD to use AD for authentication
-  template: 
-    src: sssd.conf.j2 
-    dest: /etc/sssd/sssd.conf
-    owner: root 
-  notify: restart sssd
-- name: Ensure SSSD is started and enabled on boot
-  service: 
-    name: sssd 
-    state: started 
-    enabled: yes
-- name: Ensure oddjobd for mkhomedir is started and enabled on boot
-  service: 
-    name: oddjobd 
-    state: started 
-    enabled: yes
-
BREAKS HERE
-      vhost: "%2Fpubsub"
-      vhost: "%2Fpubsub"
BREAKS HERE
-    - name: Add user opentlc-mgr for OPENTLC CloudForms integration
-      shell: "useradd opentlc-mgr"
-      ignore_errors: true
-    - shell: "rm -rf /opt/ansible_agnostic_deployer"
-    - name: Get updated files from git repository github.com/sborenst/ansible_agnostic_deployer
-      git: repo=https://{{ githubuser }}:{{ githubpassword }}@github.com/sborenst/ansible_agnostic_deployer.git dest=/opt/ansible_agnostic_deployer/ force=yes
-      ignore_errors: true
-
-    - shell: "rm -rf /opt/OPEN_Admin"
-    - name: Get updated files from git repository github.com/redhat-gpe/OPEN_Admin.git
-      git: repo=https://{{ githubuser }}:{{ githubpassword }}@github.com/redhat-gpe/OPEN_Admin.git dest=/opt/OPEN_Admin/ force=yes
-      ignore_errors: true
-
-    - file: "path=/opt/OPEN_Admin/ state=directory owner=opentlc-mgr recurse=yes group=opentlc-mgr  mode=0770"
-    - file: "path=/opt/ansible_agnostic_deployer/ state=directory owner=opentlc-mgr recurse=yes group=opentlc-mgr  mode=0770"
-        src: ~/.ssh/{key_name}.pem
-        dest: /home/opentlc-mgr/.ssh/{key_name}.pem
-    - name: Allow Access to opentlc-mgr user
-      shell: |
-          mkdir /home/opentlc-mgr/bin;
-          ln -s /opt/OPEN_Admin/OPENTLC-OCP3/provision_workshop_env.sh /home/opentlc-mgr/bin/provision_workshop_env.sh
-          ln -s /opt/OPEN_Admin/OPENTLC-OCP3/provision-ose-projects.sh /home/opentlc-mgr/bin/provision-ose-projects.sh
-          ln -s /opt/OPEN_Admin/OPENTLC-OCP3/deploy_scripts /home/opentlc-mgr/bin/deploy_scripts
-          mkdir /home/opentlc-mgr/.ssh
-          echo 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC4OojwKH74UWVOY92y87Tb/b56CMJoWbz2gyEYsr3geOc2z/n1pXMwPfiC2KT7rALZFHofc+x6vfUi6px5uTm06jXa78S7UB3MX56U3RUd8XF3svkpDzql1gLRbPIgL1h0C7sWHfr0K2LG479i0nPt/X+tjfsAmT3nWj5PVMqSLFfKrOs6B7dzsqAcQPInYIM+Pqm/pXk+Tjc7cfExur2oMdzx1DnF9mJaj1XTnMsR81h5ciR2ogXUuns0r6+HmsHzdr1I1sDUtd/sEVu3STXUPR8oDbXBsb41O5ek6E9iacBJ327G3/1SWwuLoJsjZM0ize+iq3HpT1NqtOW6YBLR opentlc-mgr@inf00-mwl.opentlc.com' >> /home/opentlc-mgr/.ssh/authorized_keys
-          chown -R opentlc-mgr.opentlc-mgr /home/opentlc-mgr/.ssh
-          chmod 400 /home/opentlc-mgr/.ssh/authorized_keys
-          chmod 700 /home/opentlc-mgr/.ssh
-      shell: |
-          curl "https://s3.amazonaws.com/aws-cli/awscli-bundle.zip" -o "awscli-bundle.zip"
-          unzip awscli-bundle.zip
-          sudo ./awscli-bundle/install -i /usr/local/aws -b /bin/aws
-          aws --version
-      shell: |
-          mkdir /home/opentlc-mgr/.aws
-          echo '
-          aws_secret_access_key = {{aws_secret_access_key}}' > /home/opentlc-mgr/.aws/credentials
-          chown -R opentlc-mgr.opentlc-mgr /home/opentlc-mgr/.aws
-      shell: "cp /opt/ansible_agnostic_deployer/ansible/inventory/boto.cfg /etc/boto.cfg"
-      shell: "cp /opt/ansible_agnostic_deployer/ansible/configs/ansible-provisioner/files/Sync_Repositories.sh /root"
-    - shell: "mkdir {{ storage_mount_path }} &&  mount {{ storage_mount_path }}"
-    - shell: "systemctl enable http && systemctl restart httpd"
-    - shell: "ln -s {{storage_mount_path}}/repos /var/www/html/repos"
BREAKS HERE
-    - "../configs/{{ env_type }}/ssh_vars.yml"
BREAKS HERE
-  gather_facts: no
BREAKS HERE
-      include_vars:
-          file: "{{ install.hybrid }}"
-          name: bm_nodes
-      register: result_hybrid
-      when: result_hybrid|skipped
BREAKS HERE
-    - name: install cinch using pip
-    - name: install linch-pin from master branch instead of latest release from pypi
-        "{{ python }}" "{{ venv_dir }}/bin/pip install" -U
-        git+https://github.com/CentOS-PaaS-SIG/linch-pin.git@master
-      when: (linchpin_master|bool)
-    - name: install python-krbV with pip to use kerberos with Beaker
-        name: python-krbV
-
-    # work-around for https://github.com/CentOS-PaaS-SIG/linch-pin/issues/124
-    - name: generate linch-pin config file
-      command: "{{ python }} {{ venv_dir }}/bin/linchpin config --reset"
-      args:
-        chdir: "{{ temp_dir }}"
-      tags:
-        - skip_ansible_lint
-
-    # work-around for https://github.com/CentOS-PaaS-SIG/linch-pin/issues/124
-    - name: copy linch-pin config file to search path
-      copy:
-        src: "{{ temp_dir }}/linchpin_config.yml"
-        dest: "{{ venv_dir }}/lib/python2.7/site-packages/linchpin_config.yml"
BREAKS HERE
-            enablerepo: "{{ (ansible_distribution == 'RedHat') | ternary('rhel-7-server-optional-rpms','') }}"
BREAKS HERE
-  gather_facts: no
-
BREAKS HERE
-  docker:
-  docker:
BREAKS HERE
-- include_tasks: mq_setup.yml
-  with_items:
-    - oslomsg_setup_host: "{{ ironic_oslomsg_rpc_setup_host }}"
-      oslomsg_userid: "{{ ironic_oslomsg_rpc_userid }}"
-      oslomsg_password: "{{ ironic_oslomsg_rpc_password }}"
-      oslomsg_vhost: "{{ ironic_oslomsg_rpc_vhost }}"
-      oslomsg_transport: "{{ ironic_oslomsg_rpc_transport }}"
-    - oslomsg_setup_host: "{{ ironic_oslomsg_notify_setup_host }}"
-      oslomsg_userid: "{{ ironic_oslomsg_notify_userid }}"
-      oslomsg_password: "{{ ironic_oslomsg_notify_password }}"
-      oslomsg_vhost: "{{ ironic_oslomsg_notify_vhost }}"
-      oslomsg_transport: "{{ ironic_oslomsg_notify_transport }}"
-  no_log: true
BREAKS HERE
-      {{ neutron_developer_mode | ternary('--constraint /opt/developer-pip-constraints.txt', '') }}
-      {{ neutron_developer_mode | ternary('--constraint /opt/developer-pip-constraints.txt', '') }}
BREAKS HERE
-      intellij_default_maven_home: '/test/maven/home'
BREAKS HERE
-            - install.version|openstack_release >= 12
-            - (install.version|openstack_distribution) == 'OSP'
BREAKS HERE
-# Permit direct root logins
BREAKS HERE
-      if parted -s "/dev/{{ item }}" print | grep -sq boot; then
-      sgdisk -Z "/dev/{{ item }}"
-      dd if=/dev/zero of="/dev/{{ item }}" bs=1M count=200
BREAKS HERE
-- name: install congress components
-    - include: common-tasks/mysql-db-user.yml
-      vars:
-        user_name: "{{ congress_galera_user }}"
-        password: "{{ congress_container_mysql_password }}"
-        login_host: "{{ congress_galera_address }}"
-        db_name: "{{ congress_galera_database }}"
-      run_once: yes
BREAKS HERE
-- name: install the python34 fedmsg package
-  when: "'python34-fedmsg' in group_names"
BREAKS HERE
-            delay: 10
-            sleep: 3
-        retries: 6
-        delay: 30
BREAKS HERE
-         tags: "monitoring"
BREAKS HERE
-# TODO(evrardjp): Remove host_need_pip in the future
-# when the process building the repo is done before this step.
-- name: Configure containers default software, but don't run pip yet
-  vars:
-    host_need_pip: False
BREAKS HERE
-  - role: easyfix
BREAKS HERE
-    - file: dest={{ working_dir }}/hosts state=absent
BREAKS HERE
-      set_fact: mon_host_count={{ groups.mons | length }}
-      set_fact: mon_host_count={{ groups.mons | length }}
-      with_items: "{{ groups.mons }}"
-      with_items: "{{ groups.mons[0] }}"
-      delegate_to: "{{ groups.mons[0] }}"
-          docker exec ceph-osd-{{ hostvars[groups.mons[0]]['ansible_hostname'] }} ceph osd set {{ item }} --cluster {{ cluster }}
-      delegate_to: "{{ groups.mons[0] }}"
-      delegate_to: "{{ groups.mons[0] }}"
-        test "$(docker exec ceph-osd-{{ hostvars[groups.mons[0]]['ansible_hostname'] }} ceph pg stat --cluster {{ cluster }} | sed 's/^.*pgs://;s/active+clean.*//;s/ //')" -eq "$(docker exec {{ hostvars[groups.mons[0]]['ansible_hostname'] }} ceph pg stat --cluster {{ cluster }} | sed 's/pgs.*//;s/^.*://;s/ //')" && docker exec {{ hostvars[groups.mons[0]]['ansible_hostname'] }} ceph health --cluster {{ cluster }}  | egrep -sq "HEALTH_OK|HEALTH_WARN"
-      delegate_to: "{{ groups.mons[0] }}"
-      delegate_to: "{{ groups.mons[0] }}"
-          docker exec ceph-osd-{{ hostvars[groups.mons[0]]['ansible_hostname'] }} ceph osd unset {{ item }} --cluster {{ cluster }}
-      delegate_to: "{{ groups.mons[0] }}"
BREAKS HERE
-  pre_tasks:
-    - include: common-tasks/set-upper-constraints.yml
BREAKS HERE
-  # This is going to be way different
-  #- role: collectd/fedmsg-service
-  #  process: fedmsg-hub
BREAKS HERE
-    url: https://github.com/contiv/netplugin/releases/download/v0.1-02-06-2016.14-42-05.UTC/netplugin-v0.1-02-06-2016.14-42-05.UTC.tar.bz2
-    dest: /tmp/contivnet.tar.bz2
-  shell: tar vxjf /tmp/contivnet.tar.bz2
-    creates: netmaster
-  file: src=/usr/bin/contiv/netplugin/{{ item }} dest=/usr/bin/{{ item }} state=link
BREAKS HERE
-    dest: "/var/lib/resallocserver/provision/{{ item }}"
BREAKS HERE
-    - name: Create VM Disk Image
-      command: |
-        qemu-img create -f qcow2 /var/lib/libvirt/images/{{ hostvars[item]['server_hostname'] }}.img {{ default_vm_storage }}m
-        - hostvars[item]['server_vm'] | default(false) | bool
-      with_items: "{{ groups['pxe_servers'] }}"
BREAKS HERE
-      include: build_azure_nodes.yml
BREAKS HERE
-    - src: "keystone.Default.conf.j2"
-      dest: "{{ keystone_ldap_domain_config_dir }}/keystone.Default.conf"
-      config_overrides: "{{ keystone_keystone_default_conf_overrides }}"
-      config_type: "ini"
BREAKS HERE
--
-hosts: os-control-stg
-        openshift_app_subdomain: "{{ os_app_url }}"
-      when: env == 'staging',
-      tags: ['openshift-cluster','ansible-ansible-openshift-ansible']
BREAKS HERE
-    - name: Add iptables rule to ensure ssh checksum is correct
-      command: /sbin/iptables -A POSTROUTING -t mangle -p tcp --dport 22 -j CHECKSUM --checksum-fill
BREAKS HERE
-    file: path=/etc/httpd/conf.d/getfedora.org/prerelease-to-final-redirectmatch.conf state=absent
BREAKS HERE
-      validate: sshd -tf %s
-      validate: sshd -t -f %s
-      validate: sshd -t -f %s
-      validate: sshd -t -f %s
-      validate: sshd -t -f %s
-      validate: sshd -t -f %s
-      validate: sshd -t -f %s
-      validate: sshd -t -f %s
-      validate: sshd -t -f %s
-      validate: sshd -t -f %s
-      validate: sshd -t -f %s
-      validate: sshd -t -f %s
-      validate: sshd -t -f %s
-      validate: sshd -t -f %s
-      validate: sshd -t -f %s
BREAKS HERE
-    regex: /(.*)/workstation/prerelease.*$
-    target: https://getfedora.org/$1
-    regex: /(.*)/server/prerelease.*$
-    target: https://getfedora.org/$1
-    regex: /(.*)/atomic/prerelease.*$
-    target: https://getfedora.org/$1
BREAKS HERE
-      with_dict: "{{ provision.topology.nodes }}"
-          with_dict: "{{ provision.topology.nodes }}"
-          with_dict: "{{ provision.topology.nodes }}"
-          with_dict: "{{ provision.topology.nodes }}"
-      with_dict: "{{ provision.topology.nodes }}"
BREAKS HERE
-    {{ docker_exec_client_cmd | default('') }} ceph --cluster {{ cluster }}
-  delegate_to: "{{ groups.get(client_group_name)[0] }}"
-    - copy_admin_key
-    {{ docker_exec_client_cmd | default('') }} ceph --cluster {{ cluster }}
-  delegate_to: "{{ groups.get(client_group_name)[0] }}"
-    - copy_admin_key
BREAKS HERE
-- include: update-proxy-dns.yml status=disable proxies=myvms_new:&proxies
-- include: update-proxy-dns.yml status=enable proxies=myvms_new:&proxies
BREAKS HERE
-          export HEAT_INCLUDE_PASSWORD=1;
BREAKS HERE
-- name: No provisioning needed - using manual
BREAKS HERE
-# run ansible-playbook playbook/default.yml -t nexus_restore -e "nexus_restore_point=(# date of choice -> %y-%m-%d #)"
-  - name: 'blob-raw'
-    path: "{{ nexus_data_dir }}/blobs/blob-raw"
-  - name: 'blob-pypi'
-    path: "{{ nexus_data_dir }}/blobs/blob-pypi"
-  - name: 'blob-docker'
-    path: "{{ nexus_data_dir }}/blobs/blob-docker"
-  - name: 'blob-ruby'
-    path: "{{ nexus_data_dir }}/blobs/blob-ruby"
-  - name: 'blob-bower'
-    path: "{{ nexus_data_dir }}/blobs/blob-bower"
-  - name: 'blob-npm'
-    path: "{{ nexus_data_dir }}/blobs/blob-npm"
-  - name: 'blob-mvn'
-    path: "{{ nexus_data_dir }}/blobs/blob-mvn"
-    write_policy: allow_once
-  - name: db-backup
-#  - name: blob-backup
-#    cron: '0 0 22 * * ?'
-#    typeId: script
-#    taskProperties:
-#      language: bash
-#      source: "{{ nexus_script_dir }}/nexus-blob-backup.sh"
-  blob_store: blob-mvn # Note : cannot be updated once the repo has been created
-  write_policy: allow_once # allow_once or allow
-  blob_store: blob-pypi # Note : cannot be updated once the repo has been created
-  write_policy: allow_once # allow_once or allow
-    write_policy: allow_once
-  blob_store: blob-raw # Note : cannot be updated once the repo has been created
-  write_policy: allow_once # allow_once or allow
-  - name: jenkins-mirror
-    remote_url: 'http://mirrors.jenkins-ci.org/'
-  - name: jenkins-updates
-    remote_url: 'http://updates.jenkins-ci.org/'
-  - name: ubuntu-security
-    remote_url: 'http://security.ubuntu.com/ubuntu'
-  - name: ubuntu-openstack
-    remote_url: 'http://ubuntu-cloud.archive.canonical.com/ubuntu'
-  - name: ubuntu-galera
-    remote_url: 'http://nyc2.mirrors.digitalocean.com/mariadb/repo/10.1/ubuntu'
-  - name: ubuntu-mongodb
-    remote_url: 'http://repo.mongodb.org/apt/ubuntu'
-      - jenkins-mirror
-      - jenkins-updates
-      - ubuntu-security
-      - ubuntu-openstack
-      - ubuntu-galera
-      - ubuntu-mongodb
-  blob_store: blob-docker # Note : cannot be updated once the repo has been created
-  write_policy: allow_once # allow_once or allow
-  blob_store: blob-ruby # Note : cannot be updated once the repo has been created
-  write_policy: allow_once # allow_once or allow
-    blob_store: blob-ruby
-    blob_store: blob-ruby
-    blob_store: blob-ruby
-  blob_store: blob-bower # Note : cannot be updated once the repo has been created
-  write_policy: allow_once # allow_once or allow
-    blob_store: blob-bower
-    blob_store: blob-bower
-    blob_store: default
-  blob_store: blob-npm # Note : cannot be updated once the repo has been created
-  write_policy: allow_once # allow_once or allow
-    blob_store: blob-npm
-    blob_store: blob-npm
-    blob_store: blob-npm
BREAKS HERE
-          {%-   set name = override | regex_replace('(>=|<=|>|<|==|~=|!=).*$','') %}
-          {%-   set name = pin | regex_replace('(>=|<=|>|<|==|~=|!=).*$','') %}
-        - item | regex_replace('(>=|<=|>|<|==|~=|!=).*$','') not in upper_constraints_override_list
-        - item | regex_replace('(>=|<=|>|<|==|~=|!=).*$','') not in global_pins_list
-        - item | regex_replace('(>=|<=|>|<|==|~=|!=).*$','') not in upper_constraints_override_list
BREAKS HERE
-    - vars/config.yml
-
-          image: rockstat/grafana-image
-          repo: "{{git_org}}/theia-image.git"
BREAKS HERE
-        name: "{{ item }}"
-      with_items: "{{ utility_pip_packages }}"
BREAKS HERE
-  hosts: control_machine
BREAKS HERE
-  tasks:
-    - name: Set Configuration File Facts
-      set_fact:
-        haproxy_cfg_tmp_file: "/tmp/haproxy-{{ inventory_hostname }}.cfg"
-    - name: Open Allow Redis Port
-      seport:
-        ports: 6379
-        proto: tcp
-        setype: http_port_t
-        state: present
-    - name: Create HAProxy Template
-      template:
-        src: "haproxy.cfg.j2"
-        dest: "{{ haproxy_cfg_tmp_file }}"
-    - name: Fetch HAProxy file
-      fetch:
-        src: "{{ haproxy_cfg_tmp_file }}"
-        dest: "{{ haproxy_cfg_tmp_file }}"
-        flat: yes
-      delegate_to: "{{ groups['lb'][0] }}"
-    - name: Delete HAProxy Configuration File from Remote
-      file:
-        state: absent
-        path: "{{ haproxy_cfg_tmp_file }}"
-    - name: Install HAProxy and Activate configuration
-      block:
-      - include_role:
-          name: load-balancers/manage-haproxy
-          tasks_from: install
-      - include_role:
-          name: load-balancers/manage-haproxy
-          tasks_from: activate-config
-        vars:
-          haproxy_temp_file: "{{ haproxy_cfg_tmp_file }}"
-    - name: 'Open Additional HAProxy Ports'
-      firewalld:
-        port: "{{item}}"
-        permanent: yes
-        state: enabled
-        immediate: yes
-      with_items:
-        - 8080/tcp
-        - 6379/tcp
BREAKS HERE
-  hosts:
-      - controller
-      - network
-      - compute
BREAKS HERE
-  - service: name={{ item }} state=stopped
-  - service: name=httpd state=stopped
BREAKS HERE
-        changed_when: no
-      - name: "MEDIUM | RHEL-07-010260 | AUDIT | Reset password timeout to prevent locking out user."
BREAKS HERE
-          koji_hub: 'https://koji.stg.fedoraproject.org/kojihub',
BREAKS HERE
-#note there is a typo in the fix steps in the STIG
-- name: "MEDIUM | RHEL-07-010180 | PATCH | When passwords are changed the number of repeating consecutive characters must not be more than four characters."
-      line: maxrepeat = 4
-- name: "MEDIUM | RHEL-07-040160 | PATCH | All network connections associated with a communication session must be terminated at the  the session or after 10 minutes of inactivity from the user at a command prompt, except to fulfill documented and validated mission requirements."
-  lineinfile:
-      create: yes
-      dest: /etc/profile
-      regexp: ^#?TMOUT
-      line: TMOUT=600
BREAKS HERE
-# The playbook aims to takeover a cluster that was not configured with 
-  vars_files:
-    - roles/ceph-defaults/defaults/main.yml
-    - group_vars/all.yml
BREAKS HERE
-  hosts: controller
BREAKS HERE
-        name: "{{ molecule_file | molecule_instance_with_scenario_name(item.name) }}"
BREAKS HERE
-    - name: Restart Tomcat
-        name: tomcat
BREAKS HERE
-        dest: "{{ tempest.dir}}/with_venv"
-    - command: "{{ tempest.dir }}/with_venv /root/configure-tempest.sh"
-    - shell: "{{ tempest.dir }}/with_venv testr init"
-        chdir: "{{ tempest.dir }}"
-        creates: "{{ tempest.dir }}/.testrepository"
BREAKS HERE
-        - ansible_pkg_mgr == 'yum'
BREAKS HERE
-    - raw: 'type python >/dev/null 2>&1 || apt-get -y install python-minimal'
-    - setup:
-    - shell: sleep 2 && shutdown -r now "Restart for OpenVPN install"
BREAKS HERE
-      command: grep "lxc.mount.entry = /openstack/container1 opt/test1 none bind 0 0" /var/lib/lxc/container1/config
-      command: grep "lxc.mount.entry = {{ development_repo_directory }} {{ development_repo_directory | relpath('/') }} none bind 0 0" /var/lib/lxc/container2/config
BREAKS HERE
-when: ansible_os_family == 'Debian'
BREAKS HERE
-  run_once: true
-  when: containerized_deployment
-  run_once: true
-    - inventory_hostname in groups.get(client_group_name) | first
-  run_once: true
-    - inventory_hostname in groups.get(client_group_name) | first
-  run_once: true
-  run_once: true
-    - "{{ slurp_client_keys.results }}"
-    - not inventory_hostname == groups.get(client_group_name, []) | first
BREAKS HERE
-    - name: Install project pip dependencies
-      pip:
-        requirements: "{{ playbook_dir }}/requirements.txt"
-    - name: Install project pip development dependencies
-      pip:
-        requirements: "{{ playbook_dir }}/requirements-dev.txt"
-    - name: Find where pdb is installed
-      shell: "echo $(pip show pdb | grep Location: | sed 's/Location: //')/pdb/pdb.py"
-      changed_when: False
-      register: pdb_path
-    - debug:
-        msg: "Example: ./.tmp/ansible/hacking/test-module -m modules/katello_content_view.py -a @test/data/content-view.json -D {{ pdb_path.stdout }}"
BREAKS HERE
-      delay: "{{ provision.wait | int }}"
BREAKS HERE
-    with_items: "{{ k8s_network_addons_urls | default ([]) }}"
-    when: k8s_network_addons_urls is defined
BREAKS HERE
-      inventory_hostname == groups['ironic_conductor'][0]
-  - include: ironic_conductor_post_install.yml
-      inventory_hostname == groups['ironic_conductor'][0]
BREAKS HERE
-      shell: mktemp -d "${TMPDIR:-/tmp}/ansible_test.XXXXXXXXX"
BREAKS HERE
-  hosts: "{{ tester.host }}"
BREAKS HERE
-        owner: 1000
-        group: 1000
BREAKS HERE
-        when: overcloud_deploy.rc != 0
BREAKS HERE
-- name: Install Zabbix Agent on all hosts
-  hosts: "{{ ('tag_Project_' ~ env_type ~ '_' ~ guid) | replace('-', '_') }}"
BREAKS HERE
-  - { role: nfs/client, when: inventory_hostname.startswith('aarch64') , mnt_dir: '/mnt/fedora_koji',  nfs_src_dir: 'fedora_arm' }
BREAKS HERE
-- name: Set constraint file fact for developer mode
-    pip_install_options: "{{ pip_install_options|default('') }} --constraint /opt/developer-pip-constraints.txt"
BREAKS HERE
-          failed_when: ( ether1_comment | changed )
-          name:     pool1
-          ranges:   102.3.4.5
-        failed_when: not ( dhcp_server_test_1_edit | changed )
-        failed_when: ( dhcp_server_test_1_duplicate|changed )
-        failed_when: not ( dhcp_server_test_2 | changed )
-        failed_when: not ( dhcp_server_test_2_rem | changed )
-        failed_when: not ( dhcp_network_mod | changed )
-        failed_when: not ( dhcp_network_rem | changed )
-          failed_when: ( nat_idem | changed )
-          failed_when: not ( nat_change | changed )
-          failed_when: not ( nat_rem | changed )
BREAKS HERE
-    - vars/bootstrap-aio-vars.yml
BREAKS HERE
-        rsync -av {{ base_dir }}/khaleesi/collected_files/ {{ job.rsync_path }}/$BUILD_TAG
BREAKS HERE
-    managed_by: "ocp-etcd-{{ item }}"
BREAKS HERE
-    server_aliases [phab.qa.stg.fedoraproject.org, docs.qa.stg.fedoraproject.org]
BREAKS HERE
-  - {role: httpd/website, vars: {name: torrent.fedoraproject.org, cert_name: "{{wildcard_cert_name}}", sslonly: true}}
BREAKS HERE
-    es_version: "5.1.2"
-    es_version: "5.2.2"
BREAKS HERE
-# A list of files may be copied into the container image cache during its preparation.
BREAKS HERE
-  - name: Add host to inventory
-    add_host:
-      name: "{{item.name}}"
-    with_items: "{{guests}}"
-    tasks:
-    - name: Remove old known_hosts
-      file:
-        path: /root/.ssh/known_hosts
-        state: absent
BREAKS HERE
-      - name: "FOO"
-        debug:
-            msg: "fan brod {{ 'Liberty' | openstack_release }}"
-
BREAKS HERE
-
-  - name: Create configuration directory
-    local_action: file path={{configs}} state=directory
-    run_once: true
-
-  - name: Create configurations
-    template: src=ospf-config.j2 dest={{configs}}/{{inventory_hostname}}.ospf.cfg
-    tags: [ configs ]
-
-  - name: Deploy configurations
-    ios_config:
-      provider: "{{ios_provider}}"
-      src: "{{configs}}/{{inventory_hostname}}.ospf.cfg"
-    register: changes
-
-  - name: Wait for OSPF to start
-    pause: seconds=15 prompt="Waiting for OSPF to start"
-    tags: [ verify ]
-
-  - name: Collect OSPF neighbors
-    ios_command:
-      provider: "{{ios_provider}}"
-      commands: 
-      - "show ip ospf neighbor | include ^[1-9]"
-    register: ospf_neighbors
-    tags: [ verify ]
-
-  - name: Verify OSPF is running on all internal interfaces
-    assert:
-      that: "'{{ item.key }}' in ospf_neighbors.stdout[0]"
-      msg:  "OSPF session in interface {{item.key}} is missing"
-    with_dict: "{{ nodes[inventory_hostname].internal }}"
BREAKS HERE
-  command: "true"
-  changed_when: no
-  when: rhel_07_021030
-      - notimplemented
BREAKS HERE
-  max_fail_percentage: 20
-    - { role: "rabbitmq_server", tags: [ "rabbitmq-server" ] }
BREAKS HERE
-- name: Run nova-status
-    # Only run nova-status on controller nodes.
BREAKS HERE
-    ubuntu1604_selinux_disable: true
-      changed_when: False
-      changed_when: False
-      changed_when: False
BREAKS HERE
-          autostart: "{{ item.value.autostart | default('yes') }}"
-          autostart: yes
BREAKS HERE
-  - name: Create repo file, if necessary
-    file: path=/etc/yum.repos.d/{{item.name}} state=touch
-    with_items: repolist
-    when: repolist is defined and repolist|length > 0
-
-  - name: Enabling yum repos
BREAKS HERE
-  - name: Setup the http listener to your machines
-    template:
-      src: roles/nginx/files/default.template
-      dest: /etc/nginx/conf.d/default.conf
-    register: nginx_cust_config
-  - name: Restart nginx to reflect changes
-    systemd:
-      state: restarted
-      daemon_reload: yes
-      name: nginx
-    when: nginx_cust_config.changed
-  - name: Set (httpd_can_network_connect) flag on and keep it persistent across reboots
-    seboolean:
-      name: httpd_can_network_connect
-      state: yes
-      persistent: yes
BREAKS HERE
-  when: ansible_kernel | version_compare(openstack_host_required_kernel, '<')
-  when: openstack_host_specific_kernel_modules | length > 0
-  when: openstack_host_sysstat_enabled | bool
-    - journald_directory | changed
BREAKS HERE
-      lxc_container_caches:
-        - url: "https://rpc-repo.rackspace.com/container_images/rpc-trusty-container.tgz"
-          name: "trusty.tgz"
-          sha256sum: "56c6a6e132ea7d10be2f3e8104f47136ccf408b30e362133f0dc4a0a9adb4d0c"
-          chroot_path: trusty/rootfs-amd64
BREAKS HERE
-    - ansible_pkg_mgr in ['yum', 'zypper']
BREAKS HERE
-- import_playbook: "/srv/web/infra/ansible/playbooks/include/virt-create.yml myhosts=bastion"
BREAKS HERE
-  when: neutron_get_venv | bool
BREAKS HERE
-      go get github.com/prometheus/client_golang/examples/random
BREAKS HERE
-    - name: PGBlitz
-
BREAKS HERE
-
-        shell: rhos-release -r {{ distro.latest_version }} {{ distro.name }}
BREAKS HERE
-
-## Ceilometer Options
-ceilometer_db_type: mongodb
-ceilometer_db_ip: localhost
-ceilometer_db_port: 27017
-swift_ceilometer_enabled: False
-heat_ceilometer_enabled: False
-cinder_ceilometer_enabled: False
-glance_ceilometer_enabled: False
-nova_ceilometer_enabled: False
-neutron_ceilometer_enabled: False
-keystone_ceilometer_enabled: False
-
-## Aodh Options
-aodh_db_type: mongodb
-aodh_db_ip: localhost
-aodh_db_port: 27017
-
-## Glance Options
-# installed on glance
-glance_default_store: file
-glance_notification_driver: noop
-
-# `internalURL` will cause glance to speak to swift via ServiceNet, use
-# `publicURL` to communicate with swift over the public network
-glance_swift_store_endpoint_type: internalURL
-# Ceph client user for glance to connect to the ceph cluster
-#glance_ceph_client: glance
-# Ceph pool name for Glance to use
-#glance_rbd_store_pool: images
-#glance_rbd_store_chunk_size: 8
-## Nova
-#nova_libvirt_images_rbd_pool: vms
-# by default we assume you use rbd for both cinder and nova, and as libvirt
-# needs to access both volumes (cinder) and boot disks (nova) we default to
-# reuse the cinder_ceph_client
-# only need to change this if you'd use ceph for boot disks and not for volumes
-#nova_ceph_client:
-#nova_ceph_client_uuid:
-
-# This defaults to KVM, if you are deploying on a host that is not KVM capable
-# change this to your hypervisor type: IE "qemu", "lxc".
-# nova_virt_type: kvm
-# nova_cpu_allocation_ratio: 2.0
-# nova_ram_allocation_ratio: 1.0
-# dhcp_domain:
-## Glance with Swift
-# Extra options when configuring swift as a glance back-end. By default it
-# will use the local swift installation. Set these when using a remote swift
-# as a glance backend.
-#glance_swift_store_auth_version: 3
-#glance_swift_store_auth_address: "https://some.auth.url.com"
-#glance_swift_store_user: "OPENSTACK_TENANT_ID:OPENSTACK_USER_NAME"
-#glance_swift_store_key: "OPENSTACK_USER_PASSWORD"
-#glance_swift_store_container: "NAME_OF_SWIFT_CONTAINER"
-#glance_swift_store_region: "NAME_OF_REGION"
-
-## Cinder
-# Ceph client user for cinder to connect to the ceph cluster
-#cinder_ceph_client: cinder
-
-## Ceph
-# Enable these if you use ceph rbd for at least one component (glance, cinder, nova)
-#ceph_apt_repo_url_region: "www"  # or "eu" for Netherlands based mirror
-#ceph_stable_release: hammer
-# Ceph Authentication - by default cephx is true
-#cephx: true
-# Ceph Monitors
-# A list of the IP addresses for your Ceph monitors
-#ceph_mons:
-#  - 10.16.5.40
-#  - 10.16.5.41
-#  - 10.16.5.42
-# Custom Ceph Configuration File (ceph.conf)
-## SSL Settings
-# Adjust these settings to change how SSL connectivity is configured for
-# various services.  For more information, see the openstack-ansible
-# documentation section titled "Securing services with SSL certificates".
-#
-## SSL: Keystone
-# These do not need to be configured unless you're creating certificates for
-# services running behind Apache (currently, Horizon and Keystone).
-ssl_protocol: "ALL -SSLv2 -SSLv3"
-# Cipher suite string from https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/
-ssl_cipher_suite: "ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS"
-# To override for Keystone only:
-#   - keystone_ssl_protocol
-#   - keystone_ssl_cipher_suite
-# To override for Horizon only:
-#   - horizon_ssl_protocol
-#   - horizon_ssl_cipher_suite
-#
-## SSL: RabbitMQ
-# Set these variables if you prefer to use existing SSL certificates, keys and
-# CA certificates with the RabbitMQ SSL/TLS Listener
-#
-#rabbitmq_user_ssl_cert: <path to cert on ansible deployment host>
-#rabbitmq_user_ssl_key: <path to cert on ansible deployment host>
-#rabbitmq_user_ssl_ca_cert: <path to cert on ansible deployment host>
-#
-#rabbitmq_use_ssl: true
-## Multiple region support in Horizon:
-# For multiple regions uncomment this configuration, and
-# add the extra endpoints below the first list item.
-# horizon_available_regions:
-#   - { url: "{{ keystone_service_internalurl }}", name: "{{ keystone_service_region }}" }
-#   - { url: "http://cluster1.example.com:5000/v2.0", name: "RegionTwo" }
-
-#haproxy_use_keepalived: False
-haproxy_keepalived_external_vip_cidr: "{{external_lb_vip_address}}/32"
-haproxy_keepalived_internal_vip_cidr: "{{internal_lb_vip_address}}/32"
-#haproxy_keepalived_external_interface:
-#haproxy_keepalived_internal_interface:
-#haproxy_keepalived_external_virtual_router_id:
-#haproxy_keepalived_internal_virtual_router_id:
-#haproxy_keepalived_priority_master:
-#haproxy_keepalived_priority_backup:
-#haproxy_keepalived_vars_file: 'vars/configs/keepalived_haproxy.yml'
-# default, but it can be applied to all hosts by adjusting the following
-#
-apply_security_hardening: false
BREAKS HERE
-              haproxy_balance_type: http
-              haproxy_backend_options:
-                - "forwardfor"
-                - "httpchk"
-                - "httplog"
BREAKS HERE
-- group: name=www
-- user: name=www group=www
-- name: create /srv/libravatar
-  file:
-    path: /srv/libravatar
-    setype: httpd_sys_content_t
-    state: directory
-
-- git:
-    repo: https://git.linux-kernel.at/oliver/ivatar.git
-    dest: /srv/libravatar
-    version: "{{ git_branch }}"
-  ignore_errors: yes
-- name: So that Apache can talk to PostgreSQL
-    name: httpd_can_network_connect_db
-- name: So Apache can execute psyco shared lib in virtualenv with httpd_sys_content_t type
BREAKS HERE
-      when: "{{ groups['keystone_all'] | length > 1 }}"
-      when: "{{ groups['keystone_all'] | length > 1 }}"
BREAKS HERE
-    - name: Set cinder storage bridge (is_metal)
-      set_fact:
-        storage_bridge: "{{ 'ansible_' + hostvars[inventory_hostname]['container_networks']['storage_address']['bridge'] | replace('-', '_') }}"
-      when:
-        - hostvars[inventory_hostname]['container_networks']['storage_address']['bridge'] is defined
-        - is_metal | bool
-      tags:
-        - always
-    - name: Set cinder storage address (is_metal)
-      set_fact:
-        storage_address: "{{ hostvars[inventory_hostname][storage_bridge]['ipv4']['address'] }}"
-      when:
-        - hostvars[inventory_hostname]['container_networks']['storage_address']['bridge'] is defined
-        - is_metal | bool
-      tags:
-        - always
-    - name: Set cinder storage bridge (is_metal no storage network)
-      set_fact:
-        storage_address: "{{ ansible_host }}"
-      when:
-        - hostvars[inventory_hostname]['container_networks']['storage_address']['bridge'] is undefined
-        - is_metal | bool
-      tags:
-        - always
-    - name: Set cinder storage address (container)
-      set_fact:
-        storage_address: "{{ hostvars[inventory_hostname]['container_networks']['storage_address']['address'] }}"
-      when:
-        - hostvars[inventory_hostname]['container_networks']['storage_address']['address'] is defined
-        - not is_metal | bool
-      tags:
-        - always
-    - name: Set cinder storage address (container no storage network)
-      set_fact:
-        storage_address: "{{ ansible_host }}"
-      when:
-        - hostvars[inventory_hostname]['container_networks']['storage_address']['address'] is undefined
-        - not is_metal | bool
-      tags:
-        - always
BREAKS HERE
-      zap: True
-      state: absent
BREAKS HERE
-  gather_facts: yes
-
BREAKS HERE
-  hosts: etcd_all
BREAKS HERE
-# Note that the placement API service does not understand how to reload,
-# so it fails when you try to make it do so. We therefore skip the reload
-# for that service.
-- name: Reload all nova services to ensure new RPC object version is used
-  hosts: "nova_all:!nova_api_placement"
-        service_negate: "{{ ['nova-placement-api.service'] + nova_service_negate | default([]) }}"
-# Note that the placement API service does not understand how to reload,
-# so it fails when you try to make it do so. We therefore restart it instead.
-- name: Restart the nova placement API service to ensure new RPC object version is used
-  hosts: "nova_api_placement"
-        service_name: "nova-placement-api"
BREAKS HERE
-        region: ap-southeast-1
-        zone: ap-southeast-1b
-        key_name: ashrafawskey
-          role: clustercontrol
-      register: clustercontrol_ec2
-        region: ap-southeast-1
-        zone: ap-southeast-1b
-        key_name: ashrafawskey
-          role: galeracluster
-      register: galeracluster_ec2
BREAKS HERE
-  ### Three entries for taskotron for production
-    proxyurl: http://resultsdb-01.qa.fedoraproject.org
BREAKS HERE
-        haproxy_backend: "{{ backend_name }}"
-      loop_control:
-        loop_var: backend_name
-      with_items:
-        - "keystone_service-back"
-        - "keystone_admin-back"
-    # todo(cloudnull): this task is being run only if/when keystone is installed on a physical host.
-    #  This is not being run within a container because it is an unsupported action due to this
-    #  issue: (https://bugs.launchpad.net/ubuntu/+source/lxc/+bug/1279041)
-    #  This issue was resolved however we'll need to eval it in the next LTS release.
-    #  Related OSA Bug: https://launchpad.net/bugs/1426371
-    - name: Add keystone reserved port to physical host
-      sysctl:
-        name: "{{ item.key }}"
-        value: "{{ item.value }}"
-        sysctl_set: "{{ item.set|default('yes') }}"
-        state: "{{ item.state|default('present') }}"
-        reload: "{{ item.reload|default('yes') }}"
-      with_items:
-        - { key: "net.ipv4.ip_local_reserved_ports", value: "{{ keystone_admin_port }}"}
-      when: is_metal | bool
-
-        haproxy_backend: "{{ item }}"
-      with_items:
-        - "keystone_service-back"
-        - "keystone_admin-back"
BREAKS HERE
-        shell: source ~/{{ tester.venv_dir }}/bin/activate && nosetests -v --with-xunit --xunit-file=horizon.xml openstack_dashboard/test/integration_tests/tests
BREAKS HERE
-  - ls /etc/nginx/auth_basic/
-  - ls /etc/nginx/conf.d/
-  - ls /etc/nginx/sites-enabled/
BREAKS HERE
-    name: koji.stg..fedoraproject.org
-    - koji.stg..stg.fedoraproject.org
BREAKS HERE
-      - default_conf: "{%- if install.version <= 13 -%}
-                /usr/share/instack-undercloud/undercloud.conf.sample
-                {%- else -%}
-                /usr/share/python-tripleoclient/undercloud.conf.sample
-                {%- endif -%}"
BREAKS HERE
-    - compute
-    - manila-share
-    - neutron-server
BREAKS HERE
-    - shell: rm -rf /tmp//slurm /etc/munge/munge.key
-    #only for ifb (should be removed later)
-    - shell: umount /dev/vdc
-      ignore_errors: yes
-    - shell: mount /dev/vdc /home
-      ignore_errors: yes
-    - shell: rm -rf /home/galaxy
BREAKS HERE
-      static: no
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      vars:
-        user_name: "{{ octavia_galera_user }}"
-        password: "{{ octavia_container_mysql_password }}"
-        login_host: "{{ octavia_galera_address }}"
-        db_name: "{{ octavia_galera_database }}"
-      when: inventory_hostname == groups['octavia_all'][0]
-
-  vars:
-    octavia_galera_user: octavia
-    octavia_galera_database: octavia
-    octavia_galera_address: "{{ galera_address }}"
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - octavia
BREAKS HERE
-- name: Install JRE on Ubuntu 16.04
-    shell: java -jar "/home/{{ admin_username }}/helloworld.jar" 2>&1 &
BREAKS HERE
-          {%-   if (not role.src | match(".*git.openstack.org.*")) or (role.name | match("os_previous_.*")) %}
BREAKS HERE
-      script: files/test_script.ps1
BREAKS HERE
-    server_hardware_name: '0000A66102, bay 12'
-
-
-            hostname : '{{ server_hardware_hostname }}'
-
-            hostname : '{{ server_hardware_hostname }}'
-
-            hostname : '{{ server_hardware_hostname }}'
-
-            hostname : '{{ server_hardware_hostname }}'
-
-            hostname: '{{ server_hardware_name }}'
-
-            hostname: '{{ server_hardware_name }}'
-
-            hostname : '{{ server_hardware_hostname }}'
-
-            hostname : '{{ server_hardware_hostname }}'
BREAKS HERE
-- name: Apply local customizations
BREAKS HERE
-          
-    - name: Setup completed!
-      pause: prompt="Press [Enter] to reboot."
-    - name: reboot
-      command: /sbin/reboot
-      notify: wait_for_reboot
-
-  handlers:
-    - name: wait_for_reboot
BREAKS HERE
-            - "{{dirs.uploads}}:/go/src/heavyload/uploads"
BREAKS HERE
-# Copyright (2016) Hewlett Packard Enterprise Development LP
-    build_plan_uri: '/rest/build-plans/e48052b0-2944-4579-8e81-730efffcfe81'
-            resourceUri: '{{ build_plan_uri }}'
-            readOnly: '{{ build_plan_read_only }}'
BREAKS HERE
-  - [ "jenkins-slave/defaults/main.yml" ]
-  - [ "jenkins-slave/vars/{{ ansible_distribution }}-{{ ansible_architecture }}.yml", "roles/jenkins-slave/vars/{{ ansible_distribution }}.yml" ]
-       docker_files_generated_directory         : "jenkins-slave"  
BREAKS HERE
-        tar czf {{ inventory_hostname }}.tar.gzip {{ inventory_hostname }}
-      fetch: src=/tmp/{{ inventory_hostname }}.tar.gzip flat=yes dest=../collected_files/{{ inventory_hostname }}.tar.gzip
-
-
-
BREAKS HERE
-        - ceph_stable
-        - ceph_stable
-    - role: "ceph-mon"
-    - role: "ceph-mgr"
-    - role: "rsyslog_client"
-    - role: "system_crontab_coordination"
-        - ceph_stable
-        - ceph_stable
-    - role: "ceph-osd"
-    - role: "rsyslog_client"
-    - role: "system_crontab_coordination"
BREAKS HERE
-- name: Cleanup nodes from nova
-  hosts: local
-    - { role: cleanup_nodes, nodes: '{{ cleanup_nodes }}' }
BREAKS HERE
-    when: '.s390.' not in inventory_hostname
BREAKS HERE
-          "shell iptables -t nat -I PREROUTING -p tcp -d {{hostvars['localhost']['ansible_default_ipv4']['address']}} --dport 80 -j DNAT --to-destination {{infranode_ip_addr}}:80"
-          "shell iptables -t nat -I PREROUTING -p tcp -d {{hostvars['localhost']['ansible_default_ipv4']['address']}} --dport 443 -j DNAT --to-destination {{infranode_ip_addr}}:443"
-          "shell iptables -t nat -I PREROUTING -p tcp -d {{hostvars['localhost']['ansible_default_ipv4']['address']}} --dport 8443 -j DNAT --to-destination {{master_ip_addr}}:8443"
BREAKS HERE
-#  * Fedora 25
BREAKS HERE
-      println("Configuring Jenkins External URL (current URL: '\${locationConfig.url}')...")
BREAKS HERE
-  - letsencrypt
BREAKS HERE
-      include_tasks: "{{lookup('first_found', post_locations_look)}}"
BREAKS HERE
-    when: not testing and ansible_distribution_major_version|int > 22
-    when: not testing and ansible_distribution_major_version|int > 22
-    when: testing and ansible_distribution_major_version|int > 22
-    when: testing and ansible_distribution_major_version|int > 22
BREAKS HERE
-  include: {{ ANSIBLE_REPO_PATH }}/configs/{{ env_type }}/post_ocp_nfs_config.yml
BREAKS HERE
-        - ceph_versions_osd | string | search("ceph version 12")
BREAKS HERE
-    name: "{{ item }}"
-  with_items: "{{ neutron_requires_pip_packages }}"
-    name: "{{ item }}"
-  with_items: "{{ neutron_pip_packages }}"
BREAKS HERE
-  command: "true"
-  changed_when: no
-      - notimplemented
BREAKS HERE
-          network.tenant_network_cidr 192.168.0.0/24 \
BREAKS HERE
-          [[ ! -d "/lib/modules" ]] && mkdir -p "/lib/modules"
-          - "lxc.mount.entry=/lib/modules lib/modules none bind 0 0"
BREAKS HERE
-      {%- for host in groups['all'] -%}
BREAKS HERE
-    url: https://github.com/contiv/contivctl/releases/download/v0.0.0-01-31-2016.17-56-53.UTC/contivctl-v0.0.0-01-31-2016.17-56-53.UTC.tar.bz2
-    dest: /tmp/contivctl-v0.0.0-01-31-2016.17-56-53.UTC.tar.bz2
-  shell: tar vxjf /tmp/contivctl-v0.0.0-01-31-2016.17-56-53.UTC.tar.bz2
-    creates: contivctl
BREAKS HERE
-
-    - name: Get info about the virt storage pools
-- name: Prepare & Create VMs
-  hosts: pxe_servers
-  gather_facts: no
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - deploy-vms
-  tasks:
-    - name: Stop running VMs
-        name: "{{ server_hostname }}"
-      when:
-        - server_vm | default(false) | bool
-      delegate_to: "{{ item }}"
-      with_items: "{{ groups['vm_hosts'] }}"
-    - name: Delete VM LV
-        lv: "{{ server_hostname }}"
-      when:
-        - server_vm | default(false) | bool
-      delegate_to: "{{ item }}"
-      with_items: "{{ groups['vm_hosts'] }}"
-    - name: Delete VM Disk Image
-        path: "{{ hostvars[item]['virt_pools'].pools.default.path | default('/data/images') }}/{{ server_hostname }}.img"
-      when:
-        - server_vm | default(false) | bool
-      delegate_to: "{{ item }}"
-      with_items: "{{ groups['vm_hosts'] }}"
-    - name: Undefine the VM
-        name: "{{ server_hostname }}"
-      when:
-        - server_vm | default(false) | bool
-      delegate_to: "{{ item }}"
-      with_items: "{{ groups['vm_hosts'] }}"
-
-    - name: Create VM LV
-      lvol:
-        vg: "{{ default_vm_disk_vg }}"
-        lv: "{{ server_hostname }}"
-        size: "{{ default_vm_storage }}"
-      when:
-        - server_vm | default(false) | bool
-        - default_vm_disk_mode == "lvm"
-      delegate_to: "{{ item }}"
-      with_items: "{{ groups['vm_hosts'] }}"
-            paths: "{{ hostvars[item]['virt_pools'].pools.default.path | default('/data/images') }}"
-          delegate_to: "{{ item }}"
-          with_items: "{{ groups['vm_hosts'] }}"
-          delegate_to: "{{ item }}"
-          with_items: "{{ groups['vm_hosts'] }}"
-        - name: Create VM Disk Image
-          command: >-
-            qemu-img create
-            -f qcow2
-            {% if vm_use_snapshot | bool %}
-            -b {{ hostvars[item]['virt_pools'].pools.default.path | default('/data/images') }}/{{ server_hostname }}-base.img
-            {% endif %}
-            {{ hostvars[item]['virt_pools'].pools.default.path | default('/data/images') }}/{{ server_hostname }}.img
-            {{ default_vm_storage }}m
-          when:
-            - server_vm | default(false) | bool
-          delegate_to: "{{ item }}"
-          with_items: "{{ groups['vm_hosts'] }}"
-            --add {{ hostvars[item]['virt_pools'].pools.default.path | default('/data/images') }}/{{ hostvars[item]['server_hostname'] }}.img
-    - name: Wait for guest capabilities to appear
-      command: "virsh capabilities"
-      register: virsh_caps
-      until: "'<guest>' in virsh_caps.stdout"
-      retries: 6
-      delay: 10
-      delegate_to: "{{ item }}"
-      with_items: "{{ groups['vm_hosts'] }}"
-          {%- if vm_use_snapshot | bool %}
-          {{ lookup('file', hostvars[item]['virt_pools'].pools.default.path | default('/data/images') ~ '/' ~ hostvars[item]['server_hostname'] ~ '.xml') }}
-- name: Start VMs
-    - name: Start the VM
-    - file:
-    - lineinfile:
-    - lineinfile:
BREAKS HERE
-        state: present
BREAKS HERE
-# Set this to `true` if you want your undercloud and overcloud vms to
-# have a VNC console available.
-enable_vnc_console: false
BREAKS HERE
-    detect_release_hosts: all
-  hosts:
-    - all
-      when: kolla_action == "precheck"
-  hosts: collectd
-  hosts: zookeeper
-  hosts: elasticsearch
-  hosts: influxdb
-  hosts: kibana
-  hosts: mariadb
-  hosts: memcached
-  hosts: rabbitmq
-  hosts: outward-rabbitmq
-  hosts: qdrouterd
-  hosts: etcd
-  hosts: keystone
-  hosts: kafka
-  hosts: karbor
-  hosts: opendaylight
-  hosts: panko-api
-  hosts: rally
BREAKS HERE
-            NODE_UUID=$({{ ironic_cli }} | awk '/{{ item.name | default(item) }}/ {print $2}' )
BREAKS HERE
-    - playbooks/test-vars.yml
BREAKS HERE
-# Disable the machinctl quota system.
-lxc_host_machine_quota_disabled: true
BREAKS HERE
-      - name: enable undercloud-ssl
-        include_tasks: tasks/ssl.yml
-        when: install.ssl or install.version|openstack_release > 13
-        tags: ssl
-
BREAKS HERE
-    - name: grab boto version
-      command: python -c 'import boto3; print(boto3.__version__)'
-      register: py_cmd
-
-    - name: make sure we are running correct boto version
-      assert:
-        that:
-        - py_cmd.stdout.startswith('1.7')
-    - name: check for underscores
-      fail:
-        msg: "Amazon AWS does not allow underscores _ for s3 websites, please see https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html"
-      when:
-        - create_login_page is defined
-        - create_login_page
-        - "'_' in ec2_name_prefix"
-
-    - name: grab information about AWS user
-      aws_caller_facts:
-        region: "{{ ec2_region }}"
-      register: whoami
-
-    - name: save username of AWS user
-      set_fact:
-         linklight_user: '{{ whoami.arn.split("/")[-1] }}'
-
-    - debug: msg='{{ whoami.arn.split("/")[-1] }}'
-
BREAKS HERE
-    - include: common/tasks/ssh_host_based_authentication.yml hosts={{groups.all}}
BREAKS HERE
-# Download the Galaxy instead cloning, for throw away boxes (e.g. Docker)
BREAKS HERE
-        src: "https://releases.ansible.com/ansible-tower/setup/ansible-tower-setup-latest.tar.gz"
BREAKS HERE
-          password: redhat123
BREAKS HERE
-    goss_dst: /usr/local/bin/goss
-    goss_sha256sum: 53dd1156ab66f2c4275fd847372e6329d895cfb2f0bcbec5f86c1c4df7236dde
-    goss_url: "https://github.com/aelsabbahy/goss/releases/download/{{ goss_version }}/goss-linux-{{ goss_arch }}"
-    goss_test_directory: /tmp
-        url: "{{ goss_url }}"
-        dest: "{{ goss_dst }}"
-      register: download_goss
-      until: download_goss is succeeded
-      retries: 3
-        src: "{{ item }}"
-        dest: "{{ goss_test_directory }}/{{ item | basename }}"
-      with_fileglob:
-        - "{{ lookup('env', 'MOLECULE_VERIFIER_TEST_DIRECTORY') }}/test_*.yml"
-      command: "{{ goss_dst }} -g {{ item }} validate --format {{ goss_format }}"
BREAKS HERE
-  when: ansible_distribution_version is version ('42', '<')
-
-- name: Determine latest openSUSE container build information (Leap 42)
-  # Leap 42
-  block:
-    - uri:
-        url: "{{ _lxc_hosts_container_image_url_base }}/Dockerfile"
-        return_content: true
-      register: _lxc_opensuse_image_build_info
-    - set_fact:
-        opensuse_image_build_info: "{{ _lxc_opensuse_image_build_info.content|regex_search('Version: (.*)', '\\1')|join(' ') }}"
-  when: ansible_distribution_version is version ('42', '>=')
BREAKS HERE
-  vars:
-    ansible_user: root
BREAKS HERE
-- name: configure openQA
-   - { role: openqa/server, tags: ['openqa_server'] }
-   - { role: openqa/dispatcher, tags: ['openqa_dispatcher'] }
BREAKS HERE
-    no_docker_storage_setup: false
-      no_docker_storage_setup: "{{no_docker_storage_setup}}"
-  - { role: docker_setup, when: not hostvars['bastion']['no_docker_storage_setup'], device: '/dev/vdb'}
BREAKS HERE
-      shell: "ls /etc/ceph/*.conf"
BREAKS HERE
-  hosts:
-      - controller
-      - network
-      - compute
BREAKS HERE
-  - name: lits installed koschei packages
BREAKS HERE
-        name: ceph-defaults
-    - import_role:
-        name: ceph-facts
-    - import_role:
-        {{ container_binary }} exec ceph-mon-{{ hostvars[inventory_hostname]['ansible_hostname'] }} ceph --cluster "{{ cluster }}" -s --format json
-    - name: ensure /var/lib/ceph/bootstrap-rbd-mirror is present
-      file:
-        path: /var/lib/ceph/bootstrap-rbd-mirror
-        owner: "{{ ceph_uid if containerized_deployment else 'ceph' }}"
-        group: "{{ ceph_uid if containerized_deployment else 'ceph' }}"
-        mode: '755'
-        state: directory
-      when:
-        - cephx
-      delegate_to: "{{ item }}"
-      with_items: "{{ groups[mon_group_name] }}"
-      when:
-        - inventory_hostname == groups[mon_group_name][0]
-
-    - name: create potentially missing keys (rbd and rbd-mirror)
-      ceph_key:
-        name: "client.{{ item.0 }}"
-        state: present
-        dest: "/var/lib/ceph/{{ item.0 }}/"
-        caps:
-          mon: "allow profile {{ item.0 }}"
-        cluster: "{{ cluster }}"
-      when:
-        - cephx
-      delegate_to: "{{ item.1 }}"
-      ignore_errors: True # this might fail for upgrade from J to L on rbd-mirror and also on partially updated clusters
-      with_nested:
-        - ['bootstrap-rbd', 'bootstrap-rbd-mirror']
-        - "{{ groups[mon_group_name] | difference([inventory_hostname]) }}" # so the key goes on all the nodes
-
BREAKS HERE
-    - name: dockerhub_org_name
-    - openshift_setup
-    - env_hacks
-            Hostname:                  {{ openshift_hostname }}
-            Visit https://{{ openshift_hostname }}:8443 for the web console
-            oc login --insecure-skip-tls-verify {{ openshift_hostname }}:8443 -u {{ cluster_user }} -p {{ cluster_user_password }}
BREAKS HERE
-#    - name: "debugging localhost var mapreduce_task_io_sort_mb" 
-#      debug: var={{ hostvars['localhost']['mapreduce_task_io_sort_mb'] }}
-    - name: "debug dict"
-      debug: msg="this is a value {{ str(hostvars['localhost']['ams_hbase_env']['regionserver_xmn_size']) }}"
BREAKS HERE
-    galaxy_hostname: "{{ INSTALL_HOSTNAME|default('localhost') }}"
BREAKS HERE
-      image: "{{ ceph_mds_docker_username }}/{{ ceph_mds_docker_imagename }}:{{ ceph_mds_docker_image_tag }}"
-      name: "{{ ceph_mds_docker_username }}/{{ ceph_mds_docker_imagename }}"
-      tag: "{{ ceph_mds_docker_image_tag }}"
-      image: "{{ ceph_rgw_docker_username }}/{{ ceph_rgw_docker_imagename }}:{{ ceph_rgw_docker_image_tag }}"
-      name: "{{ ceph_rgw_docker_username }}/{{ ceph_rgw_docker_imagename }}"
-      tag: "{{ ceph_rgw_docker_image_tag }}"
-      image: "{{ ceph_rbd_mirror_docker_username }}/{{ ceph_rbd_mirror_docker_imagename }}:{{ ceph_rbd_mirror_docker_image_tag }}"
-      name: "{{ ceph_rbd_mirror_docker_username }}/{{ ceph_rbd_mirror_docker_imagename }}"
-      tag: "{{ ceph_rbd_mirror_docker_image_tag }}"
-      image: "{{ ceph_nfs_docker_username }}/{{ ceph_nfs_docker_imagename }}:{{ ceph_nfs_docker_image_tag }}"
-      name: "{{ ceph_nfs_docker_username }}/{{ ceph_nfs_docker_imagename }}"
-      tag: "{{ ceph_nfs_docker_image_tag }}"
-      image: "{{ ceph_osd_docker_username }}/{{ ceph_osd_docker_imagename }}:{{ ceph_osd_docker_image_tag }}"
-      image: "{{ ceph_osd_docker_username }}/{{ ceph_osd_docker_imagename }}:{{ ceph_osd_docker_image_tag }}"
-      image: "{{ ceph_osd_docker_username }}/{{ ceph_osd_docker_imagename }}:{{ ceph_osd_docker_image_tag }}"
-      image: "{{ ceph_osd_docker_username }}/{{ ceph_osd_docker_imagename }}:{{ ceph_osd_docker_image_tag }}"
-      image: "{{ ceph_osd_docker_username }}/{{ ceph_osd_docker_imagename }}:{{ ceph_osd_docker_image_tag }}"
-      image: "{{ ceph_osd_docker_username }}/{{ ceph_osd_docker_imagename }}:{{ ceph_osd_docker_image_tag }}"
-      name: "{{ ceph_osd_docker_username }}/{{ ceph_osd_docker_imagename }}"
-      tag: "{{ ceph_osd_docker_image_tag }}"
-      image: "{{ ceph_mon_docker_username }}/{{ ceph_mon_docker_imagename }}:{{ ceph_mon_docker_image_tag }}"
-      image: "{{ ceph_restapi_docker_username }}/{{ ceph_restapi_docker_imagename }}:{{ ceph_restapi_docker_image_tag }}"
-      name: "{{ ceph_mon_docker_username }}/{{ ceph_mon_docker_imagename }}"
-      tag: "{{ ceph_mon_docker_image_tag }}"
BREAKS HERE
-        register: result
-        failed_when: "result.rc != 0 and result.stderr.find('Flavor with name baremetal already exists') != -1"
-            openstack flavor set --property 'cpu_arch'='x86_64' --property 'capabilities:boot_option'='local' baremetal
-      - name: read instackenv file for bm deployment
-              failed_when: "result.rc != 0 and result.stderr.find('Flavor with name {{ item.name }} already exists') != -1"
BREAKS HERE
-      import_role:
-    - name: "CLEANUP: remove profiles"
-      import_role:
BREAKS HERE
-#    - dovecot-solr
-# - name: Copy configuration template
-#   register: config
-#   template:
-#     src: "{{ file }}"
-#     dest: "/etc/dovecot/{{ file }}"
-#   with_items:
-#     - dovecot.conf
-#     - dovecot-dict-auth.conf.ext
-#     - dovecot-dict-sql.conf.ext
-#     - dovecot-ldap.conf.ext
-#     - dovecot-sql.conf.ext
-#   loop_control:
-#     loop_var: file
-    
-# - name: Copy configuration template
-#   register: config
-#   template:
-#     src: "conf.d/{{ file }}"
-#     dest: "/etc/dovecot/conf.d/{{ file }}"
-#   with_items:
-#     - 10-auth.conf
-#     - 10-director.conf
-#     - 10-logging.conf
-#     - 10-logging.conf.ucf-dist
-#     - 10-mail.conf
-#     - 10-mail.conf.ucf-dist
-#     - 10-master.conf
-#     - 10-ssl.conf
-#     - 10-tcpwrapper.conf
-#     - 15-lda.conf
-#     - 15-mailboxes.conf
-#     - 15-mailboxes.conf.ucf-dist
-#     - 20-imap.conf
-#     - 20-imap.conf.ucf-dist
-#     - 20-lmtp.conf
-#     - 20-lmtp.conf.ucf-dist
-#     - 20-managesieve.conf
-#     - 20-pop3.conf
-#     - 20-pop3.conf.ucf-dist
-#     - 90-acl.conf
-#     - 90-plugin.conf
-#     - 90-quota.conf
-#     - 90-sieve.conf
-#     - 90-sieve.conf.ucf-dist
-#     - 90-sieve-extprograms.conf
-#     - auth-checkpassword.conf.ext
-#     - auth-deny.conf.ext
-#     - auth-dict.conf.ext
-#     - auth-ldap.conf.ext
-#     - auth-master.conf.ext
-#     - auth-passwdfile.conf.ext
-#     - auth-sql.conf.ext
-#     - auth-static.conf.ext
-#     - auth-system.conf.ext
-#     - auth-vpopmail.conf.ext
-#   loop_control:
-#     loop_var: file
-    
BREAKS HERE
-  # create - mount data disk in order above method was unreliable
-  # Container Storage for emptydir  /var/lib/origin /dev/sdc
-  - name: Azure | Master node mount container managed disk
-    azure_rm_managed_disk:
-      name: "ocp-master-container-{{ item }}" 
-      location: "{{ location }}"
-      resource_group: "{{ rg }}"
-      disk_size_gb: 32 
-      managed_by: "ocp-master-{{ item }}"
-    loop: "{{ master_nodes }}"
-  # Docker VG /dev/sdd
-  - name: Azure | Master node mount docker managed disk
-    azure_rm_managed_disk:
-      name: "ocp-master-docker-{{ item }}" 
-      location: "{{ location }}"
-      resource_group: "{{ rg }}"
-      disk_size_gb: 32
-      managed_by: "ocp-master-{{ item }}"
-    loop: "{{ master_nodes }}"
-  # ETCD /dev/sde
-  - name: Azure | Master node mount ETCD managed disk
-    azure_rm_managed_disk:
-      name: "ocp-master-etcd-{{ item }}" 
-      location: "{{ location }}"
-      resource_group: "{{ rg }}"
-      disk_size_gb: 32
-      managed_by: "ocp-etcd-{{ item }}"
-    loop: "{{ master_nodes }}"
-  # create - mount data disk in order above method was unreliable
-  # Container Storage for emptydir  /var/lib/origin /dev/sdc
-  - name: Azure | Infra node mount container managed disk
-    azure_rm_managed_disk:
-      name: "ocp-infra-container-{{ item }}" 
-      location: "{{ location }}"
-      resource_group: "{{ rg }}"
-      disk_size_gb: 64
-      managed_by: "ocp-infra-{{ item }}"
-    loop: "{{ infra_nodes }}"
-  # Docker VG /dev/sdd
-  - name: Azure | Infra node mount docker managed disk
-    azure_rm_managed_disk:
-      name: "ocp-infra-docker-{{ item }}" 
-      location: "{{ location }}"
-      resource_group: "{{ rg }}"
-      disk_size_gb: 32
-      managed_by: "ocp-infra-{{ item }}"
-    loop: "{{ infra_nodes }}"
-  # create - mount data disk in order above method was unreliable
-  # Container Storage for emptydir  /var/lib/origin /dev/sdc
-  - name: Azure | App node mount container managed disk
-    azure_rm_managed_disk:
-      name: "ocp-app-container-{{ item }}" 
-      location: "{{ location }}"
-      resource_group: "{{ rg }}"
-      disk_size_gb: 64
-      managed_by: "ocp-app-{{ item }}"
-    loop: "{{ app_nodes }}"
-  # Docker VG /dev/sdd
-  - name: Azure | App node mount docker managed disk
-    azure_rm_managed_disk:
-      name: "ocp-app-docker-{{ item }}" 
-      location: "{{ location }}"
-      resource_group: "{{ rg }}"
-      disk_size_gb: 32
-      managed_by: "ocp-app-{{ item }}"
-    loop: "{{ app_nodes }}"
BREAKS HERE
-# During the Ocata development cycle, the role will begin adding the RHEL 7
-# STIG content. By default, all operating systems will use the RHEL 6 STIG
-# until the work has completed.
-#
-# This variable should only be adjusted for testing purposes.
-stig_version: rhel6
BREAKS HERE
-  hosts: simple-koji-ci-dev.fedorainfracloud.org, simple-koji-ci-prod.fedorainfracloud.org
-  hosts: simple-koji-ci-dev.fedorainfracloud.org, simple-koji-ci-prod.fedorainfracloud.org
BREAKS HERE
-  env: [{name: LIBVIRT_LXC_NOSECLABEL}]
-  env: [{name: DISPLAY_SKIPPED_HOSTS}]
-  env: [{name: NETWORK_GROUP_MODULES}]
BREAKS HERE
-  - name: Source gitlab server
-    command: cat ./git_servers.out
-    register: gitlab_server
-
-  - name: Set gitlab server fact
-    set_fact:
-      gitlab_server: "{{ gitlab_server.stdout_lines }}"
-
BREAKS HERE
-  hosts: neutron_server
-  user: root
-  become: true
-  gather_facts: true
-  any_errors_fatal: true
-  roles:
-    - role: "os_neutron"
-  vars_files:
-    - test-vars.yml
-
-
-
-- name: Deploy the rest of neutron
-  hosts: "neutron_all:!neutron_server"
BREAKS HERE
-    roles:
-      - azure_infra
BREAKS HERE
-        src: "{{ ansible_user_dir }}/src/{{ current_test_repo }}/logs/"
-        dest: "{{ zuul.executor.log_root }}"
BREAKS HERE
-        content=orchestator-token-x86_64.stdout
BREAKS HERE
-        src:"{{files}}/osbs/fix-docker-iptables.{{ env }}"
BREAKS HERE
-
-## Host security hardening
-# The openstack-ansible-security role provides security hardening for hosts
-# by applying security configurations from the STIG. Hardening is disabled by
-# default, but an option to opt-in is available by setting the following
-# variable to 'true'.
-# Docs: http://docs.openstack.org/developer/openstack-ansible-security/
-# apply_security_hardening: true
BREAKS HERE
-    is_metal: "{{ properties.is_metal|default(false) }}"
BREAKS HERE
-  - name: "Docs | Create DNS entry for {{ site_prefix }}.{{ ec2_route53_zone }}"
-      record: "{{ site_prefix }}.{{ ec2_route53_zone }}"
BREAKS HERE
-          {% for item in groups['all_containers'] -%}
BREAKS HERE
-      namespace: ""
BREAKS HERE
-- name: Execute container commands
-  lxc_container:
-    name: "{{ inventory_hostname }}"
-    container_command: |
-      {{ lxc_container_commands }}
-  delegate_to: "{{ physical_host }}"
-
-# NOTE(cloudnull): Should a container already be up and running with a defined container interface
-#                  the shell command will use the MAC address already set within the container as
-#                  it's value. This allows the tasks to remain idempotent while ensuring that a
-#                  container restart isn't required to set a static mac.
-#                  This task is being done to allow a container to have a static mac address.
-#                  before this task a container had a dynamic mac address which would
-#                  change when a container was restarted. This restart process causes terrible
-#                  issues in several network intensive systems (RabbitMQ, Neutron, etc). To
-#                  resolve the rotating mac address issue this task is setting the mac in a hwaddr
-#                  file and a lookup is being used in the container-interface.ini template to render
-#                  the static hardware address as expected.
-  shell: |
-    C_PID="$(lxc-info --name {{ inventory_hostname }} --pid | awk '/PID:/ {print $2}')"
-    C_ADDR="/proc/${C_PID}/root/sys/class/net/{{ item.value.interface }}/address"
-    HARDWARE_ADDR="/var/lib/lxc/{{ inventory_hostname }}/{{ item.value.interface }}.hwaddr"
-    HEXCHARS="0123456789abcdef"
-    if ! cat "${C_ADDR}" > "${HARDWARE_ADDR}"; then
-      echo "00:16:3e$(
-        for i in {1..6}; do
-          echo -n "${HEXCHARS:$(( $RANDOM % 16 )):1}"
-        done | sed -e 's/\(..\)/:\1/g'
-      )" > "${HARDWARE_ADDR}"
-    fi
-    executable: /bin/bash
-  with_dict: "{{ container_networks | default({}) }}"
-  tags:
-    - skip_ansible_lint
-  with_dict: "{{ container_networks | default({}) }}"
-# NOTE(cloudnull): To dynamically set the the mac address "facts" Ansible line format is being used
-  with_dict: "{{ container_networks | default({}) }}"
-  delegate_to: "{{ physical_host }}"
-
-- name: Create start
-  lxc_container:
-    name: "{{ inventory_hostname }}"
-    state: started
-  delegate_to: "{{ physical_host }}"
-
-- name: Drop container network file (interfaces)
-  template:
-    src: "{{ lxc_container_interface }}"
-    dest: "{{ lxc_container_interface_target }}"
-    owner: "root"
-    group: "root"
-    mode: "0644"
-  with_dict: "{{ container_networks | default({}) }}"
-
-- name: Drop container network file (routes)
-  template:
-    src: "{{ lxc_container_route_interface }}"
-    dest: "{{ lxc_container_default_route_interfaces }}"
-    owner: "root"
-    group: "root"
-    mode: "0644"
-  when:
-    - lxc_container_route_interface is defined
-    - lxc_container_default_route_interfaces is defined
-    - item.value.static_routes is defined or
-      (item.value.gateway is defined and ansible_pkg_mgr == "zypper")
-  with_dict: "{{ container_networks | default({}) }}"
-
-- name: Drop container setup script
-  template:
-    src: "container-setup.sh.j2"
-    dest: "/opt/container-setup.sh"
-    owner: "root"
-    group: "root"
-    mode: "0755"
-
-- name: Run container setup script
-  command: /opt/container-setup.sh
-  register: container_setup
-  changed_when: false
-  failed_when: container_setup.rc != 0
-
-# NOTE(major): the lxc.network.veth.pair line must appear *immediately* after
-# the lxc.network.name congfiguration line or it will be ignored.  That's why
-# you'll find a "insertafter" in this YAML block.
-- name: Add veth pair name to match container name
-  lineinfile:
-    dest: "/var/lib/lxc/{{ inventory_hostname }}/config"
-    line: "lxc.network.veth.pair = {{ lxc_container_network_veth_pair_prefix }}_eth0"
-    insertafter: "^lxc.network.name"
-    backup: "true"
-  with_dict: "{{ container_networks | default({}) }}"
-    dest: "/usr/local/bin/lxc-veth-wiring"
-- name: Run container veth wiring script
-  command: >
-    /usr/local/bin/lxc-veth-wiring
-    "{{ inventory_hostname }}"
-    "{{ lxc_container_network_veth_pair[-15:] }}"
-    "{{ item.value.interface }}"
-    "{{ item.value.bridge }}"
-  register: wiring_script
-  with_dict: "{{ container_networks | default({}) }}"
-  when:
-    - item.value.interface is defined
-    - item.value.type is not defined or item.value.type == 'veth'
-  failed_when: wiring_script.rc not in [3, 0]
-  changed_when: wiring_script.rc == 3
-  delegate_to: "{{ physical_host }}"
-
-#  TODO(someone) This should be removed once an upstream patch has
-#  been submitted to either the kernel or LXC to fix the veth issues.
-#  Container restart is not happening here because it's not needed.
-# Flush the handlers to ensure the container and networking is online.
-- meta: flush_handlers
-- name: Wait for container connectivity
-  wait_for_connection:
-    connect_timeout: "{{ lxc_container_wait_params.connect_timeout | default(omit) }}"
-    delay: "{{ lxc_container_wait_params.delay | default(omit) }}"
-    sleep: "{{ lxc_container_wait_params.sleep | default(omit) }}"
-    timeout: "{{ lxc_container_wait_params.timeout | default(omit) }}"
-# NOTE(hwoarang) openSUSE randomly fails to start the service
-# with an error like the following one
-# sysctl-container.service: Failed at step CGROUP spawning /sbin/sysctl: No such device
-# Until this is fixed, we workaround it by simply retrying a few more times
-# before giving up
-# https://bugzilla.suse.com/show_bug.cgi?id=1055426
-# https://bugs.launchpad.net/openstack-ansible/+bug/1712741
-- name: Enable container sysctl service
-  service:
-    name: "sysctl-container"
-    state: started
-    enabled: yes
-    daemon_reload: yes
-  remote_user: root
-  register: _sysctl_service_started
-  until: _sysctl_service_started|success
-  retries: 5
-  delay: 5
BREAKS HERE
-          line: "{{ provision.topology.username }} ALL=(root) NOPASSWD:ALL"
BREAKS HERE
-    # The following variables must be set to run this example.
-    # logical_switch_group_name: 'Logical Switch Group Cisco Nexus 55xx'  # Logical Switch Group which the logical switch is derived from
-    ssh_password: 'lsg1&2_password'  # SSH Password
-            # newName: 'Test Logical Switch - Renamed'
-    # - name: Reclaim the top-of-rack switches in the logical switch
-    #   oneview_logical_switch:
-    #     config: "{{ config }}"
-    #     state: refreshed
-    #     data:
-    #       logicalSwitch:
-    #         name: 'Test Logical Switch - Renamed'
-    #   delegate_to: localhost
-    #
-    # - name: Delete the Logical Switch
-    #   oneview_logical_switch:
-    #     config: "{{ config }}"
-    #     state: absent
-    #     data:
-    #       logicalSwitch:
-    #         name: 'Test Logical Switch - Renamed'
-    #   delegate_to: localhost
BREAKS HERE
-    admin_user: "{{ openshift_master_htpasswd_users.keys() }}"
-      shell: oc adm policy add-cluster-role-to-user cluster-admin {{ admin_user }}
BREAKS HERE
-                 --disk size={{item.disks.var.size | default('40')}},{{item.disks.var.options | default('format=qcow2,cache=none,io=native')}}
BREAKS HERE
-      add_host:
BREAKS HERE
-  hosts: notifs-web:notifs-web-stg:notifs-backend:notifs-backend-stg
-  hosts: notifs-web;notifs-web-stg
BREAKS HERE
-  - name: yum update bugzilla2fedmsg packages from main repo
-    yum: name="python-bugzilla2fedmsg" state=latest
-  - name: yum update bugzilla2fedmsg packages from testing repo
BREAKS HERE
-        state: latest
-        state: latest
-        state: latest
-        extra_args: "{{ pip_install_options|default('') }}"
BREAKS HERE
-  hosts: all
BREAKS HERE
-  default: ~
-  default: ~
BREAKS HERE
-DEFAULT_SUDO:
-  default: False
-  deprecated:
-    why: In favor of Ansible Become, which is a generic framework
-    version: "2.9"
-    alternatives: become
-  description: 'Toggle the use of "sudo" for tasks.'
-  env: [{name: ANSIBLE_SUDO}]
-  ini:
-  - {key: sudo, section: defaults}
-  type: boolean
-DEFAULT_SUDO_EXE:
-  name: sudo executable
-  default: sudo
-  deprecated:
-    why: In favor of Ansible Become, which is a generic framework. See become_exe.
-    version: "2.9"
-    alternatives: become
-  description: 'specify an "sudo" executable, otherwise it relies on PATH.'
-  env: [{name: ANSIBLE_SUDO_EXE}]
-  ini:
-  - {key: sudo_exe, section: defaults}
-DEFAULT_SUDO_FLAGS:
-  name: sudo flags
-  default: '-H -S -n'
-  deprecated:
-    why: In favor of Ansible Become, which is a generic framework. See become_flags.
-    version: "2.9"
-    alternatives: become
-  description: 'Flags to pass to "sudo"'
-  env: [{name: ANSIBLE_SUDO_FLAGS}]
-  ini:
-  - {key: sudo_flags, section: defaults}
-DEFAULT_SUDO_USER:
-  name: sudo user
-  default:
-  deprecated:
-    why: In favor of Ansible Become, which is a generic framework. See become_user.
-    version: "2.9"
-    alternatives: become
-  description: 'User you become when using "sudo", leaving it blank will use the default configured on the target (normally root)'
-  env: [{name: ANSIBLE_SUDO_USER}]
-  ini:
-  - {key: sudo_user, section: defaults}
BREAKS HERE
-    - "koji01{{ env_prefix }}.phx2.fedoraproject.org"
-    - "koji02{{ env_prefix }}.phx2.fedoraproject.org"
BREAKS HERE
-    url: https://github.com/contiv/netplugin/releases/download/v0.0.0-11-12-2015.00-55-50.UTC/netplugin-v0.0.0-11-12-2015.00-55-50.UTC.tar.bz2
-- name: copy environment file for netplugin
-  copy: src=netplugin dest=/etc/default/netplugin
-
-- name: copy systemd units for netplugin
-  copy: src=netplugin.service dest=/etc/systemd/system/netplugin.service
-
-- name: start netplugin
-  shell: systemctl daemon-reload && systemctl start netplugin
-
-- name: setup netmaster host alias
-  shell: echo "{{ service_vip }} netmaster" >> /etc/hosts
-
-# XXX: need to move the following to correct roles
-
-- name: install docker-compose
-  shell: pip install docker-compose
-
-- name: install contiv-compose
-  shell: >
-     source /etc/profile.d/00golang.sh && \
-     go get github.com/contiv/libcompose && \
-     cd $GOPATH/src/github.com/contiv/libcompose && \
-     make binary && \
-     ln -s `pwd`/bundles/libcompose-cli_linux-amd64 $GOPATH/bin/contiv-compose
BREAKS HERE
-    with_items: encrypted_ceph_partuuid.stdout_lines
BREAKS HERE
-  - name: expire-caches
-    command: yum clean expire-cache
-    when: ansible_distribution_major_version|int < 22
-
-  - name: yum -y {{ yumcommand }}
-    command: yum -y {{ yumcommand }}
-    async: 7200
-    poll: 30
-    when: ansible_distribution_major_version|int < 22
-
-  - name: dnf -y {{ yumcommand }} --refresh
-    command: dnf -y {{ yumcommand }} --refresh
-    when: ansible_distribution_major_version|int > 21 and ansible_cmdline.ostree is not defined
BREAKS HERE
-        update_cache: yes
-        cache_valid_time: 600
BREAKS HERE
-    docker_cmd: "docker run -d --name=plex -p {{plex.port}}:{{plex.port}}/tcp -e PUID={{uid.stdout}} -e PGID={{gid.stdout}} -e CHANGE_CONFIG_DIR_OWNERSHIP=false -v /etc/localtime:/etc/localtime:ro -v /opt/plex:/config -v {{plex.transcodes}}:/transcode -v /mnt/unionfs/Media:/data --restart=always --network=host plexinc/pms-docker"
BREAKS HERE
-      command: /usr/bin/ansible --version
BREAKS HERE
-        - "{{ mysql_running.rc == 0 }}"
-      with_items: "{{ play_hosts }}"
-        - "{{ mysql_running.rc == 0 }}"
-        - "{{ hostvars[item]['mysql_running']['rc'] == 0 }}"
-        - "{{ hostvars[inventory_hostname]['mysql_status']['wsrep_incoming_addresses'] != hostvars[item]['mysql_status']['wsrep_incoming_addresses'] }}"
-      with_items: "{{ play_hosts }}"
-      when: "{{ hostvars[item].mysql_running.rc == 0 }}"
BREAKS HERE
-    ansible_private_key_file: "~/.ssh/id_rsa"
-    ansible_private_key_file: "~/.ssh/id_rsa"
BREAKS HERE
-   - answerfile.yml
BREAKS HERE
-            root_disk: "{{ install.root.disk.get('override', []) }}"
-        when: install.get('root',{}).get('disk',{})
-        tags: skip_ansible_lint
BREAKS HERE
-    owner: {{user}}
-    group=: {user}}
BREAKS HERE
-- hosts: localhost
-    - sshd
-    - pip_install
-    - bootstrap-host
BREAKS HERE
-
-- name: create a test dir to copy
-  file:
-    path: '{{ output_dir }}/top_dir'
-    state: directory
-
-- name: create a test dir to symlink to
-  file:
-    path: '{{ output_dir }}/linked_dir'
-    state: directory
-
-- name: create a file in the test dir
-  copy:
-    dest: '{{ output_dir }}/linked_dir/file1'
-    content: 'hello world'
-
-- name: create a link to the test dir
-  file:
-    path: '{{ output_dir }}/top_dir/follow_link_dir'
-    src: '{{ output_dir }}/linked_dir'
-    state: link
-
-- name: copy the directory's link
-  copy:
-    src: '{{ output_dir }}/top_dir'
-    dest: '{{ output_dir }}/new_dir'
-    follow: True
-
-- name: stat the copied path
-  stat:
-    path: '{{ output_dir }}/new_dir/top_dir/follow_link_dir'
-  register: stat_dir_result
-
-- name: stat the copied path
-  stat:
-    path: '{{ output_dir }}/new_dir/top_dir/follow_link_dir/file1'
-  register: stat_file_in_dir_result
-
-- name: assert that the directory exists
-  assert:
-    that:
-    - stat_dir_result.stat.exists
-    - stat_dir_result.stat.isdir
-    - stat_file_in_dir_result.stat.exists
-    - stat_file_in_dir_result.stat.isreg
BREAKS HERE
-- name: Get nodes from nova
-
BREAKS HERE
-      oracle_java_rpm_validate_certs: no
-      oracle_java_set_as_default: yes
-      oracle_java_rpm_validate_certs: no
-      oracle_java_apt_repository: 'deb http://ppa.launchpad.net/webupd8team/java/ubuntu bionic main'
-      oracle_java_apt_repository_key: 'C2518248EEA14886'
BREAKS HERE
-          dest: /var/lib/libvirt/images/{{item.name}}.qcow2"
BREAKS HERE
-      db_password: "{{ nova_container_mysql_password }}"
BREAKS HERE
-    - "/home/{{ odoo_user }}/odoo/server/openerp/addons"
BREAKS HERE
-  gather_facts: no
BREAKS HERE
-  - include_vars: group_vars/all
BREAKS HERE
-    priv: "{{ beaker_db_name }}.*:ALL,GRANT state=present"
BREAKS HERE
-      server: https://openshift-master.libvirt
BREAKS HERE
-
BREAKS HERE
-           --subdomains="{{ item.value.multisite.subdomains | default('false') }}"
BREAKS HERE
-      do_not_subscribe: "{{do_not_subscribe}}"
-    when: not hostvars['bastion']['do_not_subscribe']
-    when: not hostvars['bastion']['do_not_subscribe']
BREAKS HERE
-  - { role: ansible-role-pxe_bootstrap, tags: [ 'pxe_bootstrap' ] }
-  - { role: ansible-role-dhcp_server, tags: [ 'dhcp_server', 'dhcpserver' ] }
-  - { role: ansible-role-dnsmasq, tags: [ 'dnsmasq' ] }
-  - { role: ansible-role-pxe_config, tags: [ 'pxe_config' ] }
BREAKS HERE
-- name: Drop Keystone Configs
BREAKS HERE
-  regexp: '^relayhost = bastion.phx2.fedoraproject.org'
BREAKS HERE
-          - (ansible_distribution == 'Ubuntu' and ansible_distribution_release == 'xenial') or
-        msg: "The only supported platforms for this release are Ubuntu 16.04 LTS (Xenial), Ubuntu 18.04 LTS (Bionic),  CentOS 7 (WIP) and openSUSE Leap 42.X and openSUSE Leap 15.X"
BREAKS HERE
-  gather_facts: no
BREAKS HERE
-          welcome_url: "/static/welcome.html"
BREAKS HERE
-# We need swift_vars to exist for the "swift_vars.drives is defined check" to work
-- name: "Set swift_vars if undefined"
-  set_fact:
-    swift_vars: "{}"
-  when: swift_vars is not defined
-  tags:
-    - always
-
BREAKS HERE
-  hosts: local:&rax
BREAKS HERE
-      # TODO(yprokule): figure out if we should migrate instances from computes ?
-      - name: put node to standby mode
-        command: |
-            sudo pcs node standby --wait=300
-        ignore_errors: True
-        register: standby_node
-      - name: unstandby pacemaker node
-        command: pcs node unstandby
-        when: standby_node|succeeded
-        shell: pcs status | grep 'Online:'
-        when: standby_node|succeeded
-      - name: cleanup resource
-          - standby_node|succeeded
-      - name: Check for any stopped pcs resources
-            - standby_node|succeeded
-        when: standby_node|succeeded
BREAKS HERE
-  - debug: msg="CAUTION! DO NOT STOP THIS PLAYBOOK ONCE STARTED!"
-  - pause: prompt="DO NOT ABORT THIS PLAYBOOK, IT WILL TAKE LONG!"
-    register: copy_results
-    with_items: "{{copy_results.results}}"
-    when: ansible_system_vendor == "Dell Inc." and "'stat' in item and not item.stat.exists"
-    with_items: "{{copy_results.results}}"
-  - debug: var=update_results
-    with_items: "{{applied_results.results}}"
-    with_items: "{{copy_results.results}}"
BREAKS HERE
-#in case VM was created in UEFI mode libvirt requires undefine task first
-- name: undefine relevant VMs
-  virt:
-      name: "{{ item }}"
-      command: undefine
-  with_items: "{{ vm_names.list_vms }}"
-  when: provision.bootmode == 'uefi'
-
-  virt:
-      name: "{{ item }}"
-      command: undefine
-  when: provision.bootmode != 'uefi'
BREAKS HERE
-    - { role: bie-hub,          bie_hub: ala-bie, bie_hub_version: "1.4.2" }
BREAKS HERE
-    nova_serial: "{{ nova_compute_serial | default('50%') }}"
BREAKS HERE
-          msg: "Using {{logspout_target}} target"
-            loggly: 
-              SYSLOG_STRUCTURED_DATA: "{{_loggly_api_key}}@41058 tag=\"Logspout\""
-          drdc_memory_limit: '200m'
-          drdc_cmd: "{{endpoints[logspout_target]}}"
-          drdc_env: "{{envs[logspout_target]|default({})}}"
-      when: "_logspout_enabled|default(False) == True"
BREAKS HERE
-#ceph_conf_file: |
-#  [global]
-#  fsid = 00000000-1111-2222-3333-444444444444
-#  mon_initial_members = mon1.example.local,mon2.example.local,mon3.example.local
-#  mon_host = 10.16.5.40,10.16.5.41,10.16.5.42
-#  # optionally, you can use this construct to avoid defining this list twice:
-#  # mon_host = {{ ceph_mons|join(',') }}
-#  auth_cluster_required = cephx
-#  auth_service_required = cephx
BREAKS HERE
-  copy:
-    src: /etc/hosts
-    dest: /etc/hosts
BREAKS HERE
-      - "{{ combined_devices_list }}"
-      - "{{ combined_devices_list }}"
BREAKS HERE
-vars:
-  mon_group_name:       mons
-  osd_group_name:       osds
-  mds_group_name:       mdss
-  rgw_group_name:       rgws
-
-hosts:
-  - "{{ mon_group_name }}"
-  - "{{ osd_group_name }}"
-  - "{{ mds_group_name }}"
-  - "{{ rgw_group_name }}"
BREAKS HERE
-## Authentication
-## Fail2ban
BREAKS HERE
-          'address': "{{ item.instances[0].public_dns_name }}",
BREAKS HERE
-    ptp_domains:
-    clock_step_threshold: 0.0001
-    min_time_sources: 2
BREAKS HERE
-      register: output
-      register: output
-    - debug: var=output.stdout_lines
BREAKS HERE
-      keepalived_sync_groups: "{{ keepalived_master_sync_groups }}"
-      keepalived_scripts: "{{ keepalived_master_scripts }}"
-      keepalived_instances: "{{ keepalived_master_instances }}"
-      when: >
-        haproxy_use_keepalived|bool and
-        inventory_hostname in groups['haproxy'][0]
-    - role: "keepalived"
-      keepalived_sync_groups: "{{ keepalived_backup_sync_groups }}"
-      keepalived_scripts: "{{ keepalived_backup_scripts }}"
-      keepalived_instances: "{{ keepalived_backup_instances }}"
-      when: >
-        haproxy_use_keepalived|bool and
-        inventory_hostname in groups['haproxy'][1:]
BREAKS HERE
-    - name: Ensuring lxc_hosts_package_state is well overriden
-    - name: Ensuring security_package_state is overriden
BREAKS HERE
-      command: ping -i 5 -c 6 8.8.8.8
BREAKS HERE
-  tags: sieve
-    src: 'sieve/{{ file }}'
-    dest: '/etc/dovecot/sieve/{{ file }}'
-  with_items:
-    - learn-spam.sh
-    - learn-ham.sh
-  loop_control:
-    loop_var: file
BREAKS HERE
-      when: '{{ovs_plugin}} == "multitenant"'
BREAKS HERE
-    SSLCertificateChainFile: wildcard-2017.app.os.stg.fedoraproject.org.intermediate.cert
BREAKS HERE
-# container image. If the cache map is overriden, the following fields are
BREAKS HERE
-      command: scripts/test-log-collect.sh
-        chdir: "src/{{ current_test_repo }}"
-        src: "src/{{ current_test_repo }}/logs"
-        dest: "{{ zuul.executor.log_root }}"
BREAKS HERE
-    ignore_errors: "{{ ignore_register_errors | default(yes) }}"
-    ignore_errors: "{{ ignore_disable_errors | default(yes) }}"
-    ignore_errors: "{{ ignore_enable_errors | default(yes) }}"
BREAKS HERE
-    when: remove_osd_mountpoints.failed is defined
BREAKS HERE
-        hostvars[mon_host]['ansible_hostname'] in (ceph_health_raw.stdout | from_json)["quorum_names"]
-        hostvars[mon_host]['ansible_hostname'] in (ceph_health_raw.stdout | from_json)["quorum_names"]
BREAKS HERE
-      host: "{{item}}"
-      name: "{{item}}"
-    loop:
-     - "{{ hostvars[groups['webservers'][0]].ansible_host }}"
-     - "{{ hostvars[groups['webservers'][1]].ansible_host }}"
BREAKS HERE
-      username: "user@dbnamer"
-      password: "P@ssw0rd"
-        username: "{{ username }}"
-        password: "{{ password }}"
-        username: "{{ username }}"
-        password: "{{ password }}"
-        username: "{{ username }}"
-        password: "{{ password }}"
-        username: "{{ username }}"
-        password: "{{ password }}"
BREAKS HERE
-  - { role: openshift/object, vars: {app: greenwave, template: buildconfig.yml, when : env != 'staging' }}
-  - { role: openshift/start-build, vars: {app: greenwave, name: greenwave-docker-build, when: env != 'staging' }}
BREAKS HERE
-- name: Update 7.4 to latest
-- name: prepare storage for crio
-  - { role: crio_setup}
BREAKS HERE
-  - name: Pause for some seconds
BREAKS HERE
-        ansible_ssh_private_key_file="{{ lookup('env','HOME')}}/.ssh/id_rsa"
BREAKS HERE
-    - name: Check for the lack of presence of any bound mount (except the backup bind mount) for container3
-      command: grep "lxc.mount.entry = /openstack/container" /var/lib/lxc/container3/config
-      register: container1_bind_mount
-      failed_when: container1_bind_mount.rc == 0
BREAKS HERE
-    - reprovision: "{{ hostvars['localhost'].reprovision }}"
BREAKS HERE
-        shell: "{{ product.name }}-release {{ product.version|int }}"
-        shell: "rhos-release  -x {{ product.version|int }}{{ installer_host_repo | default('')}};  rhos-release  -d {{ product.version|int }}"
BREAKS HERE
-          koji_hub: 'https://koji.stg.fedoraproject.org/kojihub'
BREAKS HERE
-      - "localhost"
-    monit_alert_address: address@address.com
BREAKS HERE
-            user: "{{ hostvars['hypervisor'].ansible_ssh_user }}"
BREAKS HERE
-          group_name: "{{ ec2_security_group_tower}}"
-        - proto: icmp
-          from_port: -1
-          to_port: -1
-          group_name: "{{ ec2_security_group_tower}}"
BREAKS HERE
-  - include: badges-backend
BREAKS HERE
-      - ceph_data_partlabels.rc == 0
-      - osd_scenario == 'non-collocated'
BREAKS HERE
-    ports: "{{ (item.ports|replace('Eth','Ethernet')).split(', ') }}"
-    items: "^(?P<id>\\d+)\\s+(?P<name>\\S+)\\s+(?P<status>\\w+)\\s+(?P<ports>.+)"
BREAKS HERE
-- name: set required facts
-  hosts: controller
-  tasks:
-      - name: compute the directory basename
-        set_fact: component_basename={{ installer.component.dir.split('/')|last }}
-
-      - name: find the test dependencies file used for the test-run
-        set_fact: test_deps_file="{{ installer.component.dir + '/' + installer.component.config_file }}" #"
-
-- name: load test config
-  hosts: controller
-  tasks:
-      - name: load config
-        include_vars: "{{test_deps_file}}"
-        register: result
-
-      - name: set test_distro
-        set_fact: test_distro="{{ ansible_distribution + '-' + ansible_distribution_version.split(".")[0] }}" #"
-
-      - name: set test_env
-        set_fact: test_env="{{ test_config.virt[test_distro] }}" #"
-
BREAKS HERE
-
-
BREAKS HERE
-  - name: generate user info string for email when a workshop, create_login_page set to true, S3 bucket
-    debug:
-      msg: 
-      - "user.info: The list of VMs for this workshop is available at:"
-      - "user.info: https://s3.amazonaws.com/{{ guid }}.{{ workshop_dns_zone }}/{{ guid }}-index.html" 
BREAKS HERE
-nexus_anonymous_access: true
-  write_policy: allow # allow_once or allow # Note: nexus restore wont work with 'allow_once'
-  write_policy: allow # allow_once or allow # Note: nexus restore wont work with 'allow_once'
-  write_policy: allow # allow or allow
-  write_policy: allow # allow_once or allow # Note: nexus restore wont work with 'allow_once'
-  write_policy: allow # allow_once or allow # Note: nexus restore wont work with 'allow_once'
-  write_policy: allow # allow_once or allow # Note: nexus restore wont work with 'allow_once'
-  write_policy: allow # allow_once or allow # Note: nexus restore wont work with 'allow_once'
BREAKS HERE
-            --setopt 'rhelosp-{{ install.version }}.0-devtools-puddle.includepkgs=openstack-packstack*,puppet-tempest' \
-            --enable rhelosp-{{ install.version }}.0-devtools-puddle >>/var/log/yum.log
BREAKS HERE
-
-          aws_access_key_id = "{{aws_access_key_id}}"
-          aws_secret_access_key = "{{aws_secret_access_key}}"' > /home/opentlc-mgr/.aws/credentials
-    - name: Copy boto.cfg file to /etc
-        mode: 0755
-        line: "/dev/{{ storage_vg }}/nfsmount {{ storage_mount_path}} ext4 defaults 0 0"
-    - shell: "mount {{ storage_mount_path }} || echo already mounted"
-
-    - shell: "systemctl enable httpd && systemctl restart httpd"
BREAKS HERE
-    regex: /(.*)/workstation/prerelease.*$
-    regex: /(.*)/server/prerelease.*$
-    regex: /(.*)/atomic/prerelease.*$
-    regex: /(.*)/prerelease.*$
-    regex: /(.*)/prerelease.*$
-    regex: /(.*)/prerelease.*$
BREAKS HERE
-        wait: true
-          'region': "{{ item.instances[0].tags['molecule_region'] }}",
-          'user': "{{ item.instances[0].tags['molecule_ssh_user'] }}",
-        port: "{{ ssh_port }}"
-        delay: 10
BREAKS HERE
-    - name: Ensure dependencies are installed
BREAKS HERE
-#    ldap_auth: 'simple'
-#    ldap_auth_username: 'username' (or DN)
-#    ldap_auth_password: 'password'
BREAKS HERE
-- name: "MEDIUM | RHEL-07-010150 | PATCH | When passwords are changed or new passwords are assigned, the new password must contain at least one special character."
-- name: "MEDIUM | RHEL-07-010200 | PATCH | The PAM system service must be configured to store only encrypted representations of passwords."
-      name: system-auth
-      state: "{{ item.state }}"
-      module_arguments: "{{ item.args }}"
-  with_items:
-      - state: args_present
-        args:
-            - "sha512"
-      - state: args_absent
-        args:
-            - "md5"
-            - "bigcrypt"
-            - "sha256"
-            - "blowfish"
-- name: "MEDIUM | RHEL-07-010270 | PATCH | Passwords must be prohibited from reuse for a minimum of five generations."
-  pamd:
-      name: system-auth
-      state: args_present
-      type: password
-      control: sufficient
-      module_path: pam_unix.so
-      module_arguments: "remember=5"
-              "MEDIUM | RHEL-07-010320 | PATCH | Accounts subject to three unsuccessful login attempts within 15 minutes must be locked for the maximum configurable period."
-              "MEDIUM | RHEL-07-010330 | PATCH | If three unsuccessful logon attempts within 15 minutes occur the associated account must be locked."
-            module_arguments: "preauth silent audit deny={{ rhel7stig_pam_faillock.attempts }}{{ (rhel7stig_pam_faillock.fail_for_root) | ternary(' even_deny_root ',' ') }}fail_interval={{ rhel7stig_pam_faillock.interval }} unlock_time={{ rhel7stig_pam_faillock.unlock_time }}"
-      - name: "MEDIUM | RHEL-07-010330 | PATCH | If three unsuccessful logon attempts within 15 minutes occur the associated account must be locked."
-            module_arguments: "authfail audit deny={{ rhel7stig_pam_faillock.attempts }}{{ (rhel7stig_pam_faillock.fail_for_root) | ternary(' even_deny_root ',' ') }}fail_interval={{ rhel7stig_pam_faillock.interval }} unlock_time={{ rhel7stig_pam_faillock.unlock_time }}"
-      - name: "MEDIUM | RHEL-07-010330 | PATCH | If three unsuccessful logon attempts within 15 minutes occur the associated account must be locked."
-- name: "MEDIUM | RHEL-07-010340 | PATCH | Users must provide a password for privilege escalation."
-- name: "MEDIUM | RHEL-07-010350 | PATCH | Users must re-authenticate for privilege escalation."
-- name: "MEDIUM | RHEL-07-010430 | PATCH | The delay between logon prompts following a failed console logon attempt must be at least four seconds."
BREAKS HERE
-    - "{{ rolename | basename }}"
BREAKS HERE
-    - "./env_vars.yml"
-    - "./env_secret_vars.yml"
BREAKS HERE
-  - name: install special git.fedorahosted-redirects.conf with letsencrypt info
BREAKS HERE
-
-# TODO(inc0): we're dropping iptables rules but in fact we should create
-# linuxbridge-managed tunnels for control and dataplane
-
-    - name: Drop iptables rules
-      command: "iptables -F"
-      become: true
BREAKS HERE
-- name: Install cinder server
-  max_fail_percentage: 20
-  pre_tasks:
-    - include: common-tasks/dynamic-address-fact.yml
-      vars:
-        network_address: "storage_address"
-    - include: common-tasks/os-lxc-container-setup.yml
-      static: no
-      vars:
-        aa_profile: "unconfined"
-        extra_container_config:
-          - "lxc.autodev=0"
-          - "lxc.cgroup.devices.allow=a *:* rmw"
-          - "lxc.mount.entry=udev dev devtmpfs defaults 0 0"
-        extra_container_config_no_restart:
-          - "lxc.start.order=79"
-      when:
-        - inventory_hostname in groups['cinder_volume']
-        - cinder_backend_lvm_inuse | bool
-    - include: common-tasks/os-lxc-container-setup.yml
-      static: no
-      when:
-        - inventory_hostname not in groups['cinder_volume']
-    - include: common-tasks/rabbitmq-vhost-user.yml
-      static: no
-        - inventory_hostname == groups['cinder_all'][0]
-        - groups[cinder_rabbitmq_host_group] | length > 0
-    - include: common-tasks/rabbitmq-vhost-user.yml
-      static: no
-        - cinder_ceilometer_enabled | bool
-        - inventory_hostname == groups['cinder_all'][0]
-        - groups[cinder_rabbitmq_telemetry_host_group] is defined
-        - groups[cinder_rabbitmq_telemetry_host_group] | length > 0
-        - groups[cinder_rabbitmq_telemetry_host_group] != groups[cinder_rabbitmq_host_group]
-    - include: common-tasks/os-log-dir-setup.yml
-      vars:
-        log_dirs:
-          - src: "/openstack/log/{{ inventory_hostname }}-cinder"
-            dest: "/var/log/cinder"
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      when: inventory_hostname == groups['cinder_all'][0]
-    - include: common-tasks/package-cache-proxy.yml
-
-    - name: Add volume group block device to cinder
-      shell: |
-        {% if item.value.volume_group is defined %}
-        if [ "$(pvdisplay | grep -B1 {{ item.value.volume_group }} | awk '/PV/ {print $3}')" ];then
-          for device in `pvdisplay | grep -B1 {{ item.value.volume_group }} | awk '/PV/ {print $3}'`
-            do lxc-device -n {{ container_name }} add $device
-          done
-        fi
-        {% else %}
-        echo "{{ item.key }} volume_group not defined"
-        {% endif %}
-      with_dict: "{{ cinder_backends | default({}) }}"
-        - physical_host != container_name
-        - cinder_backend_lvm_inuse | bool
-      delegate_to: "{{ physical_host }}"
-    - name: udevadm trigger
-      command: udevadm trigger
-      delegate_to: "{{ physical_host }}"
-      when: cinder_backend_lvm_inuse | bool
-  roles:
-    - role: "os_cinder"
-      cinder_storage_address: "{{ storage_address }}"
-    - role: "ceph_client"
-      openstack_service_system_user: "{{ cinder_system_user_name }}"
-      openstack_service_venv_bin: "{{ cinder_bin }}"
-        - inventory_hostname in groups['cinder_volume']
-        - cinder_backend_rbd_inuse | default(false) | bool
-      tags:
-        - ceph
-    - role: "rsyslog_client"
-      rsyslog_client_log_rotate_file: cinder_log_rotate
-      rsyslog_client_log_dir: "/var/log/cinder"
-      rsyslog_client_config_name: "99-cinder-rsyslog-client.conf"
-      tags:
-        - rsyslog
-    - role: "system_crontab_coordination"
-      tags:
-        - crontab
-  vars:
-    is_metal: "{{ properties.is_metal|default(false) }}"
-    cinder_galera_user: cinder
-    cinder_galera_database: cinder
-    cinder_galera_address: "{{ galera_address }}"
BREAKS HERE
-    - osbs-dockercfg-secret
-    - osbs-dockercfg-secret
-    when: env == "staging"
BREAKS HERE
-              url: install.storage.config
-            when: install.storage.config is match("http[s]://.*")
BREAKS HERE
-    - not horizon_external_ssl | bool
-  when: not horizon_external_ssl | bool
-   - not horizon_external_ssl | bool
-   - ansible_pkg_mgr == 'apt'
BREAKS HERE
-odoo_user_sshkeys: False    # ../../path/to/public_keys/*
BREAKS HERE
-      line: "space_left = {{ [rhel7stig_auditd_space_left|int, 51] | min }}"
BREAKS HERE
-  gather_facts: False
-  hosts: all
-    local_action:
BREAKS HERE
-        sequence_no: 20
BREAKS HERE
-      with_dict: cinder_backends|default({})
BREAKS HERE
-      - name: copy deployment folder
-        copy:
-            src: "{{ source_dir }}/"
-            dest: "{{ template_base }}"
-            directory_mode: u=rwX,g=rX,o=rX
-            force: yes
BREAKS HERE
-  vars:
-    ironic_conductor: ironic_conductor
-        - name: "get {{ ironic_conductor }} container ID"
-          command: 'docker ps -f name={{ ironic_conductor }} --format {% raw %}"{{ .ID }}"{% endraw %}'
-          register: conductor_id
-        - name: set the iscsi verify attempt count
-          notify: restart ironic conductor
-      when: "'vqfx-0' in hostvars"
-    - name: restart ironic conductor
-      command: docker restart "{{ conductor_id.stdout }}"
BREAKS HERE
-- name: Install apt packages
BREAKS HERE
-    proxyurl: http://resultsdb01.qa.fedoraproject.org
BREAKS HERE
-        group_when:
-          - "neutron_plugin_type == 'ml2.ovs.dvr'"
-
BREAKS HERE
-  roles:
-      - role: cdn_registery
-        cdn_args_file: "{{ install.cdn|default('') }}"
-        install_version: "{{ install.version }}"
-        when:
-            - "install.cdn|default(False) or cdn_status.rc == 0"
-            - "install.version|openstack_distribution == 'OSP'"
-        become: yes
-
-      - role: rhos-release
-        release: "{{ install.version }}"
-        rr_buildmods: "{{ install.buildmods }}"
-        build: "{{ install.build | default(omit) }}"
-        director_build: "{{ install.get('director', {}).build|default(omit) }}"
-        mirror: "{{ install.mirror | default(omit) }}"
-        when:
-            - "install.version|openstack_distribution == 'OSP'"
-            - "not install.cdn|default(False)"
-            - "cdn_status.rc != 0"
-        become: yes
-
-      - role: rdo-release
-        rr_buildmods: "{{ install.buildmods }}"
-        release: "{{ install.version }}"
-        when: "install.version|openstack_distribution == 'RDO'"
-        become: yes
-        package:
-            state: latest
-            name: ipxe-roms-qemu
-        register: update
BREAKS HERE
-    validate: '/usr/bin/visudo -cf %s'
-    validate: '/usr/bin/visudo -cf %s'
BREAKS HERE
-- name: Setup the undercloud
-  hosts: undercloud
-  gather_facts: no
-  vars:
-    ansible_user: root
-  roles:
-    - undercloud-setup
-
-- name:  Inventory the undercloud instance
-  hosts: localhost
-  gather_facts: yes
-  roles:
-    - tripleo-inventory
-
-- name: Fetch the overcloud images
-  hosts: undercloud
-  gather_facts: no
-  roles:
-   - fetch-images
-
-- name: Build test packages using DLRN
-  hosts: undercloud
-  roles:
-    - {role: build-test-packages, when: build_test_packages|default(false)|bool }
-
-- name: Install the built package on the undercloud
-  hosts: undercloud
-  gather_facts: no
-  roles:
-    - {role: install-built-repo, when: build_test_packages|default(false)|bool }
-
BREAKS HERE
-        version: '{{boxuser.stdout}}'
BREAKS HERE
-## sysctl settings (kernel)
BREAKS HERE
-- name: Allow apache to modify files in /srv/.well-known
-    target: '/srv/.well-known(/.*)?'
-    setype: httpd_git_rw_content_t
-    state: present
-
-- name: Allow apache to read files in /srv/libravatar
-  sefcontext:
-    target: '/srv/libravatar(/.*)?'
BREAKS HERE
-    - {{odcs_gluster_server}}
BREAKS HERE
-    name: "172.18.16.1" # change for your switch name
-          - "portId": "{{ port_id }}"
-            "portName": "{{ port_name }}"
-            "enabled": true
BREAKS HERE
-    - name: Create security group
BREAKS HERE
-        dockerfile: "{{ item.item.dockerfile | default(item.invocation.module_args.dest) }}"
BREAKS HERE
-  when: inventory_hostname == groups['horizon_all'][0]
-    - not horizon_external_ssl|bool
-  when: not horizon_external_ssl|bool
-   - not horizon_external_ssl|bool
-    - horizon-translations-update
BREAKS HERE
-      dest: /tmp/bodhi.debug
BREAKS HERE
-    - name: unset osd flags
-      command: ceph osd unset {{ item }} --cluster {{ cluster }}
-      with_items:
-        - noout
-        - noscrub
-        - nodeep-scrub
-      delegate_to: "{{ groups[mon_group_name][0] }}"
-      when: not containerized_deployment
-    - name: unset containerized osd flags
-      command: |
-          docker exec ceph-mon-{{ hostvars[groups[mon_group_name][0]]['ansible_hostname'] }} ceph osd unset {{ item }} --cluster {{ cluster }}
-      when: containerized_deployment
-      command: ceph --cluster {{ cluster }} versions
-      when: not containerized_deployment
-
-    - name: containers - get osd versions
-      command: |
-        docker exec ceph-mon-{{ hostvars[groups[mon_group_name][0]]['ansible_hostname'] }} ceph --cluster {{ cluster }} versions
-      register: ceph_versions
-      delegate_to: "{{ groups[mon_group_name][0] }}"
-      when: containerized_deployment
-      command: ceph --cluster {{ cluster }} osd require-osd-release luminous
-        - not containerized_deployment
-    - name: containers - complete osds upgrade
-      command: |
-        docker exec ceph-mon-{{ hostvars[groups[mon_group_name][0]]['ansible_hostname'] }} ceph --cluster {{ cluster }} osd require-osd-release luminous
-      delegate_to: "{{ groups[mon_group_name][0] }}"
-      when:
-        - containerized_deployment
-        - (ceph_versions.stdout|from_json).osd | length == 1
-        - ceph_versions_osd | string | search("ceph version 12")
BREAKS HERE
-    http_port: 9080
-    http_port: 9081
-    http_port: 9082
BREAKS HERE
-    - "{{private}}/vars.yml"
-  roles: rabbit/queue
-  username: "faf"
-  queue_name: "faf"
-  routing_keys:
-    - "org.fedoraproject.*.faf.report.threshold1"
-    - "org.fedoraproject.*.faf.report.threshold10"
-    - "org.fedoraproject.*.faf.report.threshold100"
-    - "org.fedoraproject.*.faf.report.threshold1000"
-    - "org.fedoraproject.*.faf.report.threshold1000"
-    - "org.fedoraproject.*.faf.report.threshold10000"
-    - "org.fedoraproject.*.faf.report.threshold100000"
-    - "org.fedoraproject.*.faf.report.threshold1000000"
-    - "org.fedoraproject.*.faf.problem.threshold1"
-    - "org.fedoraproject.*.faf.problem.threshold10"
-    - "org.fedoraproject.*.faf.problem.threshold100"
-    - "org.fedoraproject.*.faf.problem.threshold1000"
-    - "org.fedoraproject.*.faf.problem.threshold1000"
-    - "org.fedoraproject.*.faf.problem.threshold10000"
-    - "org.fedoraproject.*.faf.problem.threshold100000"
-    - "org.fedoraproject.*.faf.problem.threshold1000000"
-   tags:
-    - rabbitmq
BREAKS HERE
-    - module-build-service
-    - python-modulemd
-    - module-build-service
-    - python-modulemd
-    - module-build-service
-    - python-modulemd
-    - module-build-service
-    - python-modulemd
BREAKS HERE
-# $ ansible-playbook exercise2a.yml -i ./ansible-hosts-2a
BREAKS HERE
-  - role: beaker/virthost
BREAKS HERE
-      openshift_hostname: "{{ inventory_hostname }}"
-  - role: ../../../galaxy/infra-ansible/roles/dns
BREAKS HERE
-  lineinfile: dest=/root/.ssh/known_hosts regexp='{{ item.hostname }}' line='{{ item.hostname }} {{ item.signature }}' create=yes owner={{ item.user }} group={{ slaves_group }}
BREAKS HERE
-      ipmi_interface: "eth0"
BREAKS HERE
-    - name: setup environment vars
-      template: src={{ base_dir }}/khaleesi/playbooks/installer/rdo-manager/templates/instack-undercloud-setup-env.j2 dest=~/instack-undercloud-setup-env mode=0755
-
-    - name: Contents of instack-undercloud-setup-env
-      shell: >
-        cat {{ instack_user_home }}/instack-undercloud-setup-env
-
-      shell: >
-       source {{ instack_user_home }}/instack-undercloud-setup-env;
-       openstack undercloud install --debug &> {{ instack_user_home }}/undercloud_install_initial_install.log
-      shell: >
-       source {{ instack_user_home }}/instack-undercloud-setup-env;
-       openstack undercloud install &> {{ instack_user_home }}/undercloud_install_idempotent_check.log
BREAKS HERE
-# Some temporary debugging
-- debug: msg="dist_tag={{ dist_tag }}"
-
BREAKS HERE
-  # Silently fails?
-  # - name: Apply all available patches
-  #   command: /usr/sbin/syspatch
BREAKS HERE
-        when: enable_cinder_backend_lvm | bool }
BREAKS HERE
-## General Neutron configuration
BREAKS HERE
-- name: Installation and setup of Neutron
-  pre_tasks:
-    - include: common-tasks/dynamic-address-fact.yml
-      vars:
-        network_address: "tunnel_address"
-    - include: common-tasks/os-lxc-container-setup.yml
-      # This config is specific to Ubuntu
-      vars:
-        aa_profile: "unconfined"
-        list_of_bind_mounts:
-          - bind_dir_path: "/lib/modules"
-            mount_path: "/lib/modules"
-        extra_container_config:
-          - "lxc.cgroup.devices.allow=a *:* rmw"
-        extra_container_config_no_restart:
-          - "lxc.start.order=79"
-      when:
-        - inventory_hostname in groups['neutron_agent']
-        - ansible_pkg_mgr == 'apt'
-    - include: common-tasks/os-lxc-container-setup.yml
-      # This config is specific to CentOS.
-      vars:
-        aa_profile: "unconfined"
-        list_of_bind_mounts:
-          - bind_dir_path: "/usr/lib/modules"
-            mount_path: "/usr/lib/modules"
-        extra_container_config:
-          - "lxc.cgroup.devices.allow=a *:* rmw"
-      when:
-        - inventory_hostname in groups['neutron_agent']
-        - ansible_pkg_mgr == 'yum'
-    - include: common-tasks/os-lxc-container-setup.yml
-      static: no
-      when: inventory_hostname not in groups['neutron_agent']
-    - include: common-tasks/rabbitmq-vhost-user.yml
-      static: no
-        - inventory_hostname == groups['neutron_all'][0]
-    - include: common-tasks/rabbitmq-vhost-user.yml
-      static: no
-        - inventory_hostname == groups['neutron_all'][0]
-    - include: common-tasks/os-log-dir-setup.yml
-      vars:
-        log_dirs:
-          - src: "/openstack/log/{{ inventory_hostname }}-neutron"
-            dest: "/var/log/neutron"
-    - include: common-tasks/os-log-dir-setup.yml
-      vars:
-        log_dirs:
-          - src: "/openstack/log/{{ inventory_hostname }}-calico"
-            dest: "/var/log/calico"
-      when: "{{ 'neutron_calico_dhcp_agent' in group_names }}"
-    - include: common-tasks/mysql-db-user.yml
-      when: inventory_hostname == groups['neutron_all'][0]
-    - include: common-tasks/package-cache-proxy.yml
-
-    - name: Create the neutron provider networks facts
-      provider_networks:
-        provider_networks: "{{ provider_networks }}"
-        bind_prefix: "{{ bind_prefix }}"
-        is_metal: "{{ is_metal }}"
-      register: pndata
-      tags:
-        - always
-    - name: Set provider network fact(s)
-      set_fact:
-        _provider_networks: "{{ pndata }}"
-      tags:
-        - always
-  roles:
-    - role: "os_neutron"
-      neutron_overlay_network: "{{ _overlay_network }}"
-      neutron_provider_networks: "{{ _provider_networks }}"
-      neutron_local_ip: "{{ tunnel_address }}"
-    - role: "bird"
-      when:
-        - "'neutron_calico_dhcp_agent' in group_names"
-      tags:
-        - bird
-    - role: "openstack_openrc"
-      tags:
-        - openrc
-    - role: "rsyslog_client"
-      rsyslog_client_log_rotate_file: neutron_log_rotate
-      rsyslog_client_log_dir: "/var/log/neutron"
-      rsyslog_client_config_name: "99-neutron-rsyslog-client.conf"
-      rsyslog_client_log_files:
-        - /var/log/conntrackd.log
-        - /var/log/conntrackd-stats.log
-      tags:
-        - rsyslog
-    - role: "rsyslog_client"
-      rsyslog_client_log_rotate_file: calico_log_rotate
-      rsyslog_client_log_dir: "/var/log/calico"
-      rsyslog_client_config_name: "99-calico-rsyslog-client.conf"
-      when: "'neutron_calico_dhcp_agent' in group_names"
-      tags:
-        - rsyslog
-    - role: "system_crontab_coordination"
-      tags:
-        - crontab
-    is_metal: "{{ properties.is_metal|default(false) }}"
-    bind_prefix: "{{ provider_network_bind_prefix|default('') }}"
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - neutron
BREAKS HERE
-        name: myrole
BREAKS HERE
-            msg: "Not enought memory on Hypervisor. Current free memory is {{ ansible_memory_mb.nocache.free }}MB, required is {{ nodes_memory }}MB. Use `--host-memory-overcommit True` if you want to overcommit memory usage"
BREAKS HERE
-    - shell: '~/{{ tester.dir }}/runner.sh {{ tester.testset }} /root/nosetests.xml | tee ~/{{ tester.dir }}.log'
BREAKS HERE
-      retries: 10
BREAKS HERE
-      dest: "{{ ANSIBLE_REPO_PATH }}/workdir/{{cloud_provider}}_cloud_template.{{ env_type }}.{{ guid }}.json"
-      template: "{{ ANSIBLE_REPO_PATH }}/workdir/{{cloud_provider}}_cloud_template.{{ env_type }}.{{ guid }}.json"
-      groups: "{{item['tags']['AnsibleGroup']}},tag_Project_{{env_type| replace('-', '_')}}_{{guid}},tag_{{env_type| replace('-', '_')}}_{{guid}}_{{item['tags'][project_tag]}},tag_{{env_type| replace('-', '_')}}_{{guid}}_ostype_{{item['tags'][project_tag_ostype] | default('unknown')}},{{item['tags'][project_tag_ostype] | default('unknowns')}}"
BREAKS HERE
-    goss_sha256sum: 2f6727375db2ea0f81bee36e2c5be78ab5ab8d5981f632f761b25e4003e190ec
BREAKS HERE
-    dcname: coreos-cincinnatii-stub
BREAKS HERE
-glance_api_servers: "{{ glance_host }}:{{ glance_service_port }}"
BREAKS HERE
-      dest: "/var/tmp/{{ imagename }}"
-      url: "{{ url }}"
-      file: "{{ filename }}"
BREAKS HERE
-# NOTE(mancdaz): rabbitmq cannot be upgraded in serial, so when
-# rabbitmq_upgrade=True, serial is set to 0, else it is 1 for installs
-  serial: "{{ rabbitmq_upgrade|default(false) | bool | ternary(0, 1)}}"
BREAKS HERE
-        log_driver: none
BREAKS HERE
-  hosts: copr-keygen-dev:copr-keygen-stg:copr-keygen
BREAKS HERE
-  hosts: blockerbugs:blockerbugs01.stg.phx2.fedoraproject.org
BREAKS HERE
-    - neutron-pip-packages
BREAKS HERE
-  environment:
-    SDKMAN_DIR: /usr/local/sdkman
BREAKS HERE
-    local_action: shell ssh-keyscan {{ inventory_hostname }} >> {{user_ssh_dir}}/known_hosts
BREAKS HERE
-    - name: Power off the Drive Enclosure
-            name: '0000A66102, bay 1'
-#    - debug: var=drive_enclosure
-    - name: Power on the UID for the Drive Enclosure
-            name: '0000A66102, bay 1'
-#    - debug: var=drive_enclosure
-            name: '0000A66102, bay 1'
-#    - debug: var=drive_enclosure
-            name: '0000A66102, bay 1'
-            refreshState: 'NotRefreshing'
-#    - debug: var=drive_enclosure
BREAKS HERE
-# no_proxy_env: "localhost,127.0.0.1,{% for host in groups['all_containers'] %}{{ hostvars[host]['container_address'] }}{% if not loop.last %},{% endif %}{% endfor %}"
BREAKS HERE
-        - tcmu-runner
BREAKS HERE
-          - "'$template DDF' in (rsyslog_conf.content | b64decode)"
BREAKS HERE
-- debug: msg="{{ansible_nodename}}  {{inventory_hostname}} {{ansible_distribution_major_version|int}}"
-
-  command: hostnamectl set-hostname {{ inventory_hostname }}
-  when: ( ansible_nodename != inventory_hostname ) and ansible_distribution_major_version|int == 7
BREAKS HERE
-nova_consoleauth_init_overrides: {}
-  nova-consoleauth:
-    group: nova_console
-    service_name: nova-consoleauth
-    init_config_overrides: "{{ nova_consoleauth_init_overrides }}"
-    start_order: 3
-    execstarts: "{{ nova_bin }}/nova-consoleauth"
BREAKS HERE
-    - shell: '~/{{ tester.dir }}/runner.sh {{ tester.testset }} /root/nosetests.xml | tee "~/{{ tester.dir }}.log"'
BREAKS HERE
-    package: name={{ item }} state=present
-    with_items:
-    - tar
BREAKS HERE
-      suffix: /var/tmp/badges-tempdir
BREAKS HERE
-- hosts: [ masters, agents, agents_public]
BREAKS HERE
-    - set_fact:
-      when: ceph_docker_image_tag | match("latest") or ceph_docker_image_tag | search("ubuntu")
-    - set_fact:
-      when: ceph_docker_image_tag | search("centos") or ceph_docker_image | search("rhceph") or ceph_docker_image_tag | search("fedora")
BREAKS HERE
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      vars:
-        user_name: "{{ rally_galera_user }}"
-        password: "{{ rally_galera_password }}"
-        login_host: "{{ rally_galera_address }}"
-        db_name: "{{ rally_galera_database_name }}"
-      when: inventory_hostname == groups['utility_all'][0]
-  environment: "{{ deployment_environment_variables | default({}) }}"
BREAKS HERE
-    - include: create-grant-db.yml
-      db_name: "{{ keystone_galera_database }}"
-      db_password: "{{ keystone_container_mysql_password }}"
-  vars_files:
-    - test-vars.yml
-
-  vars_files:
-    - test-vars.yml
-  vars_files:
-    - test-vars.yml
BREAKS HERE
-      # pacemaker managed services
-      - name: start pcs cluster on a node
-        become: true
-        command: pcs cluster start --wait=300
-        when:
-            - pcs_active|failed
-            - pcmk_remote|failed
-
-        retries: 6
BREAKS HERE
-        when: install.version|default(undercloud_version) | openstack_release >= 11
BREAKS HERE
-    replace: dest={{ item }} regexp='^META=all' replace='META={{ volname }}'
BREAKS HERE
-          builder_openshift_url: 'https://172.17.0.1:8443/'
BREAKS HERE
-  default: 'all'
BREAKS HERE
-- include: "playbooks/test-install-openstack-hosts.yml"
BREAKS HERE
-        - inventory_hostname == groups[mon_group_name]|last
BREAKS HERE
-    - Load lxc-openstack apparmor profile
-    - Restart apparmor
BREAKS HERE
-      vhost: /pubsub
-      vhost: /pubsub
BREAKS HERE
-
BREAKS HERE
-  - name: unsilence nagios
-    nagios: action=unsilence service=host host={{ inventory_hostname_short }}{{ env_suffix }}
-    delegate_to: noc01.phx2.fedoraproject.org
-    ignore_errors: true
BREAKS HERE
-      - plugin: mapper-murmur3
BREAKS HERE
-    # - okapi
-    # - mod-users
-    # - mod-users-data
-    # - mod-metadata
-    # - mod-metadata-data
BREAKS HERE
-      nics: "{{ nics.results | map(attribute='stdout') | list }}"
BREAKS HERE
-            user: "{{ hostvars['hypervisor'].ansible_user }}"
BREAKS HERE
-        - noscrub
-        - nodeep-scrub
-        - noscrub
-        - nodeep-scrub
-        ((ceph_health_post.stdout | from_json).pgmap.pgs_by_state | length) == 1
-        and
-        (ceph_health_post.stdout | from_json).pgmap.pgs_by_state.0.state_name == "active+clean"
-        - noscrub
-        - nodeep-scrub
BREAKS HERE
-security_package_state: "latest"
BREAKS HERE
-
-
-- name: Request Let's Encrypt Wildcard Certificates
-  hosts: bastions[0]
-  roles:
-    - role: "{{ ANSIBLE_REPO_PATH }}/roles/lets-encrypt"
-      when: install_lets_encrypt_certificates|bool
BREAKS HERE
-        dest: generated_report.html
BREAKS HERE
-    image_name: "copr-builder-ppc64le-f27-new-kernel"
-  - name: create mock default.cfg
-    command: ln -f -s /etc/mock/fedora-27-ppc64le.cfg /etc/mock/default.cfg
-
BREAKS HERE
-  # prepare prerequisites which is used in this playbook
-  pre_tasks:
-    - name: Add a System Api Roles SELinux User
-        comment: System Api Roles SELinux User
-  roles:
-    - selinux
BREAKS HERE
-    shell: mysqladmin -u root password "{{setmysqlrootpassword}}"
BREAKS HERE
-- name: WP installed?
-  command: wp core is-installed --allow-root
-  args:
-    chdir: "{{ www_root }}/{{ item.key }}/current/"
-  register: site_statuses
-  with_dict: wordpress_sites
-  changed_when: False
-  failed_when: site_statuses.stderr != ""
-
-           --url="{{ item.item.value.env.wp_home }}"
-           --title="{{ item.item.value.site_title | default(item.item.key) }}"
-           --admin_user="{{ item.item.value.admin_user }}"
-           --admin_password="{{ item.item.value.admin_password }}"
-           --admin_email="{{ item.item.value.admin_email }}"
-    chdir: "{{ www_root }}/{{ item.item.key }}/current/"
-  with_items: site_statuses.results
-  when: item.rc == 1 and item.item.value.site_install == True and (item.item.value.multisite.enabled | default(False) == False)
-           --url="{{ item.item.value.env.wp_home }}"
-           --base="{{ item.item.value.multisite.base_path | default('/') }}"
-           --subdomains="{{ item.item.value.multisite.subdomains | default('false') }}"
-           --title="{{ item.item.value.site_title | default(item.item.key) }}"
-           --admin_user="{{ item.item.value.admin_user }}"
-           --admin_password="{{ item.item.value.admin_password }}"
-           --admin_email="{{ item.item.value.admin_email }}"
-    chdir: "{{ www_root }}/{{ item.item.key }}/current/"
-  with_items: site_statuses.results
-  when: item.rc == 1 and item.item.value.site_install == True and (item.item.value.multisite.enabled | default(False) == True)
BREAKS HERE
-      failed_when: "{{ result.rc not in [0, 2] }}"
BREAKS HERE
-# it is best to include config/general_config/devmode.yml rather than
BREAKS HERE
-    - name: MODIFY VIP MAP CHANGE EXTERNAL IP - CHANGE
BREAKS HERE
-
-
BREAKS HERE
-  - debug:
-      msg: "Step 000 Pre Infrastructure - Dummy action"
BREAKS HERE
-      command: ceph tell mon.{{ ansible_hostname }} compact
-        ceph -s | grep monmap | sed 's/.*quorum//' | egrep -sq {{ ansible_hostname }}
-      command: ceph osd set {{ item }}
-        test "$(ceph pg stat | sed 's/^.*pgs://;s/active+clean.*//;s/ //')" -eq "$(ceph pg stat | sed 's/pgs.*//;s/^.*://;s/ //')" && ceph health | egrep -sq "HEALTH_OK|HEALTH_WARN"
-      command: ceph osd unset {{ item }}
BREAKS HERE
-  tags: security,fail2ban
BREAKS HERE
-  - name: Install Common Packages (Takes Awhile)
-    apt:
-      name:
-      - fuse
-      - nano
-      - fail2ban
-      - wget
-      - lsb-release
-      - figlet
-      - update-notifier-common
-      - software-properties-common
-      - unrar
-      - unzip
-      - glances
-      - python-pip
-      - python3-pip
-      - python-passlib
-      - zip
-      - curl
-      - man-db
-      - htop
-      - openssh-server
-      - dirmngr
-      - npm
-      - zip
-      - apt-transport-https
-      - ca-certificates
-      - tree
-      - ncdu
-      - ctop
-      - dialog
-      - dnsutils
-      - mc
-      - apache2-utils
-      - lsof
-      - pwgen
-      - gawk
-      - python-lxml
-      - acl
-      - bc
-    #ignore_errors: yes
BREAKS HERE
-    - role: mortik.nginx-rails # Install nginx. If missing run: ansible-galaxy install mortik.nginx-rails
-        - { src: "templates/nginx.conf.j2", dest: "{{ nginx_path }}/nginx.conf", mode: "644" }
-        - { src: "templates/ssl.js.j2", dest: "{{ nginx_path }}/{{ app }}_ssl", mode: "644" }
BREAKS HERE
-
-    - name: Overwrite ansible hosts file with lab hosts file
-      template:
-        src: "../../configs/{{ env_type }}/files/labs_hosts_template.j2"
-        dest: "../../workdir/labs_hosts-{{ env_type }}-{{ guid }}"
BREAKS HERE
-      register: fact_results
-      register: fact_results
-      register: fact_results
-      register: fact_results
BREAKS HERE
-      when: ansible_os_family != 'RedHat'
-    - name: restart rados gateway server(s)
-      when: ansible_os_family != 'RedHat'
BREAKS HERE
-    galera_wsrep_node_name: "{{ container_name }}"
BREAKS HERE
-      and
-      and
BREAKS HERE
-- name: Create directory and restart tomcat
-  hosts: wildflyservers
BREAKS HERE
-      - /var/lib/ceph
BREAKS HERE
-    galaxy_hostname: "{{ ansible_env.INSTALL_HOSTNAME|default('localhost') }}"
-    galaxy_user: "{{ ansible_env.GALAXY_USER|default('galaxy') }}"
-    galaxy_admin: "{{ ansible_env.GALAXY_ADMIN|default('artimed@gmail.com') }}"
-    galaxy_data: "{{ ansible_env.GALAXY_DATA|default('/home/galaxy/galaxy') }}" #"/galaxy_data"
-    galaxy_port: "{{ ansible_env.GALAXY_PORT|default('8080') }}"
-    ftp_port: "{{ ansible_env.FTP_PORT|default('21') }}"
-    galaxy_api: "{{ ansible_env.GALAXY_KEY|default('379ab2a47d714a74b5cee4081703368e') }}"
-#    - role: postgresql.movedb
-#      galaxy_database: "/galaxy_database"
-#      galaxy_user_name: "{{ galaxy_user }}"      
BREAKS HERE
-      register: container1_bind_mount
-      failed_when: container1_bind_mount.rc != 0
-    - name: Check for the lack of presence of a bound mount for container2
-      command: grep "lxc.mount.entry = /openstack/container2 opt/test1 none bind 0 0" /var/lib/lxc/container2/config
-      register: container2_bind_mount
-      failed_when: container2_bind_mount.rc == 0
-    - name: Check for the lack of presence of a bound mount for container3
-      command: grep "lxc.mount.entry = /openstack/container3 opt/test1 none bind 0 0" /var/lib/lxc/container3/config
BREAKS HERE
-- name: Copy etc hosts file to all hosts
BREAKS HERE
-  - { role: openshift/object, vars: {app: greenwave, template: imagestreamtag.yml, when : env == 'staging' }}
BREAKS HERE
-      shell: "git remote set-url origin https://github.com/openstack/keystone.git"
BREAKS HERE
-      - include: tasks/loadbalancer.yml
-        when: "not overcloud_deploy_script|default('') and groups.loadbalancer is defined"
-
BREAKS HERE
-    - name: Copying virtual interface config file
-      template: src=./templates/virtualIface dest=/etc/network/interfaces.d/virtualIface force=yes
-    
-    - name: Restarting networking service
-      service: name=networking state=restarted
BREAKS HERE
-    freshmaker_servername: {{inventory_hostname}}
-  - freshmaker/backend
BREAKS HERE
-- hosts: [ masters, agents, agents_public]
BREAKS HERE
-  with_items: "{{ rhel_07_010210_audit.stdout_lines }}"
-  with_items: "{{ rhel_07_010230_audit.stdout_lines }}"
-  with_items: "{{ rhel_07_040640_audit.files }}"
-  with_items: "{{ rhel_07_040650_audit.files }}"
BREAKS HERE
-- name: Install neutron distro packages
-  register: install_packages
-  until: install_packages|success
-  retries: 5
-  delay: 2
-  with_items: "{{ neutron_distro_packages }}"
-
-- name: Install distro packages for LXB
-  package:
-    name: "{{ item }}"
-    state: "{{ neutron_package_state }}"
-  register: install_packages
-  until: install_packages|success
-  retries: 5
-  delay: 2
-  with_items: "{{ neutron_lxb_distro_packages }}"
-  when:
-    - neutron_services['neutron-linuxbridge-agent']['group'] in group_names
-    - neutron_services['neutron-linuxbridge-agent'].service_en | bool
-
-- name: Install distro packages for OVS
-  package:
-    name: "{{ item }}"
-    state: "{{ neutron_package_state }}"
-  register: install_packages
-  until: install_packages|success
-  retries: 5
-  delay: 2
-  with_items: "{{ neutron_ovs_distro_packages }}"
-  when:
-    - (neutron_services['neutron-openvswitch-agent']['group'] in group_names and
-       neutron_services['neutron-openvswitch-agent'].service_en | bool) or
-      (neutron_services['dragonflow-controller-agent']['group'] in group_names and
-       neutron_services['dragonflow-controller-agent'].service_en | bool) or
-      (neutron_services['dragonflow-l3-agent']['group'] in group_names and
-       neutron_services['dragonflow-l3-agent'].service_en | bool)
-
-- name: Install distro packages for LBaaS
-  package:
-    name: "{{ item }}"
-    state: "{{ neutron_package_state }}"
-  register: install_packages
-  until: install_packages|success
-  retries: 5
-  delay: 2
-  with_items: "{{ neutron_lbaas_distro_packages }}"
-  when:
-    - neutron_services['neutron-lbaasv2-agent']['group'] in group_names
-    - neutron_lbaasv2 | bool
-
-- name: Install distro packages for VPNaaS
-  package:
-    name: "{{ item }}"
-    state: "{{ neutron_package_state }}"
-  register: install_packages
-  until: install_packages|success
-  retries: 5
-  delay: 2
-  with_items: "{{ neutron_vpnaas_distro_packages }}"
-    - neutron_services['neutron-vpnaas-agent']['group'] in group_names
-    - neutron_vpnaas | bool
BREAKS HERE
-  no_log: "{{ not lookup('env', 'MOLECULE_DEBUG') | bool }}"
BREAKS HERE
-      when: hostvars[groups['mdm'][2]] is defined
-      when: hostvars[groups['mdm'][2]] is defined
-      when: hostvars[groups['tb'][1]] is defined
-      when: hostvars[groups['tb'][1]] is defined
-      when: scaleio_mdm_tertiary_ip is not defined
-      when: scaleio_mdm_tertiary_ip is defined
-      when: hostvars[groups['gateway'][1]] is defined
BREAKS HERE
-  pre_tasks:
-    - include_role:
-        name: check_secrets
-
BREAKS HERE
-    task_path: tasks
-    - name: Task in level 2
-        msg: "Task in level 2"
BREAKS HERE
-    - name: Fail if there is no Ansible inventory group matching the blueprint's host_vars
BREAKS HERE
-      for id in $(ls /var/lib/ceph/osd/ |grep -oh '[0-9]*'); do
BREAKS HERE
-        - m_startup
-        - cli_startup
BREAKS HERE
-        src: "{{ groups.gluster[0] }}:/{{ gluster_brick_name }}"
BREAKS HERE
-- name: Disable cache for apt update for hosts
-  copy:
-    content: |
-      Acquire::http::No-Cache true;
-    dest: "/etc/apt/apt.conf.d/00apt-no-cache"
-  tags:
-    openstack_hosts-config
-  when:
-    - ansible_pkg_mgr == 'apt'
-    - >
-      global_environment_variables.http_proxy is defined or
-      global_environment_variables.HTTP_PROXY is defined or
-      global_environment_variables.https_proxy is defined or
-      global_environment_variables.HTTPS_PROXY is defined
-
BREAKS HERE
-      - name: "update python-tripleoclient rpm for osp13"
-        shell: |
-            yum localinstall -y http://download.eng.bos.redhat.com/brewroot/vol/rhel-7/packages/python-tripleoclient/9.2.7/9.el7ost/noarch/python-tripleoclient-9.2.7-9.el7ost.noarch.rpm
-        when: "install.version|openstack_release == 13"
BREAKS HERE
-    - name: Setup clouds.yaml
BREAKS HERE
-      nics: "{{ resources.networks | map(attribute='id') | list | map('add_prefix', 'net-id=') | list }}"
BREAKS HERE
-  tasks:
BREAKS HERE
-    - name: Ensure phpMyAdmin is running on the specified port.
BREAKS HERE
-          version: 8.0.202-zulu
-          version: '3.5.1'
-        gradle: '4.6'
BREAKS HERE
-odoo_databases: []      # [{name: prod, locale: 'en_US.UTF-8'}]
BREAKS HERE
-
-- name: install Grafana
-  import_playbook: /usr/share/ansible/openshift-ansible/playbooks/openshift-grafana/config.yml
-  ignore_errors: true
-  tags: 
-    - logging-metrics
-    - ocp-deploy
-
-  ignore_errors: true
BREAKS HERE
-  post_tasks:
-      mode: 0644
BREAKS HERE
-    certbot_addhost: pkgs02.fedoraproject.org
BREAKS HERE
-    when: datacenter != 'staging'
-  - koji_hub
BREAKS HERE
-    - include: common-tasks/mysql-db-user.yml
-      static: no
-      vars:
-        user_name: "{{ horizon_galera_user }}"
-        password: "{{ horizon_container_mysql_password }}"
-        login_host: "{{ horizon_galera_address }}"
-        db_name: "{{ horizon_galera_database }}"
-      when: inventory_hostname == groups['horizon_all'][0]
-
-  vars:
-    horizon_galera_user: horizon
-    horizon_galera_database: horizon
-    horizon_galera_address: "{{ galera_address }}"
-  environment: "{{ deployment_environment_variables | default({}) }}"
-  tags:
-    - horizon
BREAKS HERE
-  roles:
-    - validate-build-and-import
-    - validate-start-stop-restart
-    - validate-config
BREAKS HERE
-            hostname : '{{ server_hardware_hostname }}'
BREAKS HERE
-      when: hostvars["localhost"].rpm_build_rc == 0
-      when: hostvars["localhost"].rpm_build_rc == 0
BREAKS HERE
-  when: "'python34-fedmsg' not in group_names"
BREAKS HERE
-      when: etc_ar_stat.stat.exists and usr_ar_stat.stat.checksum != etc_ar_stat.stat.checksum
BREAKS HERE
-      tar_images: "{{ install.images.files | default(tar_files[install.version|openstack_release]) | default(tar_files[8])}}"
BREAKS HERE
-  default: False
-    - If 'false' it will allow for the group name but warn about the issue.
-    - When 'true' it will replace any invalid charachters with '_' (underscore).
-  type: bool
BREAKS HERE
-    - name: Create a basic web security group (TCP 22, 88, 443 IN)
BREAKS HERE
-        host: "{{ hostvars[groups['cluster-metrics'][0]]['ansible_host'] }}"
BREAKS HERE
-  - debug: msg="root auth users {{}}"
BREAKS HERE
-  - name: bobbins
-    action: command echo ${oldskool}
-
-  - name: blah
-    action: debug oops a missing {{bracket}
-
BREAKS HERE
-  when: vault_tls_disable == 0
BREAKS HERE
-        _customs: "{{customs}}"
BREAKS HERE
-   - { role: fedmsg/relay, tags: ['fedmsg_relay', 'fedmsg'] }
BREAKS HERE
-# neutron system services
-- include: neutron_init_common.yml
BREAKS HERE
-- name: "Workaround RHBZ#1265306 updated sqlalchemy breaks rhos-5 db_sync"
-  hosts: openstack_nodes
-  gather_facts: no
-  sudo: yes
-  tasks:
-      - name: exclude python-sqlalchemy from rhel-7.2 repo for rhos-5.0
-        command: yum-config-manager --save --setopt=rhelosp-rhel-7-server.exclude=python-sqlalchemy
-        when: (workarounds['rhbz1265306']['enabled'] is defined and workarounds['rhbz1265306']['enabled'] | bool) and
-              product.version.code_name == 'icehouse'
-
BREAKS HERE
-      
BREAKS HERE
-
-  - debug: msg="{{ result.msg }}"
BREAKS HERE
-- name: Slurm common Playbook
-  hosts: slurm_master:slurm_clients
-  vars:
-    slurm_src_baseurl: "{{slurm_source_baseurl|default('http://www.schedmd.com/download/latest/')}}"
-    slurm_src_filename: "{{slurm_source_filename|default('slurm-15.08.4.tar.bz2')}}"
-    slurm_pkg_name: "{{slurm_package_name|default('slurm-15.08.4-1.el6.x86_64.rpm')}}"
-  roles:
-    - { role: iptables, default_input_policy: 'ACCEPT' }
-  tasks:
-    - set_fact:
-        slurm_name: slurm-llnl
-        slurmdbd_name: slurm-llnl-slurmdbd
-      when: is_debian_or_ubuntu
-      tags: slurm
-
-    - set_fact:
-        slurm_name: slurm
-        slurmdbd_name: slurmdbd
-      when: is_centos
-      tags: slurm
-
-    - include: slurm/tasks/main.yml
-    - include: cluster/tasks/packages.yml
-  handlers:
-    - include: common/handlers/main.yml
-    - include: slurm/handlers/main.yml
-
-
-  vars:
-    slurmdbd_mysql_db: "{{slurm_mysql_db|default('slurm')}}"
-    slurmdbd_mysql_user: "{{slurm_mysql_user|default('slurm')}}"
-    slurmdbd_mysql_pwd: "{{slurm_mysql_pwd|default('ua7diKee')}}"
-  tasks: 
-    - include: common/tasks/nfs.yml
-
-    - nfsexport: path=/home dest=/etc/exports clients="{{groups.slurm_clients}}" options=rw,no_root_squash,async state=exported
-      notify: 
-        - ensure nfs service is running
-        - reload exports
-
-      tags:
-        - nfs
-
-    - include: slurm/tasks/master.yml
-  handlers:
-    - include: common/handlers/main.yml
-    - include: slurm/handlers/main.yml
-  hosts: slurm_clients
-  tasks: 
-    - include: common/tasks/nfs-clients.yml nfsserver={{groups.slurm_master[0]}} nfspath=/home nfsmount=/home
-  handlers:
-    - include: common/handlers/main.yml
-    - include: slurm/handlers/main.yml
-
-# Get infos with
-# ansible -i hostsfile -m setup hostname
BREAKS HERE
-      ((continuous_integration|bool)|default(false,true))
-      ((continuous_integration|bool)|default(false,true))
BREAKS HERE
-    - { role: repo-setup, repo_inject_image_path: "$HOME/ironic-python-agent.initramfs", repo_run_live: false, initramfs_image: true, libguestfs_mode: false, when: not to_build|bool }
-    - { role: install-built-repo, ib_repo_image_path: "$HOME/ironic-python-agent.initramfs", initramfs_image: true, libguestfs_mode: false, when: compressed_gating_repo is defined and not to_build|bool }
-
BREAKS HERE
-    - when: install_zabbix_bool
BREAKS HERE
-      register: result
-    - debug: var=result
-      register: result
-    - debug: var=result
BREAKS HERE
-        when: "{{ install.images.packages|default('') or install.images['update'] }}"
BREAKS HERE
-  become: false
BREAKS HERE
-DUPLICATE_DICT_KEY_WARNINGS:
-  name: Toggle warnings for duplicate dict keys in YAML
-  default: True
-  env: [{name: ANSIBLE_DUPLICATE_DICT_KEY_WARNINGS}]
-  - {key: duplicate_dict_key_warnings, section: defaults}
-  type: boolean
BREAKS HERE
-        src: "{{files}}/osbs/fedora-dnsmasq.{{env}}"
BREAKS HERE
-      when: az_resultis failed
-      when: rwait is failed is succeeded
-      when: rwait is failed is succeeded
-      when: rwait is failed is succeeded
-      when: rwait is failed is succeeded
BREAKS HERE
-  description: Toggle Ansbile logging to syslog on the target when it executes tasks.
BREAKS HERE
-  when: download_result | changed 
-  when: download_result | changed
BREAKS HERE
-            selinux_problems: "{{ selinux_problems | int + ( hostvars[item]['selinux_problems_found'] | default([]) | length ) }}"
BREAKS HERE
-  when: swift_do_sync | bool
-  when: swift_do_sync | bool
BREAKS HERE
-  name: Set the user you 'become' via privlege escalation
BREAKS HERE
-- name: Deploy OpenShift Cluster
-  hosts: osbs-control:osbs-control-stg
-  tags:
-    - osbs-deploy-openshift
-  user: root
-  gather_facts: True
-
-  vars_files:
-    - /srv/web/infra/ansible/vars/global.yml
-    - "/srv/private/ansible/vars.yml"
-    - /srv/web/infra/ansible/vars/{{ ansible_distribution }}.yml
-
-  roles:
-    - {
-      role: ansible-ansible-openshift-ansible,
-        cluster_inventory_filename: "cluster-inventory-stg",
-        openshift_htpasswd_file: "/etc/origin/htpasswd",
-        openshift_master_public_api_url: "https://{{ osbs_url }}:8443",
-        openshift_release: "v3.6.0",
-        openshift_ansible_path: "/root/openshift-ansible",
-        openshift_ansible_playbook: "playbooks/byo/config.yml",
-        openshift_ansible_version: "release-3.6-fedora-compat",
-        openshift_ansible_ssh_user: root,
-        openshift_ansible_install_examples: false,
-        openshift_ansible_containerized_deploy: false,
-        openshift_cluster_masters_group: "osbs-masters-stg",
-        openshift_cluster_nodes_group: "osbs-nodes-stg",
-        openshift_cluster_infra_group: "osbs-masters-stg",
-        openshift_auth_profile: "osbs",
-        openshift_cluster_url: "{{osbs_url}}",
-        openshift_master_ha: false,
-        openshift_debug_level: 2,
-        openshift_shared_infra: true,
-        openshift_deployment_type: "origin",
-        openshift_ansible_python_interpreter: "/usr/bin/python3",
-      when: env == 'staging',
-      tags: ['openshift-cluster','ansible-ansible-openshift-ansible']
-    }
-    - {
-      role: ansible-ansible-openshift-ansible,
-        cluster_inventory_filename: "cluster-inventory",
-        openshift_htpasswd_file: "/etc/origin/htpasswd",
-        openshift_master_public_api_url: "https://{{ osbs_url }}:8443",
-        openshift_release: "v3.6.0",
-        openshift_ansible_path: "/root/openshift-ansible",
-        openshift_ansible_playbook: "playbooks/byo/config.yml",
-        openshift_ansible_version: "release-3.6-fedora-compat",
-        openshift_ansible_ssh_user: root,
-        openshift_ansible_install_examples: false,
-        openshift_ansible_containerized_deploy: false,
-        openshift_cluster_masters_group: "osbs-masters",
-        openshift_cluster_nodes_group: "osbs-nodes",
-        openshift_cluster_infra_group: "osbs-masters",
-        openshift_auth_profile: "osbs",
-        openshift_cluster_url: "{{osbs_url}}",
-        openshift_master_ha: false,
-        openshift_debug_level: 2,
-        openshift_shared_infra: true,
-        openshift_deployment_type: "origin",
-        openshift_ansible_python_interpreter: "/usr/bin/python3",
-      when: env == 'production',
-      tags: ['openshift-cluster','ansible-ansible-openshift-ansible']
-    }
-
BREAKS HERE
-    - name: restart ceph mdss with upstart
-        state: restarted
-    - name: restart ceph mdss with sysvinit
-        state: restarted
-    - name: restart ceph mdss with systemd
-        state: restarted
BREAKS HERE
-    vars:
-      webserver_nginx: true
-    run_once: true
-      - path: "{{ item.context }}"
BREAKS HERE
-        tags: ['never', 'grafana', 'pservice', 'docker', 'docker-container', 'full_setup']
BREAKS HERE
-    resource_group: createvmsubnetin2ndrg
-  roles:
-    - Azure.azure_preview_modules
BREAKS HERE
