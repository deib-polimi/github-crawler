-  vnet = "${lookup(var.vm_info,"vnet","")}"
-    vnet = "${local.vnet}"
-    public_key = ""
BREAKS HERE
-  vpn_gateway_id                  = "${module.vpc.vgw_id}"
-  customer_gateway_id             = "${aws_customer_gateway.main.id}"
-  vpc_id                          = "${module.vpc.vpc_id}"
-  vpc_subnet_route_table_ids      = ["${module.vpc.private_route_table_ids}"]
-    Name = "main-customer-gateway"
-  public_subnets  = ["10.10.11.0/24", "10.10.12.0/24", "10.10.13.0/24"]
BREAKS HERE
-  count = "${(var.availability_domains["bastion"] == 1 && var.install_calico == true)   ? 1 : 0}"
-      "chmod +x ~/install_calico.sh",
-      "~/install_calico.sh",
-  count = "${(var.availability_domains["bastion"] == 1 && var.install_calico == true)   ? 1 : 0}"
BREAKS HERE
-  name = "stage-socialmobileregistry."
BREAKS HERE
-  region = "eu-west-1"
-  availability_zone = "eu-west-1a"
-  availability_zone = "eu-west-1a"
-  availability_zone = "eu-west-1b"
-  availability_zone = "eu-west-1b"
-  availability_zone = "eu-west-1c"
-  availability_zone = "eu-west-1c"
BREAKS HERE
-  name = "${var.hostname}.${data.aws_route53_zone.zone.name}"
-  name               = "${var.hostname}.${data.aws_route53_zone.zone.name}-bastion-role"
-  name   = "${var.env}-bastion-policy"
BREAKS HERE
-    domain_name = "viewsourceconftage3-website-us-west-2.amazonaws.com"
BREAKS HERE
-    domain  = "${var.domain_name}"
-    dnsname = "${var.dnsname}.${var.dnszone}"
BREAKS HERE
-  owners = ["383156758163"]
-
-  count = "3"
-  count = "3"
-  count = "3"
-  count = 3
-    name           = "${element(formatlist("master-%s", data.aws_availability_zones.available.names),count.index)}"
-    cluster-name   = "${var.name}"
-    kubernetes_ami = "${data.aws_ami.kubernetes_ami.name}"
-    subnets        = "${element(formatlist("  - master-%s", data.aws_availability_zones.available.names),count.index)}"
-    instance_type  = "${var.master_instance_type}"
-    name          = "workers"
-    cluster-name  = "${var.name}"
-    subnets       = "${join("\n", formatlist("  - worker-%s", data.aws_availability_zones.available.names))}"
-    instance_type = "${var.worker_instance_type}"
-    min           = "${length(data.aws_availability_zones.available.names)}"
-    max           = "${var.max_amount_workers}"
-
-  count = "3"
-    name                  = "${var.name}"
-    k8s_version           = "${var.k8s_version}"
-    vpc_id                = "${data.aws_vpc.vpc_for_k8s.id}"
-    vpc_cidr              = "${data.aws_vpc.vpc_for_k8s.cidr_block}"
-    elb_type              = "${var.elb_type}"
-    k8s_data_bucket       = "${var.k8s_data_bucket}"
-    oidc_issuer_url       = "${var.oidc_issuer_url}"
-    etcd_members_main     = "${join("\n",data.template_file.cluster-etcd-member-spec.*.rendered)}"
-    etcd_members_events   = "${join("\n",data.template_file.cluster-etcd-member-spec.*.rendered)}"
-    master_subnets        = "${join("\n",data.template_file.master-subnet-spec.*.rendered)}"
-    worker_subnets        = "${join("\n",data.template_file.worker-subnet-spec.*.rendered)}"
-    utility_subnets       = "${join("\n",data.template_file.utility-subnet-spec.*.rendered)}"
-    master_instance_group = "${join("\n",data.template_file.master-instancegroup-spec.*.rendered)}"
-    worker_instance_group = "${data.template_file.worker-instancegroup-spec.rendered}"
-    content = "${data.template_file.cluster-spec.rendered}"
-      ${data.template_file.cluster-spec.rendered}
BREAKS HERE
-  name_filter  = "4.10.8 std #1"
BREAKS HERE
-  min_master_version = "${data.google_container_engine_versions.default.latest_node_version}"
BREAKS HERE
-    instance = "${aws_instance.nat.id}"
-    vpc = true
-  ami = "${lookup(var.amis, var.aws_region)}"
-  key_name = "${aws_key_pair.deployer.key_name}"
-      "sudo docker run --volumes-from ovpn-data --rm kylemanna/openvpn ovpn_genconfig -p ${var.vpc_cidr} -u udp://${aws_instance.nat.public_ip}"
BREAKS HERE
-    name = "k8s-etcd"
-    size = "512mb"
-    user_data = "${file("00-etcd.yaml")}"
-    ssh_keys = [
-        "${var.ssh_fingerprint}"
-    ]
-            $PWD/cfssl/generate_ca.sh
-            $PWD/cfssl/generate_server.sh k8s_etcd ${digitalocean_droplet.k8s_etcd.ipv4_address_private}
-    template = "${file("01-master.yaml")}"
-    name = "k8s-master"
-    size = "512mb"
-    ssh_keys = [
-        "${var.ssh_fingerprint}"
-    ]
-            $PWD/cfssl/generate_server.sh k8s_master "${digitalocean_droplet.k8s_master.ipv4_address},${digitalocean_droplet.k8s_master.ipv4_address_private},10.3.0.1,kubernetes.default,kubernetes"
-            $PWD/cfssl/generate_client.sh k8s_master
-    template = "${file("02-worker.yaml")}"
-    name = "${format("k8s-worker-%02d", count.index + 1)}"
-    size = "512mb"
-    ssh_keys = [
-        "${var.ssh_fingerprint}"
-    ]
-            $PWD/cfssl/generate_client.sh k8s_worker
-            $PWD/cfssl/generate_admin.sh
- 
-            kubectl create -f 03-dns-addon.yaml
-            sed -e "s/\$EXT_IP1/${digitalocean_droplet.k8s_worker.0.ipv4_address}/" < 04-microbot.yaml > ./secrets/04-microbot.rendered.yaml
BREAKS HERE
-    key = "Name"
-    value = "worker"
-    key = "builtWith"
-    value = "terraform"
-    key = "Cluster"
BREAKS HERE
-  template = "${file("${format("%s/../scripts/gceme.sh.tpl", path.module)}")}"
BREAKS HERE
-  name     = "aks-vnetrg"
-module "aks-flux" {
BREAKS HERE
-  name           = "rmd_net"
-  name            = "rmd_net_sub1"
-  dns_nameservers ="${var.dns_nameservers}"
-  name                = "rmd_router"
-resource "openstack_networking_router_interface_v2" "router" {
-resource "openstack_compute_secgroup_v2" "rmd_security" {
-  name        = "rmd_security"
-  description = "rmd_security"
BREAKS HERE
-#---------------------------------------------------------
-# SGs for access to concourse servers. One for the web farm
-# and another for SSH access and another for DB access.
-#---------------------------------------------------------
-  name        = "conc-web-sg-${data.aws_region.current.name}"
-  description = "Security group for all concourse web servers in ${data.aws_region.current.name}."
-    from_port = 8080
-    to_port   = 8080
-resource "aws_security_group_rule" "allow_worker_to_register" {
-  type                     = "ingress"
-  from_port                = 2222
-  to_port                  = 2222
-  protocol                 = "tcp"
-  source_security_group_id = "${aws_security_group.worker_sg.id}"
-  security_group_id        = "${aws_security_group.web_sg.id}"
-
-  depends_on = ["aws_security_group.worker_sg", "aws_security_group.web_sg"]
-}
-resource "aws_security_group_rule" "allow_worker_to_web" {
-  type                     = "ingress"
-  from_port                = 8080
-  to_port                  = 8080
-  protocol                 = "tcp"
-  source_security_group_id = "${aws_security_group.worker_sg.id}"
-  security_group_id        = "${aws_security_group.web_sg.id}"
-  name        = "conc-worker-sg-${data.aws_region.current.name}"
-  description = "Opens all the appropriate concourse worker ports in ${data.aws_region.current.name}"
-  name        = "conc-lb-sg-${data.aws_region.current.name}"
-  description = "Security group for the LB in ${data.aws_region.current.name}."
BREAKS HERE
-  k8s_app_namespace = "${kubernetes_namespace.haystack-app-namespace.metadata.name}"
-  k8s_app_namespace = "${kubernetes_namespace.haystack-app-namespace.metadata.name}"
-  k8s_app_namespace = "${kubernetes_namespace.haystack-app-namespace.metadata.name}"
BREAKS HERE
-  source              = "terraform-aws-modules/security-group/aws"
-  name                = "${module.airflow_labels.id}-sg"
-  description         = "Security group for ${module.airflow_labels.id} machines"
-  vpc_id              = "${data.aws_vpc.default.id}"
-  ingress_rules       = ["http-80-tcp", "https-443-tcp", "ssh-tcp"]
-      from_port   = 8080
-      to_port     = 8080
-      protocol    = "tcp"
-      from_port   = 5555
-      to_port     = 5555
-      protocol    = "tcp"
-  instance_type          = "${var.webserver_instance_type}"
-  ami                    = "${var.ami}"
-  key_name               = "${aws_key_pair.auth.id}"
-  subnet_id              = "${element(data.aws_subnet_ids.selected.ids, 0)}"
-  iam_instance_profile   = "${module.ami_instance_profile.instance_profile_name}"
-    volume_type           = "${var.root_volume_type}"
-    volume_size           = "${var.root_volume_size}"
-    content     = "${data.template_file.custom_env.rendered}"
-      agent       = false
-      type        = "ssh"
-      user        = "ubuntu"
-    content     = "${data.template_file.custom_requirements.rendered}"
-      agent       = false
-      type        = "ssh"
-      user        = "ubuntu"
-    content     = "${data.template_file.airflow_environment.rendered}"
-      agent       = false
-      type        = "ssh"
-      user        = "ubuntu"
-    content     = "${data.template_file.airflow_service.rendered}"
-      agent       = false
-      type        = "ssh"
-      user        = "ubuntu"
-      agent       = false
-      type        = "ssh"
-      user        = "ubuntu"
-  tags      = "${module.airflow_labels_webserver.tags}"
-  instance_type          = "${var.scheduler_instance_type}"
-  ami                    = "${var.ami}"
-  key_name               = "${aws_key_pair.auth.id}"
-  subnet_id              = "${element(data.aws_subnet_ids.selected.ids, 0)}"
-  iam_instance_profile   = "${module.ami_instance_profile.instance_profile_name}"
-    volume_type           = "${var.root_volume_type}"
-    volume_size           = "${var.root_volume_size}"
-    content     = "${data.template_file.custom_env.rendered}"
-      agent       = false
-      type        = "ssh"
-      user        = "ubuntu"
-    content     = "${data.template_file.custom_requirements.rendered}"
-      agent       = false
-      type        = "ssh"
-      user        = "ubuntu"
-    content     = "${data.template_file.airflow_environment.rendered}"
-      agent       = false
-      type        = "ssh"
-      user        = "ubuntu"
-    content     = "${data.template_file.airflow_service.rendered}"
-      agent       = false
-      type        = "ssh"
-      user        = "ubuntu"
-      agent       = false
-      type        = "ssh"
-      user        = "ubuntu"
-  tags      = "${module.airflow_labels_scheduler.tags}"
-  instance_type          = "${var.worker_instance_type}"
-  ami                    = "${var.ami}"
-  key_name               = "${aws_key_pair.auth.id}"
-  subnet_id              = "${element(data.aws_subnet_ids.selected.ids, 0)}"
-  iam_instance_profile   = "${module.ami_instance_profile.instance_profile_name}"
-    volume_type           = "${var.root_volume_type}"
-    volume_size           = "${var.root_volume_size}"
-    content     = "${data.template_file.custom_env.rendered}"
-      agent       = false
-      type        = "ssh"
-      user        = "ubuntu"
-    content     = "${data.template_file.custom_requirements.rendered}"
-      agent       = false
-      type        = "ssh"
-      user        = "ubuntu"
-    content     = "${data.template_file.airflow_environment.rendered}"
-      agent       = false
-      type        = "ssh"
-      user        = "ubuntu"
-    content     = "${data.template_file.airflow_service.rendered}"
-      agent       = false
-      type        = "ssh"
-      user        = "ubuntu"
-      agent       = false
-      type        = "ssh"
-      user        = "ubuntu"
-  tags      = "${module.airflow_labels_worker.tags}"
-  source      = "terraform-aws-modules/security-group/aws"
-  name        = "${module.airflow_labels.id}-database-sg"
-  vpc_id      = "${data.aws_vpc.default.id}"
-      rule                     = "postgresql-tcp"
-      description              = "Allow ${module.airflow_labels.id} machines"
-  identifier              = "${module.airflow_labels.id}-db"
-  allocated_storage       = "${var.db_allocated_storage}"
-  engine                  = "postgres"
-  engine_version          = "11.1"
-  instance_class          = "${var.db_instance_type}"
-  name                    = "${var.db_dbname}"
-  username                = "${var.db_username}"
-  password                = "${var.db_password}"
-  storage_type            = "gp2"
-  multi_az                = false
-  publicly_accessible     = false
-  apply_immediately       = true
-  skip_final_snapshot     = true
-  vpc_security_group_ids  = ["${module.sg_database.this_security_group_id}"]
-  port                    = "5432"
BREAKS HERE
-  aws_region                        = "${var.aws_region}"
-  vpc_id                            = "${module.networking.vpc_id}"
-  alb_subnet_ids                    = "${module.networking.public_subnet_ids}"
-  alb_ingress_https_listener_arn    = "${module.networking.alb_ingress_https_listener_arn}"
BREAKS HERE
-variable "grr_ca_cn" {
-  description = "Common name for internal CA"
-}
-
-variable "frontend_cn" {
-  description = "Common name to use frotend certificate"
-}
-
-variable "grr_ca_org" {
-  description = "Organization for internal CA"
-}
-
-variable "grr_ca_country" {
-  description = "Country for internal CA"
-}
-
-variable "frontend_rsa_key_length" {
-  default = 2048
-}
-
BREAKS HERE
-  security_groups = [ "${ var.etcd-security-group-id }" ]
BREAKS HERE
-  zone               = "${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"
-  target_tags        = ["${var.name}nat-${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"]
-  name               = "${var.name}nat-gateway-${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"
-  name                   = "${var.name}nat-${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"
-  next_hop_instance_zone = "${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"
-  tags                   = ["${compact(concat(list("${var.name}nat-${var.region}"), var.tags))}"]
-  name    = "${var.name}nat-${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"
-  source_tags = ["${compact(concat(list("${var.name}nat-${var.region}", "${var.name}nat-${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"), var.tags))}"]
-  target_tags = ["${compact(concat(list("${var.name}nat-${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"), var.tags))}"]
-  name    = "${var.name}nat-${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"
BREAKS HERE
- # instance MS-G-3
- resource "aws_instance" "ms_g_3" {
-     Name = "ms_g_3_${var.install_version}"
- output "ms_g_3_ip" {
-   value = "${aws_instance.ms_g_3.private_ip}"
- # instance MS-G-4
- resource "aws_instance" "ms_g_4" {
-     Name = "ms_g_4_${var.install_version}"
- output "ms_g_4_ip" {
-   value = "${aws_instance.ms_g_4.private_ip}"
BREAKS HERE
-  description = "A User Data script to execute while the server is booting. We remmend passing in a bash script that executes the run-vault script, which should have been installed in the AMI by the install-vault module."
BREAKS HERE
-  count = "${var.scw_provider == "SCALEWAY" ? var.scw_worker_instance_count : 0}"
-  count          = "${var.scw_provider == "SCALEWAY" ? var.scw_worker_instance_count : 0}"
-  type           = "${var.scw_worker_instance_type}"
-    size_in_gb = "${lookup(var.scw_additional_volume_size, var.scw_worker_instance_type)}"
-    user        = "${var.scw_ssh_user}"
-    private_key = "${file(var.scw_ssh_key)}"
-    source      = "${var.aws_ssh_key}"
-      "sh /tmp/install-worker.sh ${var.scw_docker_version} ${scaleway_server.swarm_manager.private_ip}",
-      user = "${var.scw_ssh_user}"
BREAKS HERE
-output region_instances {
-  description = "List of instances in the region instance group. Note that this can change dynamically depending on the current number of instances in the group and may be empty the first time read."
-  value       = "${data.google_compute_instance_group.regional.instances}"
-}
-
BREAKS HERE
-  name = "consul-${var.cluster_name}"
-  base_instance_name = "consul-${var.cluster_name}"
-resource "google_compute_instance_template" "consul_servers_public" {
-  name_prefix = "consul-${var.cluster_name}"
-  tags = "${concat(list("consul-server-${var.cluster_name}"), var.custom_network_tags)}"
-//  service_account {
-//    scopes = ["userinfo-email", "compute-ro", "storage-ro"]
-//  }
-resource "google_compute_instance_template" "consul_servers_private" {
-  name_prefix = "consul-${var.cluster_name}"
-  tags = "${concat(list("consul-server-${var.cluster_name}"), var.custom_network_tags)}"
-  //  service_account {
-  //    scopes = ["userinfo-email", "compute-ro", "storage-ro"]
-  //  }
-  name    = "consul-server-${var.cluster_name}"
-  source_tags = ["consul-server-${var.cluster_name}"]
-  template = "${element(concat(google_compute_instance_template.consul_servers_public.*.self_link, google_compute_instance_template.consul_servers_private.*.self_link), 0)}"
BREAKS HERE
-  name    = "${local.name}"
-  network = "${google_compute_network.main.name}"
-  region       = "${element(split("-", var.region), 0)}-${element(split("-", var.region), 1)}"
-  zone           = "${var.region}"
BREAKS HERE
-  name       = "${var.name_prefix}-keypair"
BREAKS HERE
-  filename = "${lambda_artefact_name}"
BREAKS HERE
-resource "azurerm_role_assignment" "user01" {
-  scope                = "${azurerm_resource_group.shared.id}"
-  role_definition_name = "Azure Kubernetes Service Cluster User Role"
-  principal_id         = "${var.k8sbook_aad_userid_1}"
-}
-
BREAKS HERE
-  source              = "../../../"
BREAKS HERE
-  node_instance_type = "${var.k8s_node_instance_type}"
-  node_instance_count = "${var.k8s_node_instance_count}"
BREAKS HERE
-  depends_on = ["null_resource.deploy"]
BREAKS HERE
-  description = "k8s bastion security group"
-  name = "bastion-k8s-${ var.name }"
-    Name = "bastion-k8s-${ var.name }"
-  description = "k8s etcd security group"
-  name = "etcd-k8s-${ var.name }"
-    Name = "etcd-k8s-${ var.name }"
-  description = "k8s-${ var.name } master (apiserver) external elb"
-  name = "master-external-elb-k8s-${ var.name }"
-    Name = "master-external-elb-k8s-${ var.name }"
-  description = "k8s worker security group"
-  name = "worker-k8s-${ var.name }"
-    Name = "worker-k8s-${ var.name }"
BREAKS HERE
-  tags {
BREAKS HERE
-  description = "Whether to create poweruser role"
-  description = "Path of poweruser IAM role"
-  description = "Policy ARN to use for admin role"
BREAKS HERE
-      "name": "SECRET",
-      "value": "KEY"
BREAKS HERE
-      value = "${var.haystack_cluster_name}-k8s-masters"
BREAKS HERE
-  cidr_block           = "172.16.20.0/24"
-  cidr_block              = "172.16.20.0/25"
-# Obtain AMI ID
-data "aws_ami" "ubuntu" {
-    most_recent = true
-    filter {
-        name   = "name"
-        values = ["ubuntu/images/hvm-ssd/ubuntu-bionic-18.04-amd64-server-*"]
-    }
-    filter {
-        name   = "virtualization-type"
-        values = ["hvm"]
-    }
-    owners = ["099720109477"] # Canonical
-}
-
-  key_name   = "cc-demokeypair"
-  ami           = "${data.aws_ami.ubuntu.id}"
-  private_ip = "172.16.20.10"
-  provisioner "remote-exec" {
-    inline = [
-      "chmod +x /tmp/setup/consul_n1_setup.sh",
-      "cd /tmp/setup && sudo ./consul_n1_setup.sh",
-    ]
-    connection {
-      type     = "ssh"
-      user     = "ubuntu"
-      host     = "${self.public_dns}"
-      private_key = "${tls_private_key.ssh_key_pair.private_key_pem}"
-    }
-  }
-  ami           = "${data.aws_ami.ubuntu.id}"
-  private_ip = "172.16.20.11"
-  provisioner "remote-exec" {
-    inline = [
-      "chmod +x /tmp/setup/consul_n2_setup.sh",
-      "cd /tmp/setup && sudo ./consul_n2_setup.sh",
-    ]
-
-    connection {
-      type     = "ssh"
-      user     = "ubuntu"
-      host     = "${self.public_dns}"
-      private_key = "${tls_private_key.ssh_key_pair.private_key_pem}"
-    }
-  }
BREAKS HERE
-resource "aws_iam_role_policy_attachment" "db-admin_database_backups_iam_role_policy_attachment" {
-  policy_arn = "${data.terraform_remote_state.infra_database_backups_bucket.write_database_backups_bucket_policy_arn}"
BREAKS HERE
-  description = "Balancer for ${app_name}"
BREAKS HERE
-    command = "echo '${data.template_file.grafana_cluster_addon_config.rendered}' | ${var.kubectl_executable_name} delete -f - --context ${var.kubectl_context_name}"
BREAKS HERE
-  profile = "${var.cached_install == "true" ? matchbox_profile.cached-container-linux-install.name : matchbox_profile.container-linux-install.name}"
BREAKS HERE
-  # kms_keys define which KMS keys this ecs_service can access.
-  kms_keys = "${var.kms_keys}"
-
-  # create_route53_record sets if this module creates a Route53 record.
-  create_route53_record = "${lookup(var.load_balancing_properties,"create_route53_record", var.default_load_balancing_properties_create_route53_record)}"
-
-  # route53_record_type sets the record type of the route53 record, defaults to cname
-  # route53_a_record_identifier sets the identifier of the weighted IN A Alias record
-  route53_a_record_identifier = "${lookup(var.load_balancing_properties,"route53_a_record_identifier", var.default_load_balancing_properties_route53_a_record_identifier)}"
BREAKS HERE
-# Security groups
BREAKS HERE
-    resources = ["${var.discourse_tldr_bucket_arn}"]
-  memory_size = 128
-  timeout = 10
BREAKS HERE
-  count = "${var.enabled == "true" ? 1 : 0}"
BREAKS HERE
-    uuid = "${module.network.id}"
-    port = "${module.network.master_ports[count.index]}"
-  floating_ip = "${module.network.master_floating_ips[count.index]}"
-    port = "${module.network.worker_ports[count.index]}"
-  floating_ip = "${module.network.worker_floating_ips[count.index]}"
-  depends_on = ["module.tectonic"]
-    host        = "${module.network.master_floating_ips[0]}"
BREAKS HERE
-  name = "elasticsearch-${var.es_cluster}-master-nodes"
BREAKS HERE
-  count = "${var.scw_provider == "SCALEWAY" ? var.scw_worker_instance_count : 0}"
-  count          = "${var.scw_provider == "SCALEWAY" ? var.scw_worker_instance_count : 0}"
-  type           = "${var.scw_worker_instance_type}"
-    size_in_gb = "${lookup(var.scw_additional_volume_size, var.scw_worker_instance_type)}"
-    user        = "${var.scw_ssh_user}"
-    private_key = "${file(var.scw_ssh_key)}"
-    source      = "${var.aws_ssh_key}"
-      "sh /tmp/install-worker.sh ${var.scw_docker_version} ${scaleway_server.swarm_manager.private_ip}",
-      user = "${var.scw_ssh_user}"
BREAKS HERE
-  bucket_prefix = "constellation-${var.aws_region}-net-${var.network_id}-"
-  bucket_prefix = "quorum-backup-${var.aws_region}-network-${var.network_id}-"
BREAKS HERE
-  workstation_cidr_blocks = ["${var.workstation_cidr_blocks}"]
BREAKS HERE
-    etcd_initial_cluster = "${join(",", formatlist("%s=https://%s:2380", null_resource.repeat.*.triggers.name, null_resource.repeat.*.triggers.domain))}"
-    k8s_dns_service_ip   = "${cidrhost(var.service_cidr, 10)}"
BREAKS HERE
-  volume_id       = "${element(oci_core_volume.TFVolumeInstanceEtcd.*.id, count.index)}"
BREAKS HERE
-    ports    = ["${var.service_port}"]
BREAKS HERE
-  k8s_nodes_iam-instance-profile_arn = "${module.haystack-k8s.nodes_iam-instance-profile_arn}"
BREAKS HERE
-        noncurrent_version_transition {
-        noncurrent_version_transition {
-        noncurrent_version_expiration {
BREAKS HERE
-  required_version = ">= 0.11.8"
-  version = ">= 2.6.0"
-  region  = "${var.region}"
-  version = "= 1.3.1"
-data "aws_availability_zones" "available" {}
-  version        = "1.60.0"
-  azs            = ["${data.aws_availability_zones.available.names}"]
-  cluster_name                       = "${local.cluster_name}"
-  subnets                            = ["${module.vpc.public_subnets}"]
-  vpc_id                             = "${module.vpc.vpc_id}"
-  worker_group_count                 = 0
-  worker_group_launch_template_count = 2
BREAKS HERE
-    events              = "${local.s3_0_events_list}"
-    events              = "${local.s3_1_events_list}"
-    events              = "${local.s3_2_events_list}"
-    events              = "${local.s3_3_events_list}"
-    events              = "${local.s3_4_events_list}"
-    events              = "${local.s3_5_events_list}"
-    events              = "${local.s3_6_events_list}"
-    events              = "${local.s3_7_events_list}"
-    events              = "${local.s3_8_events_list}"
-    events              = "${local.s3_9_events_list}"
BREAKS HERE
-  repository = "sous-chefs.github.io"
BREAKS HERE
-  policy_arn = "${data.terraform_remote_state.infra_database_backups_bucket.dbadmin_write_database_backups_bucket_policy_arn}"
-  policy_arn = "${data.terraform_remote_state.infra_database_backups_bucket.dbadmin_read_database_backups_bucket_policy_arn}"
BREAKS HERE
-  instance_class      = "db.m4.large"
-  instance_class             = "db.m4.large"
BREAKS HERE
-  listener_arn    = "${element(aws_lb_listener.listener.*.arn, element(compact(data.null_data_source.values.*.inputs.ssl_arn_index),count.index))}"
BREAKS HERE
-  public_key      = "${file("${path.root}/../temp_key.pub")}"
BREAKS HERE
-    sid = "s3"
-      "s3:ListBucket",
-      "s3:GetBucketLocation",
-      "arn:aws:s3:::${var.s3_bucket_prefix}*",
-      "arn:aws:s3:::${var.s3_bucket_prefix}*/*",
BREAKS HERE
-  role_name        = "govuk-poweruser"
BREAKS HERE
-  network_name    = "pf-test-int-full"
-  name                = "pf-test-int-full"
BREAKS HERE
-  name    = "nat-${element(var.nat_zones, count.index / var.nat_count_per_zone)}-${(count.index % var.nat_count_per_zone) + 1}"
-  name    = "nat-${var.env}-${var.index}.gce-${var.region}-${element(var.nat_zones, count.index / var.nat_count_per_zone)}.travisci.net"
-  records = ["${element(google_compute_address.nat.*.address, count.index)}"]
-  name           = "${var.env}-${var.index}-nat-${element(var.nat_zones, count.index / var.nat_count_per_zone)}-${(count.index % var.nat_count_per_zone) + 1}-template-${substr(sha256("${var.nat_image}${data.template_file.nat_cloud_config.rendered}"), 0, 7)}"
-      nat_ip = "${element(google_compute_address.nat.*.address, count.index)}"
-  base_instance_name = "${var.env}-${var.index}-nat-${element(var.nat_zones, count.index / var.nat_count_per_zone)}-${(count.index % var.nat_count_per_zone) + 1}"
-  name               = "nat-${element(var.nat_zones, count.index / var.nat_count_per_zone)}-${(count.index % var.nat_count_per_zone) + 1}"
-  zone               = "${var.region}-${element(var.nat_zones, count.index / var.nat_count_per_zone)}"
-    instance_template = "${element(google_compute_instance_template.nat.*.self_link, count.index)}"
-  name                   = "nat-${element(var.nat_zones, count.index / var.nat_count_per_zone)}-${(count.index % var.nat_count_per_zone) + 1}"
BREAKS HERE
-module "execution_role_label" {
-  attributes = ["${compact(concat(var.attributes, list("execution", "role")))}"]
-  execution_role_arn       = "${aws_iam_role.ecs_execution_role.arn}"
-  task_role_arn            = "${aws_iam_role.ecs_execution_role.arn}"
-data "aws_iam_policy_document" "ecs_task_execution_role" {
-resource "aws_iam_role" "ecs_execution_role" {
-  name               = "${module.execution_role_label.id}"
-  assume_role_policy = "${data.aws_iam_policy_document.ecs_task_execution_role.json}"
-data "aws_iam_policy_document" "ecs_execution_role" {
-resource "aws_iam_role_policy" "ecs_execution_role_policy" {
-  name   = "${module.execution_role_label.id}"
-  policy = "${data.aws_iam_policy_document.ecs_execution_role.json}"
-  role   = "${aws_iam_role.ecs_execution_role.id}"
-  desired_count                      = "${var.desired_count}"
-    container_name   = "${var.name}"                 #FIXME
-    container_port   = 80                            #FIXME
BREAKS HERE
-# Internet gateway for the public subnet
-resource "aws_internet_gateway" "ig_admiral_setup" {
-  vpc_id = "${aws_vpc.vpc.id}"
-  tags {
-    Name = "ig_admiral_setup_${var.install_version}"
-  }
-}
-
-# Routing table for admiral subnet
-resource "aws_route_table" "rt_admiral_setup" {
-  vpc_id = "${aws_vpc.vpc.id}"
-  route {
-    cidr_block = "0.0.0.0/0"
-    gateway_id = "${aws_internet_gateway.ig_admiral_setup.id}"
-  }
-  tags {
-    Name = "rt_admiral_setup_${var.install_version}"
-  }
-}
-
-  route_table_id = "${aws_route_table.rt_admiral_setup.id}"
BREAKS HERE
-  # s3_ro_paths define which paths on S3 can be accessed from the ecs service in read-only fashion. 
-  # s3_ro_paths define which paths on S3 can be accessed from the ecs service in read-write fashion. 
-# This sub-module creates everything regarding the connection of an ecs service to an Application Load Balancer
-  # lb_listener_arn is the arn of the listener ( HTTPS )
-  # the custom_listen_hosts will be added as a host route rule as aws_lb_listener_rule to the given service e.g. www.domain.com -> Service
-# This sub-module creates the ECS Task definition
-  # The name of the task_definition ( generally, a combination of the cluster name and the service name.)
-# This sub-module creates the ECS Service
BREAKS HERE
-  credentials             = "${aws_iam_role.api_gateway_0.arn}"
-  credentials             = "${aws_iam_role.api_gateway_1.arn}"
-  credentials             = "${aws_iam_role.api_gateway_2.arn}"
-  credentials             = "${aws_iam_role.api_gateway_3.arn}"
-  credentials             = "${aws_iam_role.api_gateway_4.arn}"
-  credentials             = "${aws_iam_role.api_gateway_5.arn}"
-  credentials             = "${aws_iam_role.api_gateway_6.arn}"
-  credentials             = "${aws_iam_role.api_gateway_7.arn}"
-  credentials             = "${aws_iam_role.api_gateway_8.arn}"
-  credentials             = "${aws_iam_role.api_gateway_9.arn}"
BREAKS HERE
-  apply_immediately    = true
BREAKS HERE
-  default     = "11.1.0"
BREAKS HERE
-  disable_dependent_services  = "${var.disable_dependant_services}"
BREAKS HERE
-  parameter_group_name = "mariadb-parameters"
BREAKS HERE
-resource "null_resource" "config" {
-  provisioner "local-exec" {
-    command = "mkdir -p '${local.algo_config}/keys'"
-  }
-}
-
BREAKS HERE
-sudo mkdir -p /hab/user/janus-gateway/config ; echo "[nat]" > /hab/user/janus-gateway/config/user.toml
-sudo echo "nat_1_1_mapping = \"$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4)\"" >> /hab/user/janus-gateway/config/user.toml
-sudo mkdir -p /hab/user/janus-gateway/config ; echo "[nat]" > /hab/user/janus-gateway/config/user.toml
-sudo echo "nat_1_1_mapping = \"$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4)\"" >> /hab/user/janus-gateway/config/user.toml
BREAKS HERE
-  name          = "alias/parameter_store_key"
BREAKS HERE
-      "Resource": "arn:aws:es:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:domain/${var.domain}/*"
BREAKS HERE
-  count    = "${length(var.map_users)}"
-  count    = "${length(var.map_roles)}"
-  count    = "${length(var.map_accounts)}"
BREAKS HERE
-  aliases = ["surveillance.mozilla.org"]
BREAKS HERE
-  name           = "${module.dynamodb_label.id}"
-  read_capacity  = "${var.autoscale_min_read_capacity}"
-  write_capacity = "${var.autoscale_min_write_capacity}"
-  hash_key       = "${var.hash_key}"
-  range_key      = "${var.range_key}"
BREAKS HERE
-  value = "${data.local_file.stdout.content}"
-  value = "${data.local_file.stderr.content}"
-  value = "${data.local_file.exitstatus.content}"
BREAKS HERE
-  name        = "${var.name}-certificate"
BREAKS HERE
-resource "consul_key_prefix" "core_integration" {
-  count       = "${var.core_integration ? 1 : 0}"
-  path_prefix = "${var.consul_key_prefix}aws-auth/"
-
-  subkeys {
-    "enabled"            = "yes"
-    "path"               = "${vault_auth_backend.aws.path}"
-    "roles/consul"       = "${var.consul_role}"
-    "roles/nomad_server" = "${var.nomad_server_role}"
-    "roles/nomad_client" = "${var.nomad_client_role}"
-    "roles/vault"        = "${var.vault_role}"
-    "README"             = "This is used for integration with the `core` module. See https://github.com/GovTechSG/terraform-modules/tree/master/modules/aws-auth"
BREAKS HERE
-  # source = "git::git@github.com:gruntwork-io/terraform-google-gke.git//modules/gke-cluster?ref=v0.0.4"
-  project    = "${var.project}"
-  location   = "${var.location}"
-  network    = "${google_compute_network.main.self_link}"
-  subnetwork = "${google_compute_subnetwork.main.self_link}"
-  cluster_secondary_range_name = "${google_compute_subnetwork.main.secondary_ip_range.0.range_name}"
-    tags         = ["main-pool-example"]
-# TODO(rileykarson): Add proper VPC network config once we've made a VPC module
-resource "google_compute_network" "main" {
-  name                    = "${var.cluster_name}-network-${random_string.suffix.result}"
-  auto_create_subnetworks = "false"
-}
-resource "google_compute_subnetwork" "main" {
-  name          = "${var.cluster_name}-subnetwork-${random_string.suffix.result}"
-  ip_cidr_range = "10.0.0.0/17"
-  region        = "${var.region}"
-  network       = "${google_compute_network.main.self_link}"
-  secondary_ip_range {
-    range_name    = "cluster-pods"
-    ip_cidr_range = "10.1.0.0/18"
-  }
BREAKS HERE
-
-resource "null_resource" "deploy_certificates" {
-  triggers = {
-    digitalocean_droplet      = "${digitalocean_droplet.algo.id}"
-  }
-
-  connection {
-    host        = "${digitalocean_droplet.algo.ipv4_address}"
-    user        = "root"
-    private_key = "${var.private_key_pem}"
-  }
-
-  provisioner "remote-exec" {
-    inline = ["bash -c 'mkdir -p /etc/ipsec.d/{cacerts,certs,private}'"]
-  }
-
-  provisioner "file" {
-    content     = "${var.ca_cert}"
-    destination = "/etc/ipsec.d/cacerts/ca.pem"
-  }
-
-  provisioner "file" {
-    content     = "${var.server_cert}"
-    destination = "/etc/ipsec.d/certs/server.pem"
-  }
-
-  provisioner "file" {
-    content     = "${var.server_key}"
-    destination = "/etc/ipsec.d/private/server.pem"
-  }
-
-  provisioner "remote-exec" {
-    inline = ["systemctl status strongswan >/dev/null 2>&1 && systemctl restart strongswan || true"]
-  }
-}
BREAKS HERE
-version: 3-released
BREAKS HERE
-    worker_role_arn = "${aws_iam_role.workers.arn}"
BREAKS HERE
-  add_monitoring_addons = false
BREAKS HERE
-    "aws_iam_role.pki",
BREAKS HERE
-  value       = "${module.first_container.json}"
BREAKS HERE
-  # create only if not specified in var.secgroup_name
-  network_security_group_name = "${azurerm_network_security_group.test.name}"
BREAKS HERE
-  kubernetes_version        = "1.8.4"
BREAKS HERE
-  gcp_region = "${var.gcp_region}"
BREAKS HERE
-    name                   = "${aws_elb.calculators-frontend_elb.dns_name}"
-    zone_id                = "${aws_elb.calculators-frontend_elb.zone_id}"
-  source                        = "../../modules/aws/node_group"
-  name                          = "${var.stackname}-calculators-frontend"
-  default_tags                  = "${map("Project", var.stackname, "aws_stackname", var.stackname, "aws_environment", var.aws_environment, "aws_migration", "calculators_frontend", "aws_hostname", "calculators-frontend-1")}"
-  instance_subnet_ids           = "${data.terraform_remote_state.infra_networking.private_subnet_ids}"
-  instance_security_group_ids   = ["${data.terraform_remote_state.infra_security_groups.sg_calculators-frontend_id}", "${data.terraform_remote_state.infra_security_groups.sg_management_id}"]
-  instance_type                 = "${var.instance_type}"
-  instance_additional_user_data = "${join("\n", null_resource.user_data.*.triggers.snippet)}"
-  instance_elb_ids_length       = "1"
-  instance_elb_ids              = ["${aws_elb.calculators-frontend_elb.id}"]
-  instance_ami_filter_name      = "${var.instance_ami_filter_name}"
-  asg_max_size                  = "${var.asg_size}"
-  asg_min_size                  = "${var.asg_size}"
-  asg_desired_capacity          = "${var.asg_size}"
-  asg_notification_topic_arn    = "${data.terraform_remote_state.infra_monitoring.sns_topic_autoscaling_group_events_arn}"
-  root_block_device_volume_size = "${var.root_block_device_volume_size}"
BREAKS HERE
-  count = "${var.create_vpc && length(var.redshift_subnets) > 0 ? length(var.redshift_subnets) : 0}"
BREAKS HERE
-  domain              = ""
BREAKS HERE
-  count = "${var.create_vpc && var.enable_s3_endpoint ? 1 : 0}"
-  count = "${var.create_vpc && var.enable_dynamodb_endpoint ? 1 : 0}"
BREAKS HERE
-    zone_id = "Z35SXDOTRQ7X7K"
-    zone_id = "Z35SXDOTRQ7X7K"
-    zone_id = "Z35SXDOTRQ7X7K"
-    zone_id = "Z35SXDOTRQ7X7K"
-    zone_id = "Z35SXDOTRQ7X7K"
-  resource "aws_route53_record" "18f_gov_grouplet-playbook_18f_gov_a" {
-    zone_id = "Z35SXDOTRQ7X7K"
-
BREAKS HERE
-    command = "echo \"export KUBECONFIG=${path.root}/generated/kubeconfig\" > source.sh "
BREAKS HERE
-  count = "${length(var.zones)}"
-  name  = "${var.env}-${var.index}-worker-com-${element(var.zones, count.index)}-template-${substr(sha256("${var.worker_image}${data.template_file.cloud_config_com.rendered}"), 0, 7)}"
-    ignore_changes = ["disk", "boot_disk"]
-
-resource "google_compute_instance_group_manager" "worker_com" {
-  count = "${length(var.zones)}"
-
-  base_instance_name = "${var.env}-${var.index}-worker-com-${element(var.zones, count.index)}"
-  instance_template  = "${element(google_compute_instance_template.worker_com.*.self_link, count.index)}"
-  name               = "worker-com-${element(var.zones, count.index)}"
-  zone               = "${var.region}-${element(var.zones, count.index)}"
-  count = "${length(var.zones)}"
-  name  = "${var.env}-${var.index}-worker-com-free-${element(var.zones, count.index)}-template-${substr(sha256("${var.worker_image}${data.template_file.cloud_config_com_free.rendered}"), 0, 7)}"
-  tags         = ["worker", "${var.env}", "com", "free"]
-    ignore_changes = ["disk", "boot_disk"]
-
-resource "google_compute_instance_group_manager" "worker_com_free" {
-  count = "${length(var.zones)}"
-
-  base_instance_name = "${var.env}-${var.index}-worker-com-free-${element(var.zones, count.index)}"
-  instance_template  = "${element(google_compute_instance_template.worker_com_free.*.self_link, count.index)}"
-  name               = "worker-com-free-${element(var.zones, count.index)}"
-  zone               = "${var.region}-${element(var.zones, count.index)}"
-  count = "${length(var.zones)}"
-  name  = "${var.env}-${var.index}-worker-org-${element(var.zones, count.index)}-template-${substr(sha256("${var.worker_image}${data.template_file.cloud_config_org.rendered}"), 0, 7)}"
-    ignore_changes = ["disk", "boot_disk"]
-
-resource "google_compute_instance_group_manager" "worker_org" {
-  count = "${length(var.zones)}"
-
-  base_instance_name = "${var.env}-${var.index}-worker-org-${element(var.zones, count.index)}"
-  instance_template  = "${element(google_compute_instance_template.worker_org.*.self_link, count.index)}"
-  name               = "worker-org-${element(var.zones, count.index)}"
-  zone               = "${var.region}-${element(var.zones, count.index)}"
BREAKS HERE
-  wait_for_elb_capacity = true
BREAKS HERE
-  type        = "list"
-  default     = []
BREAKS HERE
-  name                  = "node${ count.index + 1 }"
-    HOSTNAME       = "node${ count.index + 1 }"
BREAKS HERE
-    command = "echo 'Need to use this var so terraform waits for kubeconfig ' ${var.kubeconfig_complete};KUBECONFIG=${var.output_directory}/${var.kubeconfig_filename} ${path.module}/deploy_flux.sh -b ${var.gitops_url_branch} -f ${var.flux_repo_url} -g ${var.gitops_ssh_url} -k ${var.gitops_ssh_key} -d ${var.flux_clone_dir} -c ${var.gitops_poll_interval} -e ${var.gitops_path} -s ${var.acr_enabled}"
BREAKS HERE
-# The users from the saml role to give access
-
BREAKS HERE
-  reference_name = "${var.name}"
-  
BREAKS HERE
-    command = "${local.command} 2>${path.module}/stderr.${self.id} >${path.module}/stdout.${self.id}; echo $? >${path.module}/exitstatus.${self.id}"
BREAKS HERE
-  count = "${var.create_vpn_connection ? 1 : 0}"
-  count = "${var.create_vpn_connection ? 1 : 0}"
BREAKS HERE
-    cidr_block = "${cidrsubnet(data.template_file.quorum_cidr_block.rendered, 2, 4)}"
BREAKS HERE
-# If you don't want that just create new log groups per enviourment.
-  name              = "/var/log/dmesg"
-  name              = "/var/log/docker"
-  name              = "/var/log/ecs/ecs-agent.log"
-  name              = "/var/log/ecs/ecs-init.log"
-  name              = "/var/log/ecs/audit.log"
-  name              = "/var/log/messages"
BREAKS HERE
-resource "aws_cloudwatch_log_metric_filter" "unauthorized-api-calls" {
-resource "aws_cloudwatch_metric_alarm" "unauthorized-api-calls" {
-  metric_name               = "${aws_cloudwatch_log_metric_filter.unauthorized-api-calls.id}"
-resource "aws_cloudwatch_log_metric_filter" "no-mfa-console-signin" {
-resource "aws_cloudwatch_metric_alarm" "no-mfa-console-signin" {
-  metric_name               = "${aws_cloudwatch_log_metric_filter.no-mfa-console-signin.id}"
-resource "aws_cloudwatch_log_metric_filter" "root-usage" {
-resource "aws_cloudwatch_metric_alarm" "root-usage" {
-  metric_name               = "${aws_cloudwatch_log_metric_filter.root-usage.id}"
-resource "aws_cloudwatch_log_metric_filter" "iam-changes" {
-resource "aws_cloudwatch_metric_alarm" "iam-changes" {
-  metric_name               = "${aws_cloudwatch_log_metric_filter.iam-changes.id}"
-resource "aws_cloudwatch_log_metric_filter" "cloudtrail-cfg-changes" {
-resource "aws_cloudwatch_metric_alarm" "cloudtrail-cfg-changes" {
-  metric_name               = "${aws_cloudwatch_log_metric_filter.cloudtrail-cfg-changes.id}"
-resource "aws_cloudwatch_log_metric_filter" "console-signin-failures" {
-resource "aws_cloudwatch_metric_alarm" "console-signin-failures" {
-  metric_name               = "${aws_cloudwatch_log_metric_filter.console-signin-failures.id}"
-resource "aws_cloudwatch_log_metric_filter" "disable-or-delete-cmk" {
-resource "aws_cloudwatch_metric_alarm" "disable-or-delete-cmk" {
-  metric_name               = "${aws_cloudwatch_log_metric_filter.disable-or-delete-cmk.id}"
-resource "aws_cloudwatch_log_metric_filter" "s3-bucket-policy-changes" {
-resource "aws_cloudwatch_metric_alarm" "s3-bucket-policy-changes" {
-  metric_name               = "${aws_cloudwatch_log_metric_filter.s3-bucket-policy-changes.id}"
-resource "aws_cloudwatch_log_metric_filter" "aws-config-changes" {
-resource "aws_cloudwatch_metric_alarm" "aws-config-changes" {
-  metric_name               = "${aws_cloudwatch_log_metric_filter.aws-config-changes.id}"
-resource "aws_cloudwatch_log_metric_filter" "security-group-changes" {
-resource "aws_cloudwatch_metric_alarm" "security-group-changes" {
-  metric_name               = "${aws_cloudwatch_log_metric_filter.security-group-changes.id}"
-resource "aws_cloudwatch_log_metric_filter" "nacl-changes" {
-resource "aws_cloudwatch_metric_alarm" "nacl-changes" {
-  metric_name               = "${aws_cloudwatch_log_metric_filter.nacl-changes.id}"
-resource "aws_cloudwatch_log_metric_filter" "network-gw-changes" {
-resource "aws_cloudwatch_metric_alarm" "network-gw-changes" {
-  metric_name               = "${aws_cloudwatch_log_metric_filter.network-gw-changes.id}"
-resource "aws_cloudwatch_log_metric_filter" "route-table-changes" {
-resource "aws_cloudwatch_metric_alarm" "route-table-changes" {
-  metric_name               = "${aws_cloudwatch_log_metric_filter.route-table-changes.id}"
-resource "aws_cloudwatch_log_metric_filter" "vpc-changes" {
-resource "aws_cloudwatch_metric_alarm" "vpc-changes" {
-  metric_name               = "${aws_cloudwatch_log_metric_filter.vpc-changes.id}"
BREAKS HERE
-variable "hypercube_version" {
-    default = "v1.3.6_coreos.0"
-            user = "core"
-            user = "core"
-            user = "core"
-            user = "core"
-            user = "core"
-        HYPERCUBE_VERSION = "${var.hypercube_version}"
-            user = "core"
-            user = "core"
-            user = "core"
-            user = "core"
-            user = "core"
-            user = "core"
-            "sudo systemctl enable kubelet",
-            "until $(curl --output /dev/null --silent --head --fail http://127.0.0.1:8080); do printf '.'; sleep 5; done",
-            "curl -XPOST -H 'Content-type: application/json' -d'{\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"name\":\"kube-system\"}}' http://127.0.0.1:8080/api/v1/namespaces"
-            user = "core"
-        HYPERCUBE_VERSION = "${var.hypercube_version}"
-            user = "core"
-            user = "core"
-            user = "core"
-            user = "core"
-            user = "core"
BREAKS HERE
-  maintenence_window   = "Sat:04:00-Sat:06:00"
BREAKS HERE
-# CREATE LAUCNH CONFIGURATION TO DEFINE WHAT RUNS ON EACH INSTANCE IN THE ASG
-
-
-}
BREAKS HERE
-  name        = "${var.stackname}_publishing-api_elb_access"
-  name        = "${var.stackname}_publishing-api_elb_access"
BREAKS HERE
-  count = "${length(var.security_groups)}"
-  count = "${length(var.security_groups)}"
BREAKS HERE
-        key_data = "${file("${path.root}/../../temp_key.pub")}"
-        key_data = "${file("${path.root}/../../temp_key.pub")}"
BREAKS HERE
-  bucket = "${data.aws_caller_identity.current.account_id}-gitlab-runner-cache"
BREAKS HERE
-
-  size = 100
BREAKS HERE
-  tags = "${merge(local.tags,map("Name","${var.prefix}-${var.node_type}-${count.index}-${var.vm_version + element(module.roll.revision_list,count.index)}")}"
-  tags = "${merge(local.tags,map("Name","${var.prefix}-${var.node_type}-${count.index}-${var.vm_version + element(module.roll.revision_list,count.index)}")}"
BREAKS HERE
-  tags = "${module.tf_label.tags}"
-  tags = "${module.tf_label.tags}"
BREAKS HERE
-    client_secret = "${azurerm_azuread_service_principal_password.aks.value}"
BREAKS HERE
-  tags = "${merge(map("Name", format("%s-public", var.name)), var.public_route_table_tags, var.tags)}"
-  tags = "${merge(map("Name", (var.single_nat_gateway ? "${var.name}-private" : format("%s-private-%s", var.name, element(var.azs, count.index)))), var.private_route_table_tags, var.tags)}"
-  tags = "${merge(var.tags, var.database_route_table_tags, map("Name", "${var.name}-database"))}"
-  tags = "${merge(var.tags, var.redshift_route_table_tags, map("Name", "${var.name}-redshift"))}"
-  tags = "${merge(var.tags, var.elasticache_route_table_tags, map("Name", "${var.name}-elasticache"))}"
-  tags = "${merge(map("Name", format("%s-public-%s", var.name, element(var.azs, count.index))), var.public_subnet_tags, var.tags)}"
-  tags = "${merge(map("Name", format("%s-private-%s", var.name, element(var.azs, count.index))), var.private_subnet_tags, var.tags)}"
-  tags = "${merge(map("Name", format("%s-db-%s", var.name, element(var.azs, count.index))), var.database_subnet_tags, var.tags)}"
-  tags = "${merge(map("Name", format("%s-redshift-%s", var.name, element(var.azs, count.index))), var.redshift_subnet_tags, var.tags)}"
-  tags = "${merge(map("Name", format("%s-elasticache-%s", var.name, element(var.azs, count.index))), var.elasticache_subnet_tags, var.tags)}"
BREAKS HERE
-  count = "${var.org_id == "" && var.folder_id == "" ? 1 : 0}"
BREAKS HERE
-    zone_id                = "${local.old_cloudfront_zone_id}"
-    zone_id                = "${local.old_cloudfront_zone_id}"
BREAKS HERE
-  name_prefix            = "${var.environment_name}-${data.aws_region.current.name}-${var.vpc}-bastion-service"
-##################
-# empty security group to assist upgrade from classic_load_balancer to network_load_balancer
-##################
-resource "aws_security_group" "bastion_lb" {
-  name_prefix            = "${var.environment_name}-${data.aws_region.current.name}-${var.vpc}-bastion-lb"
-  description            = "Allow access from the Internet to the SSH Load Balancer"
-  revoke_rules_on_delete = true
-  vpc_id                 = "${var.vpc}"
-  tags                   = "${var.tags}"
-  lifecycle {
-    create_before_destroy = true
-  }
-}
BREAKS HERE
-  default     = ""
BREAKS HERE
-    target_prefix = "log/"
BREAKS HERE
-data "aws_acm_certificate" "elb_internal_cert" {
-  domain   = "${var.elb_internal_certname}"
-  statuses = ["ISSUED"]
-}
-
-aesource "aws_elb" "prometheus_external_elb" {
-  instance_elb_ids_length       = "2"
-  instance_elb_ids              = ["${aws_elb.prometheus_internal_elb.id}", "${aws_elb.prometheus_external_elb.id}"]
BREAKS HERE
-  use_metadata              = false
BREAKS HERE
-  default     = "us-east-1"
BREAKS HERE
-  count = "${var.state == "none" ? 0 : 1}"
BREAKS HERE
-  description = "Name of the internal Nomad load balancer"
-  default = "nomad-internal"
BREAKS HERE
-  source    = "git::https://github.com/cloudposse/terraform-null-label.git?ref=tags/0.3.3"
-  namespace = "${var.namespace}"
-  stage     = "${var.stage}"
-  name      = "public"
-  cidr_block        = "${cidrsubnet(signum(length(var.cidr_block)) == 1 ? var.cidr_block : data.aws_vpc.default.cidr_block, ceil(log(length(data.aws_availability_zones.available.names) * 2, 2)), length(data.aws_availability_zones.available.names) + count.index)}"
BREAKS HERE
-locals {
-  # workaround for "conditional operator cannot be used with list values"
-  runner_ssh_config = {
-    enabled  = "${var.gitlab_runner_ssh_cidr_blocks}"
-    disabled = "${list()}"
-  }
-}
-
-  ingress {
-    from_port   = 22
-    to_port     = 22
-    protocol    = "tcp"
-    cidr_blocks = "${local.runner_ssh_config["${var.enable_gitlab_runner_ssh_access == 1 ? "enabled" : "disabled"}"]}"
-  }
-
-  # count = "${var.manage_ssm_runner_token}"
-
BREAKS HERE
-  oidc_dropin = "${module.use_oidc.if_active ? indent(8,join("\n",formatlist("- %s\n",split("\n",data.template_file.oidc.rendered)))) : ""}"
BREAKS HERE
-
-# TODO: Backport usage to other platforms
-variable "tectonic_use_jumpbox" {
-  type = "string"
-
-  description = <<EOF
-(optional) Specifies whether a jumpbox should be created to manage cluster nodes.
-DISCLAIMER: This is currently experimental and can only be configured for Azure.
-DO NOT USE.
-EOF
-
-  default = "false"
-}
BREAKS HERE
-  vpc_id     = "${aws_vpc.main.id}"
-  cidr_block = "${var.subnet_cidr_block}"
BREAKS HERE
-# BACKUP S3 BUCKET
-# ---------------------------------------------------------------------------------------------------------------------
-resource "aws_s3_bucket" "data_backup" {
-  bucket_prefix = "quorum-net-${var.network_id}-backup-"
-  force_destroy = "${var.force_destroy_s3_buckets}"
-
-  versioning {
-    enabled = true
-  }
-}
-
-# ---------------------------------------------------------------------------------------------------------------------
BREAKS HERE
-data "aws_iam_policy_document" "es_management_instance_access" {
BREAKS HERE
-    filename         = "${path.module}/es-cleanup.zip"
-    function_name    = "${var.prefix}es-cleanup"
-    timeout          = 300
-    runtime          = "python${var.python_version}"
-    role             = "${aws_iam_role.role.arn}"
-    handler          = "es-cleanup.lambda_handler"
-    source_code_hash = "${data.archive_file.es_cleanup_lambda.output_base64sha256}"
-
-    environment {
-        variables = {
-            es_endpoint  = "${var.es_endpoint}"
-            index        = "${var.index}"
-            delete_after = "${var.delete_after}"
-            index_format = "${var.index_format}"
-            sns_alert    = "${var.sns_alert}"
-        }
BREAKS HERE
-  internal = true
BREAKS HERE
-  security_groups = [
-    "${ var.security-group-id }",
-  ]
-
-  user_data = "${ template_file.cloud-config.rendered }"
-
-
-
BREAKS HERE
-  subnet_id = "${ aws_subnet.private.0.id }"
BREAKS HERE
-    ignore_changes = ["filename"]
BREAKS HERE
-# create a custom nixos images based on the nix code
-  source       = "../../google_image_nixos_custom"
-  bucket_name  = "${google_storage_bucket.nixos-images.name}"
-  nixos_config = "${path.module}/image_nixos_custom.nix"
-}
-
-# spin up the instance
-resource "google_compute_instance" "image-nixos-custom" {
-  name         = "image-nixos-custom"
-  machine_type = "n1-standard-1"
-  zone         = "us-central1-a"
-
-  boot_disk {
-    initialize_params {
-      image = "${module.nixos_image_custom.self_link}"
-      size  = "20"
-    }
-  }
-
-  network_interface {
-    network = "default"
-    // Give it a public IP
-    access_config {}
-  }
-
-  lifecycle {
-    // No need to re-deploy the machine if the image changed
-    // NixOS is already immutable
-    ignore_changes = ["boot_disk"]
-  }
-}
-
-module "deploy_nixos" {
-  source       = "../../deploy_nixos"
-  target_host  = "${google_compute_instance.image-nixos-custom.network_interface.0.network_ip}"
-
-  triggers = {
-    // Also re-deploy whenever the VM is re-created
-    instance_id = "${google_compute_instance.image-nixos-custom.id}"
-  }
-
-  // Pass some secrets. See the terraform-servets-provider to handle secrets
-  // in Terraform
-  keys = {
-    foo = "bar"
-  }
BREAKS HERE
-    bootkube                     = "quay.io/coreos/bootkube:v0.6.2"
-    hyperkube                    = "quay.io/coreos/hyperkube:v1.7.9_coreos.0"
-    ingress_controller           = "gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.15"
-    pod_checkpointer             = "quay.io/coreos/pod-checkpointer:3517908b1a1837e78cfd041a0e51e61c7835d85f"
BREAKS HERE
-  tags              = "${merge(var.tags, map("Name", format("%s-subnet-private-%s", var.name, element(var.azs, count.index))), map("Tier", "private"))}"
-  tags              = "${merge(var.tags, map("Name", format("%s-database-subnet-%s", var.name, element(var.azs, count.index))), map("Tier", "database"))}"
-  tags              = "${merge(var.tags, map("Name", format("%s-elasticache-subnet-%s", var.name, element(var.azs, count.index))), map("Tier", "elasticache"))}"
-  tags              = "${merge(var.tags, map("Name", format("%s-subnet-public-%s", var.name, element(var.azs, count.index))), map("Tier", "public"))}"
BREAKS HERE
-      to_port = 65535
BREAKS HERE
-variable "maintenance_window_schedule" {
-  default = "cron(0 0 18 ? * SUN *)"
-variable "patch_groups" {
BREAKS HERE
-    custom_data = "${ data.template_file.cloud-config.rendered }"
BREAKS HERE
-  name = "${var.haystack_cluster_name}--app-nodes"
BREAKS HERE
-  count    = "${var.write_aws_auth_config ? 1 : 0}"
-
-  count = "${var.manage_aws_auth ? 1 : 0}"
BREAKS HERE
-  source      = "terraform-aws-modules/security-group/aws"
-  version     = "2.7.0"
BREAKS HERE
-  description = "The security policy if using HTTPS externally on the load balancer. See: https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html"
-  description = "S3 bucket (externally created) for storing load balancer access logs."
BREAKS HERE
-  count    = "${length(var.vpn_users)}"
-  wg_server_private_key   = "${base64encode(random_string.wg_server_private_key.result)}"
-    Peers = "${join("\n", data.template_file.wireguard_peer.*.rendered)}"
-
BREAKS HERE
-resouce "aws_db_subnet_group" "default" {
BREAKS HERE
-    Name = "public"
-    Cluster = "${ var.name }"
BREAKS HERE
-    protocol = "http"
-    protocol = "http"
BREAKS HERE
-  source     = "git::https://github.com/cloudposse/terraform-null-label.git?ref=tags/0.3.1"
BREAKS HERE
-  count                    = "${length(var.allowed_ssh_security_group_ids)}"
BREAKS HERE
-#resource "openstack_lb_monitor_v2" "monitor" {
-#  pool_id     = "${openstack_lb_pool_v2.pool.id}"
-#  type        = "HTTP"
-#  delay       = 20
-#  timeout     = 10
-#  max_retries = 5
-#}
BREAKS HERE
-  name = "${var.prefix}-k8sbook-${var.chap}-sp-aks-${var.cluster_type}"
-resource "null_resource" "aadsync_delay" {
-  // Wait for AAD async global replication
-  provisioner "local-exec" {
-    command = "sleep 90"
-  }
-
-  triggers = {
-    "before" = "${azurerm_azuread_service_principal_password.aks.id}"
-  }
-}
-
-  depends_on = ["null_resource.aadsync_delay"]
-
-    client_secret = "${azurerm_azuread_service_principal_password.aks.value}"
BREAKS HERE
-  instance_name      = "${map("Name", (var.instance_count > 1) || (var.use_num_suffix == "true") ? format("%s-%d", var.name, count.index+1) : var.name)}"
-  tags = "${merge(local.instance_name, var.tags)}"
-  tags = "${merge(local.instance_name, var.tags)}"
BREAKS HERE
-  executable_users = ["self"]
-  name_regex = "^k8s-${split(".",${var.k8s_version})[0]}.${split(".",${var.k8s_version})[1]}-debian-jessie-amd64-hvm-ebs.*"
BREAKS HERE
-variable "tectonic_base_domain" {
-  default = "azure.tectonic-integ.cloudns.cc"
-}
BREAKS HERE
-    join("|", data.aws_iam_role.master_role.*.role_name)
-  count     = "${var.master_iam_role == "" ? 0 : 1}"
-  role_name = "${var.master_iam_role}"
BREAKS HERE
-    Name = "${name_prefix}"
BREAKS HERE
-    x             = 2
BREAKS HERE
-  disk {
-    image = "ubuntu-os-cloud/ubuntu-1610-yakkety-v20170619a"
-  disk {
-    disk = "${google_compute_disk.px-master-disk.name}"
-  name         = "{var.prefix}-k8s-${count.index}"
-  disk {
-    image = "ubuntu-os-cloud/ubuntu-1610-yakkety-v20170619a"
-    # image = "ubuntu-os-cloud/ubuntu-1404-trusty-v20160602"
-  disk {
-    disk = "${element(google_compute_disk.px-disk.*.name, count.index)}"
BREAKS HERE
-  source_security_group_id = "${aws_security_group.sg_prometheus_ingress}"
BREAKS HERE
-# Download the latest Ghost Image
-resource "docker_container" "container_id" {
-  name  = "${var.container_name}"
-  image = "${docker_image.image_id.latest}"
-  ports {
-    internal = "${var.int_port}"
-    external = "${var.ext_port}"
-  }
-resource "null_resource" "sg" {
-  provisioner "local-exec" {
-      command = "../lab-scripts/sg.sh"
-  }
-}
BREAKS HERE
-  family                   = "quorum-${var.tx_privacy_engine}-${var.network_name}"
BREAKS HERE
-    helm_node          = "${var.helm_node}"
BREAKS HERE
-    cidr_blocks = [
-      "0.0.0.0/0",
-    ]
BREAKS HERE
-terraform {
-  required_version = ">= 0.9.11"
-}
-
-  count             = "${var.num_subnets}"
-  vpc_id            = "${var.vpc_id}"
-  cidr_block        = "${cidrsubnet(var.cidr,var.newbits,var.netnum+count.index)}"
-  availability_zone = "${element(data.aws_availability_zones.available.names, count.index)}"
-  tags = "${merge("${var.tags}",map("Name", "${var.project}-${var.visibility}-${var.role}-${element(data.aws_availability_zones.available.names, count.index)}", "Environment", "${var.environment}", "Project", "${var.project}", "Role", "${var.role}", "Visibility", "${var.visibility}", "AvailabilityZone", element(data.aws_availability_zones.available.names, count.index)))}"
-  count          = "${var.num_route_tables >0 ? "${var.num_subnets}" : 0 }"
-  subnet_id      = "${element(aws_subnet.subnets.*.id, count.index)}"
-  route_table_id = "${element(var.route_tables, count.index)}"
BREAKS HERE
-  name = "tf-created-AmazonECSContainerProfile-${var.name}"
-  role = "${aws_iam_role.ecs-role.name}"
-  path = "${var.iam_path}"
-  name = "tf-AmazonECSInstanceRole-${var.name}"
-  path = "${var.iam_path}"
-  name        = "tf-created-AmazonECSContainerInstancePolicy-${var.name}"
-  name               = "${replace(format("%.64s", replace("tf-consul-agentTaskRole-${var.name}-${data.aws_vpc.vpc.tags["Name"]}", "_", "-")), "/\\s/", "-")}"
-  count  = "${var.enable_agents ? 1 : 0}"
-  name   = "${replace(format("%.64s", replace("tf-consul-agentTaskPolicy-${var.name}-${data.aws_vpc.vpc.tags["Name"]}", "_", "-")), "/\\s/", "-")}"
-  role   = "${aws_iam_role.consul_task.id}"
-  policy = "${data.aws_iam_policy_document.consul_task_policy.json}"
BREAKS HERE
-variable "elb_external_cert" {
-    zone_id                = "${aws_elb.prometheus.prometheus_external_elb.zone_id}"
BREAKS HERE
-  version = "0.1.0"
-  version = "0.1.0"
-  version = "0.1.0"
-  version = "0.1.0"
-  version = "0.1.1"
-  version = "0.1.0"
BREAKS HERE
-  description = "A map of tags and values in the same format as other resources accept.  This will be zipmapped into the non-standard format that the aws_autoscaling_group requires."
BREAKS HERE
-    template = "${file("${artifacts_dir}/policy.json")}"
BREAKS HERE
-# Spinnaker, the new hotness
-resource "aws_iam_user" "spinnaker" {
-  name = "spinnaker"
-  path = "/system/"
BREAKS HERE
-    destination = "/tmp/"
BREAKS HERE
-  lb_attached = "${var.load_balancing_type != "NONE"}"
BREAKS HERE
-  
BREAKS HERE
-  default = ["critical", "non-critical"]
BREAKS HERE
-    command              = "${var.command}"
-    command_when_destroy = "${var.command_when_destroy}"
-    command = "${chomp(var.command)} 2>${path.module}/stderr.${self.id} >${path.module}/stdout.${self.id}; echo $? >${path.module}/exitstatus.${self.id}"
-    command = "${var.command_when_destroy == "" ? ":" : chomp(var.command_when_destroy)}"
BREAKS HERE
-  auto_healing_policies {
-    health_check      = "${google_compute_health_check.mig-health-check.self_link}"
-    health_check      = "${google_compute_health_check.mig-health-check.self_link}"
BREAKS HERE
-  container_name  = "${var.name}"
-  container_name = "${var.name}"
BREAKS HERE
-  name = "bosh"
BREAKS HERE
-    computer_name  = "${var.vm_hostname}"
-    computer_name  = "${var.vm_hostname}"
-    computer_name  = "${var.vm_hostname}"
-    computer_name  = "${var.vm_hostname}"
BREAKS HERE
-  iam_instance_profile = "${ aws_iam_instance_profile.master.name }"
BREAKS HERE
-# Subnets with AZ-A
-  cidr_block        = "${var.subnet-1a-prv["cidr_block"]}"
-# route table association Zone-A
BREAKS HERE
-  vpc_cidr_block = "10.77.0.0/16"
-  namespace      = "ecsV3"
-  source     = "../../../network"
-  name       = "${local.namespace}"
-  cidr_block = "${local.vpc_cidr_block}"
-  az_count   = "2"
BREAKS HERE
---project ${var.project_id}
BREAKS HERE
-  default_route_table_id = "{aws_vpc.tf_vpc.default_route_table.id}"
BREAKS HERE
-
BREAKS HERE
-  target_tags       = ["nat-${var.zone}"]
-  name              = "nat-gateway-${var.zone}"
-  network_ip        = "${var.ip == "" ? lookup(var.region_params["${var.region}"], "ip") : var.ip}"
-  name        = "nat-${var.zone}"
-  dest_range  = "0.0.0.0/0"
-  network     = "${var.network}"
-  next_hop_ip = "${var.ip == "" ? lookup(var.region_params["${var.region}"], "ip") : var.ip}"
-  tags        = ["${compact(concat(list("nat-${var.region}"), var.tags))}"]
-  priority    = "${var.route_priority}"
-  name    = "nat-${var.zone}"
-  source_tags = ["${compact(concat(list("nat-${var.zone}"), var.tags))}"]
-  target_tags = ["${compact(concat(list("nat-${var.zone}"), var.tags))}"]
-  name = "nat-${var.zone}"
BREAKS HERE
-# 4647 ... rcp, for communication beteen clients and servers
-# 4647 ... rcp, for communication beteen clients and servers
-# 4647 ... rcp, for communication beteen clients and servers
-# 4647 ... rcp, for communication beteen clients and servers
-# 4647 ... rcp, for communication beteen clients and servers
-# 4647 ... rcp, for communication beteen clients and servers
-# 4647 ... rcp, for communication beteen clients and servers
-# 4647 ... rcp, for communication beteen clients and servers
-# 4647 ... rcp, for communication beteen clients and servers
-# 4647 ... rcp, for communication beteen clients and servers
-# 4647 ... rcp, for communication beteen clients and servers
-# 4647 ... rcp, for communication beteen clients and servers
BREAKS HERE
-  profile = "${matchbox_profile.controller.name}"
-    domain_name          = "${element(var.controller_domains, count.index)}"
-    etcd_name            = "${element(var.controller_names, count.index)}"
-    etcd_initial_cluster = "${join(",", formatlist("%s=https://%s:2380", var.controller_names, var.controller_domains))}"
-    etcd_on_host         = "${var.experimental_self_hosted_etcd ? "false" : "true"}"
-    k8s_dns_service_ip   = "${module.bootkube.kube_dns_service_ip}"
-    ssh_authorized_key   = "${var.ssh_authorized_key}"
-  profile = "${matchbox_profile.worker.name}"
-
-  metadata {
-    domain_name        = "${element(var.worker_domains, count.index)}"
-    etcd_on_host       = "${var.experimental_self_hosted_etcd ? "false" : "true"}"
-    k8s_dns_service_ip = "${module.bootkube.kube_dns_service_ip}"
-    ssh_authorized_key = "${var.ssh_authorized_key}"
-  }
BREAKS HERE
-  name = "_etcd-server._tcp"
BREAKS HERE
-    "default",
-    #uuid           = "${openstack_networking_network_v2.network.id}"
-    port = "${element(openstack_networking_port_v2.network_port.*.id, count.index)}"
-    # access_network = true
-  }
-}
-
-resource "openstack_networking_port_v2" "network_port" {
-  count          = "${var.instance_count}"
-  network_id     = "${openstack_networking_network_v2.network.id}"
-  admin_state_up = "true"
-  fixed_ip       = {
-    subnet_id    = "${openstack_networking_subnet_v2.subnet.id}"
-
-#resource "openstack_compute_floatingip_associate_v2" "fip_1" {
-  #count       = "${var.instance_count}"
-  #floating_ip = "${element(openstack_compute_floatingip_v2.fip.*.address, count.index)}"
-  #instance_id = "${element(openstack_compute_instance_v2.webserver.*.id, count.index)}"
-#}
BREAKS HERE
-    value = "1GB"
-  parameter_group_name = "ship_db_pg_migrate"
BREAKS HERE
-
-
-resource "openstack_networking_router_v2" "router" {
-#### HTTP NETWORK ####
-
-resource "openstack_networking_network_v2" "network_http" {
-  name                = "${var.network_http["network_name"]}"
-# Network configuration
-resource "openstack_networking_subnet_v2" "subnet_http" {
-  network_id          = "${openstack_networking_network_v2.network_http.id}"
-resource "openstack_networking_router_interface_v2" "router_interface_http" {
-  router_id           = "${openstack_networking_router_v2.router.id}"
-  subnet_id           = "${openstack_networking_subnet_v2.subnet_http.id}"
-# Network creation
-resource "openstack_networking_network_v2" "network_db" {
-  name                = "${var.network_db["network_name"]}"
-}
-
-# Network configuration
-resource "openstack_networking_subnet_v2" "subnet_db" {
-  network_id          = "${openstack_networking_network_v2.network_db.id}"
-resource "openstack_networking_router_interface_v2" "router_interface_db" {
-  router_id           = "${openstack_networking_router_v2.router.id}"
-  subnet_id           = "${openstack_networking_subnet_v2.subnet_db.id}"
BREAKS HERE
-    device_id = "${var.alexa_device_id}"
BREAKS HERE
-    command = "mkdir -p ${path.module}/lambda && mkdir -P  ${path.module}/tmp && cp  ${path.module}/ebs_bckup/ebs_bckup.py  ${path.module}/tmp/ebs_bckup.py && echo ${data.template_file.vars.rendered} >  ${path.module}/tmp/vars.ini"
BREAKS HERE
-      port_range  = "all"
-      port_range  = "all"
-      protocol              = "icmp"
-      port_range            = "all"
-      protocol              = "tcp"
-      port_range            = "all"
BREAKS HERE
-
-# ---------------------------------------------------------------------------------------------------------------------
-# CONNECTIONS TO BOOTNODES
-# ---------------------------------------------------------------------------------------------------------------------
-resource "aws_vpc_peering_connection" "vault_to_bootnode_us_east_1" {
-  count = "${lookup(var.bootnode_vpc_peering_counts, "us-east-1", 0)}"
-
-  vpc_id      = "${var.quorum_vault_vpc_id}"
-  peer_vpc_id = "${lookup(var.bootnode_vpcs, "us-east-1")}"
-  peer_region = "us-east-1"
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode in us-east-1"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "us-east-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection_accepter" "vault_to_bootnode_us_east_1" {
-  provider = "aws.us-east-1"
-
-  count = "${lookup(var.bootnode_vpc_peering_counts, "us-east-1", 0)}"
-
-  vpc_peering_connection_id = "${aws_vpc_peering_connection.vault_to_bootnode_us_east_1.id}"
-  auto_accept               = true
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode from vault"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "us-east-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection" "vault_to_bootnode_us_east_2" {
-  count = "${lookup(var.bootnode_vpc_peering_counts, "us-east-2", 0)}"
-
-  vpc_id      = "${var.quorum_vault_vpc_id}"
-  peer_vpc_id = "${lookup(var.bootnode_vpcs, "us-east-2")}"
-  peer_region = "us-east-2"
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode in us-east-2"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "us-east-2"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection_accepter" "vault_to_bootnode_us_east_2" {
-  provider = "aws.us-east-2"
-
-  count = "${lookup(var.bootnode_vpc_peering_counts, "us-east-2", 0)}"
-
-  vpc_peering_connection_id = "${aws_vpc_peering_connection.vault_to_bootnode_us_east_2.id}"
-  auto_accept               = true
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode from vault"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "us-east-2"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection" "vault_to_bootnode_us_west_1" {
-  count = "${lookup(var.bootnode_vpc_peering_counts, "us-west-1", 0)}"
-
-  vpc_id      = "${var.quorum_vault_vpc_id}"
-  peer_vpc_id = "${lookup(var.bootnode_vpcs, "us-west-1")}"
-  peer_region = "us-west-1"
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode in us-west-1"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "us-west-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection_accepter" "vault_to_bootnode_us_west_1" {
-  provider = "aws.us-west-1"
-
-  count = "${lookup(var.bootnode_vpc_peering_counts, "us-west-1", 0)}"
-
-  vpc_peering_connection_id = "${aws_vpc_peering_connection.vault_to_bootnode_us_west_1.id}"
-  auto_accept               = true
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode from vault"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "us-west-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection" "vault_to_bootnode_us_west_2" {
-  count = "${lookup(var.bootnode_vpc_peering_counts, "us-west-2", 0)}"
-
-  vpc_id      = "${var.quorum_vault_vpc_id}"
-  peer_vpc_id = "${lookup(var.bootnode_vpcs, "us-west-2")}"
-  peer_region = "us-west-2"
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode in us-west-2"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "us-west-2"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection_accepter" "vault_to_bootnode_us_west_2" {
-  provider = "aws.us-west-2"
-
-  count = "${lookup(var.bootnode_vpc_peering_counts, "us-west-2", 0)}"
-
-  vpc_peering_connection_id = "${aws_vpc_peering_connection.vault_to_bootnode_us_west_2.id}"
-  auto_accept               = true
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode from vault"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "us-west-2"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection" "vault_to_bootnode_eu_central_1" {
-  count = "${lookup(var.bootnode_vpc_peering_counts, "eu-central-1", 0)}"
-
-  vpc_id      = "${var.quorum_vault_vpc_id}"
-  peer_vpc_id = "${lookup(var.bootnode_vpcs, "eu-central-1")}"
-  peer_region = "eu-central-1"
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode in eu-central-1"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "eu-central-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection_accepter" "vault_to_bootnode_eu_central_1" {
-  provider = "aws.eu-central-1"
-
-  count = "${lookup(var.bootnode_vpc_peering_counts, "eu-central-1", 0)}"
-
-  vpc_peering_connection_id = "${aws_vpc_peering_connection.vault_to_bootnode_eu_central_1.id}"
-  auto_accept               = true
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode from vault"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "eu-central-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection" "vault_to_bootnode_eu_west_1" {
-  count = "${lookup(var.bootnode_vpc_peering_counts, "eu-west-1", 0)}"
-
-  vpc_id      = "${var.quorum_vault_vpc_id}"
-  peer_vpc_id = "${lookup(var.bootnode_vpcs, "eu-west-1")}"
-  peer_region = "eu-west-1"
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode in eu-west-1"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "eu-west-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection_accepter" "vault_to_bootnode_eu_west_1" {
-  provider = "aws.eu-west-1"
-
-  count = "${lookup(var.bootnode_vpc_peering_counts, "eu-west-1", 0)}"
-
-  vpc_peering_connection_id = "${aws_vpc_peering_connection.vault_to_bootnode_eu_west_1.id}"
-  auto_accept               = true
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode from vault"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "eu-west-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection" "vault_to_bootnode_eu_west_2" {
-  count = "${lookup(var.bootnode_vpc_peering_counts, "eu-west-2", 0)}"
-
-  vpc_id      = "${var.quorum_vault_vpc_id}"
-  peer_vpc_id = "${lookup(var.bootnode_vpcs, "eu-west-2")}"
-  peer_region = "eu-west-2"
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode in eu-west-2"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "eu-west-2"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection_accepter" "vault_to_bootnode_eu_west_2" {
-  provider = "aws.eu-west-2"
-
-  count = "${lookup(var.bootnode_vpc_peering_counts, "eu-west-2", 0)}"
-
-  vpc_peering_connection_id = "${aws_vpc_peering_connection.vault_to_bootnode_eu_west_2.id}"
-  auto_accept               = true
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode from vault"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "eu-west-2"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection" "vault_to_bootnode_ap_south_1" {
-  count = "${lookup(var.bootnode_vpc_peering_counts, "ap-south-1", 0)}"
-
-  vpc_id      = "${var.quorum_vault_vpc_id}"
-  peer_vpc_id = "${lookup(var.bootnode_vpcs, "ap-south-1")}"
-  peer_region = "ap-south-1"
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode in ap-south-1"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "ap-south-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection_accepter" "vault_to_bootnode_ap_south_1" {
-  provider = "aws.ap-south-1"
-
-  count = "${lookup(var.bootnode_vpc_peering_counts, "ap-south-1", 0)}"
-
-  vpc_peering_connection_id = "${aws_vpc_peering_connection.vault_to_bootnode_ap_south_1.id}"
-  auto_accept               = true
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode from vault"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "ap-south-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection" "vault_to_bootnode_ap_northeast_1" {
-  count = "${lookup(var.bootnode_vpc_peering_counts, "ap-northeast-1", 0)}"
-
-  vpc_id      = "${var.quorum_vault_vpc_id}"
-  peer_vpc_id = "${lookup(var.bootnode_vpcs, "ap-northeast-1")}"
-  peer_region = "ap-northeast-1"
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode in ap-northeast-1"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "ap-northeast-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection_accepter" "vault_to_bootnode_ap_northeast_1" {
-  provider = "aws.ap-northeast-1"
-
-  count = "${lookup(var.bootnode_vpc_peering_counts, "ap-northeast-1", 0)}"
-
-  vpc_peering_connection_id = "${aws_vpc_peering_connection.vault_to_bootnode_ap_northeast_1.id}"
-  auto_accept               = true
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode from vault"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "ap-northeast-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection" "vault_to_bootnode_ap_northeast_2" {
-  count = "${lookup(var.bootnode_vpc_peering_counts, "ap-northeast-2", 0)}"
-
-  vpc_id      = "${var.quorum_vault_vpc_id}"
-  peer_vpc_id = "${lookup(var.bootnode_vpcs, "ap-northeast-2")}"
-  peer_region = "ap-northeast-2"
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode in ap-northeast-2"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "ap-northeast-2"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection_accepter" "vault_to_bootnode_ap_northeast_2" {
-  provider = "aws.ap-northeast-2"
-
-  count = "${lookup(var.bootnode_vpc_peering_counts, "ap-northeast-2", 0)}"
-
-  vpc_peering_connection_id = "${aws_vpc_peering_connection.vault_to_bootnode_ap_northeast_2.id}"
-  auto_accept               = true
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode from vault"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "ap-northeast-2"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection" "vault_to_bootnode_ap_southeast_1" {
-  count = "${lookup(var.bootnode_vpc_peering_counts, "ap-southeast-1", 0)}"
-
-  vpc_id      = "${var.quorum_vault_vpc_id}"
-  peer_vpc_id = "${lookup(var.bootnode_vpcs, "ap-southeast-1")}"
-  peer_region = "ap-southeast-1"
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode in ap-southeast-1"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "ap-southeast-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection_accepter" "vault_to_bootnode_ap_southeast_1" {
-  provider = "aws.ap-southeast-1"
-
-  count = "${lookup(var.bootnode_vpc_peering_counts, "ap-southeast-1", 0)}"
-
-  vpc_peering_connection_id = "${aws_vpc_peering_connection.vault_to_bootnode_ap_southeast_1.id}"
-  auto_accept               = true
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode from vault"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "ap-southeast-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection" "vault_to_bootnode_ap_southeast_2" {
-  count = "${lookup(var.bootnode_vpc_peering_counts, "ap-southeast-2", 0)}"
-
-  vpc_id      = "${var.quorum_vault_vpc_id}"
-  peer_vpc_id = "${lookup(var.bootnode_vpcs, "ap-southeast-2")}"
-  peer_region = "ap-southeast-2"
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode in ap-southeast-2"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "ap-southeast-2"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection_accepter" "vault_to_bootnode_ap_southeast_2" {
-  provider = "aws.ap-southeast-2"
-
-  count = "${lookup(var.bootnode_vpc_peering_counts, "ap-southeast-2", 0)}"
-
-  vpc_peering_connection_id = "${aws_vpc_peering_connection.vault_to_bootnode_ap_southeast_2.id}"
-  auto_accept               = true
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode from vault"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "ap-southeast-2"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection" "vault_to_bootnode_ca_central_1" {
-  count = "${lookup(var.bootnode_vpc_peering_counts, "ca-central-1", 0)}"
-
-  vpc_id      = "${var.quorum_vault_vpc_id}"
-  peer_vpc_id = "${lookup(var.bootnode_vpcs, "ca-central-1")}"
-  peer_region = "ca-central-1"
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode in ca-central-1"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "ca-central-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection_accepter" "vault_to_bootnode_ca_central_1" {
-  provider = "aws.ca-central-1"
-
-  count = "${lookup(var.bootnode_vpc_peering_counts, "ca-central-1", 0)}"
-
-  vpc_peering_connection_id = "${aws_vpc_peering_connection.vault_to_bootnode_ca_central_1.id}"
-  auto_accept               = true
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode from vault"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "ca-central-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection" "vault_to_bootnode_sa_east_1" {
-  count = "${lookup(var.bootnode_vpc_peering_counts, "sa-east-1", 0)}"
-
-  vpc_id      = "${var.quorum_vault_vpc_id}"
-  peer_vpc_id = "${lookup(var.bootnode_vpcs, "sa-east-1")}"
-  peer_region = "sa-east-1"
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode in sa-east-1"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "sa-east-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
-
-resource "aws_vpc_peering_connection_accepter" "vault_to_bootnode_sa_east_1" {
-  provider = "aws.sa-east-1"
-
-  count = "${lookup(var.bootnode_vpc_peering_counts, "sa-east-1", 0)}"
-
-  vpc_peering_connection_id = "${aws_vpc_peering_connection.vault_to_bootnode_sa_east_1.id}"
-  auto_accept               = true
-
-  tags {
-    Name       = "Network ${var.network_id} VPC peering to bootnode from vault"
-    NetworkId  = "${var.network_id}"
-    FromRegion = "${var.primary_region}"
-    ToRegion   = "sa-east-1"
-    ToType     = "Bootnode"
-    FromType   = "Vault"
-  }
-}
BREAKS HERE
-    etcd          = "3.1.8"
-    prometheus    = "v1.7.1"
-    alertmanager  = "v0.7.1"
-    monitoring    = "1.3.0"
-    kubernetes    = "1.6.7+tectonic.1"
-    tectonic      = "1.6.7-tectonic.1"
-    tectonic-etcd = "0.0.1"
BREAKS HERE
-  vpc_id = "${aws_vpc.openshift.id}"
BREAKS HERE
-resource "docker_image" "headerdebug" {
-  name          = "${data.docker_registry_image.headerdebug.name}"
-  pull_triggers = ["${data.docker_registry_image.headerdebug.sha256_digest}"]
-}
BREAKS HERE
-  security_groups = ["${data.terraform_remote_state.infra_security_groups.sg_offsite_ssh_id}"]
BREAKS HERE
-resource "aws_route53_record" "18f_gov_lean-product-design_18f_gov-route_a" {
-  name = "lean-product-design.18f.gov-route."
BREAKS HERE
-  name = "${var.base_iam_role_name}"
-  name = "${var.base_iam_role_name}_profile"
-  name = "${var.jenkins_iam_role_name}"
-  name = "jenkins_policy"
-  name = "jenkins_profile"
-  name = "${var.properties_and_logging_iam_role_name}"
-  name = "tagging_policy"
-  name = "properties_and_logging_profile"
-  name = "spinnaker"
-    name = "spinnaker"
-  name = "${var.spinnaker_iam_role_name}"
-  name = "spinnaker_admin_policy"
-  name = "spinnaker_profile"
BREAKS HERE
-  description = "Namespace (e.g. `cp` or `cloudposse`)"
-  description = "Name  (e.g. `bastion` or `db`)"
-  description = "Delimiter to be used between `name`, `namespace`, `stage`, etc."
-  description = "Additional attributes (e.g. `policy` or `role`)"
-  description = "Additional tags (e.g. map('BusinessUnit`,`XYZ`)"
-  description = "When set, will cause CloudFront to request your content from a directory in your Amazon S3 bucket or your custom origin. It must begin with a `/`. Do not add a `/` at the end of the path."
-aws-cli is a bucket owned by amazon that will perminantly exist
-It doesn't get used for anything else, this is a safe workaround for handling the fact that 
-Don't change this bucket name, its a variable so that we can provide this description.
BREAKS HERE
-    generation = "${var.generation}"
-  value = "${var.generation}"
BREAKS HERE
-  version = "~> 2.3.0"
-  version = "~> 2.3.0"
BREAKS HERE
-    "${var.aws_elb_subnet}"]
BREAKS HERE
-  records = ["v=spf1 include:spf.mandrillapp.com ?all"]
-  records = ["v=DKIM1; k=rsa; p=MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCrLHiExVd55zd/IQ/J/mRwSRMAocV/hMB3jXwaHH36d9NaVynQFYV8NaWi69c1veUtRzGt7yAioXqLj7Z4TeEUoOLgrKsn8YnckGs9i3B3tVFB+Ch/4mPhXWiNfNdynHWBcPcbJ8kjEQ2U8y78dHZj1YeRXXVvWob2OaKynO8/lQIDAQAB;"]
-  records = ["v=spf1 include:spf.mandrillapp.com ?all"]
-  records = ["v=DKIM1; k=rsa; p=MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCrLHiExVd55zd/IQ/J/mRwSRMAocV/hMB3jXwaHH36d9NaVynQFYV8NaWi69c1veUtRzGt7yAioXqLj7Z4TeEUoOLgrKsn8YnckGs9i3B3tVFB+Ch/4mPhXWiNfNdynHWBcPcbJ8kjEQ2U8y78dHZj1YeRXXVvWob2OaKynO8/lQIDAQAB;"]
-  records = ["v=spf1 include:spf.mandrillapp.com ?all"]
-  records = ["v=DKIM1; k=rsa; p=MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCrLHiExVd55zd/IQ/J/mRwSRMAocV/hMB3jXwaHH36d9NaVynQFYV8NaWi69c1veUtRzGt7yAioXqLj7Z4TeEUoOLgrKsn8YnckGs9i3B3tVFB+Ch/4mPhXWiNfNdynHWBcPcbJ8kjEQ2U8y78dHZj1YeRXXVvWob2OaKynO8/lQIDAQAB;"]
-  records = ["v=spf1 include:spf.mandrillapp.com ?all"]
-  records = ["v=DKIM1; k=rsa; p=MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCrLHiExVd55zd/IQ/J/mRwSRMAocV/hMB3jXwaHH36d9NaVynQFYV8NaWi69c1veUtRzGt7yAioXqLj7Z4TeEUoOLgrKsn8YnckGs9i3B3tVFB+Ch/4mPhXWiNfNdynHWBcPcbJ8kjEQ2U8y78dHZj1YeRXXVvWob2OaKynO8/lQIDAQAB;"]
-  records = ["v=spf1 include:spf.mandrillapp.com ?all"]
-  records = ["v=DKIM1; k=rsa; p=MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCrLHiExVd55zd/IQ/J/mRwSRMAocV/hMB3jXwaHH36d9NaVynQFYV8NaWi69c1veUtRzGt7yAioXqLj7Z4TeEUoOLgrKsn8YnckGs9i3B3tVFB+Ch/4mPhXWiNfNdynHWBcPcbJ8kjEQ2U8y78dHZj1YeRXXVvWob2OaKynO8/lQIDAQAB;"]
BREAKS HERE
-    destination       = "req.url"
-    source            = "regsub(req.url, \"^/education-standards\", \"\");"
BREAKS HERE
- *      source            = "github.com/segmentio/stack"
-  value = "${aws_eip.bastion.external_ip}"
BREAKS HERE
-    public_ip_address_id          = "${azurerm_public_ip.tectonic_api_ip.id}"
-    public_ip_address_id          = "${azurerm_public_ip.tectonic_console_ip.id}"
BREAKS HERE
-    command = "${local.assume_role_cmd} && ${var.cmd}"
-    command = "${local.assume_role_cmd} && ${var.destroy_cmd}"
-  description = "The ID of the null_resource used to provison the resource via cli. Useful for creating dependecies between cli resources"
BREAKS HERE
-resource "aws_s3_bucket" "content_publisher_activestorage" {
-resource "aws_iam_user" "content_publisher_app" {
-  name        = "govuk-${var.aws_environment}-content-publisher-app-s3-writer-policy"
-  policy      = "${data.template_file.s3_writer_policy_template.rendered}"
-  description = "Allows writing to to the govuk-${var.aws_environment}-content-publisher-activestorage"
-  name       = "archive-writer-policy-attachment"
-  users      = ["${aws_iam_user.content_publisher_app.name}"]
-data "template_file" "s3_writer_policy_template" {
-  template = "${file("${path.module}/../../policies/content_publisher_s3_writer_policy.tpl")}"
-    bucket          = "${aws_s3_bucket.content_publisher_activestorage.id}"
-
-# Outputs
-# --------------------------------------------------------------
-
-output "s3_writer_bucket_policy_arn" {
-  value       = "${aws_iam_policy.s3_writer.arn}"
-  description = "ARN of the S3 writer bucket policy"
-}
BREAKS HERE
- ]3
-resource "aws_elb" "lb_msg" {
BREAKS HERE
-  source = "workers"
BREAKS HERE
-    namespace = "aws:elasticbeanstalk:command"
-    name      = "DeploymentPolicy"
-    value     = "Rolling"
-  }
-  setting {
-    value     = "true"
BREAKS HERE
-    command = "echo \"domain=${ var.use_cloudflare == true ? format("%s.%s", var.cluster_prefix, var.cloudflare_domain) : format("%s.nip.io", element(concat(var.edge_public_ip, var.master_public_ip), 0))}\" >> inventory"
BREAKS HERE
-
-output "stdout" {
-  value = "${chomp(data.local_file.stdout.content)}"
-}
-
-output "stderr" {
-  value = "${chomp(data.local_file.stderr.content)}"
-}
-
-output "exitstatus" {
-  value = "${chomp(data.local_file.exitstatus.content)}"
-}
BREAKS HERE
-  desired_capacity      = "${lookup(var.worker_groups[count.index], "asg_desired_capacity", lookup(local.workers_group_defaults, "asg_desired_capacity"))}"
-  max_size              = "${lookup(var.worker_groups[count.index], "asg_max_size",lookup(local.workers_group_defaults, "asg_max_size"))}"
-  min_size              = "${lookup(var.worker_groups[count.index], "asg_min_size",lookup(local.workers_group_defaults, "asg_min_size"))}"
-  vpc_zone_identifier   = ["${split(",", coalesce(lookup(var.worker_groups[count.index], "subnets", ""), lookup(local.workers_group_defaults, "subnets")))}"]
-  protect_from_scale_in = "${lookup(var.worker_groups[count.index], "protect_from_scale_in", lookup(local.workers_group_defaults, "protect_from_scale_in"))}"
-      map("key", "k8s.io/cluster-autoscaler/${lookup(var.worker_groups[count.index], "autoscaling_enabled", lookup(local.workers_group_defaults, "autoscaling_enabled")) == 1 ? "enabled" : "disabled"  }", "value", "true", "propagate_at_launch", false),
-  associate_public_ip_address = "${lookup(var.worker_groups[count.index], "public_ip", lookup(local.workers_group_defaults, "public_ip"))}"
-  security_groups             = ["${local.worker_security_group_id}", "${var.worker_additional_security_group_ids}", "${compact(split(",",lookup(var.worker_groups[count.index],"additional_security_group_ids",lookup(local.workers_group_defaults, "additional_security_group_ids"))))}"]
-  iam_instance_profile        = "${element(aws_iam_instance_profile.workers.*.id, count.index)}"
-  image_id                    = "${lookup(var.worker_groups[count.index], "ami_id", lookup(local.workers_group_defaults, "ami_id"))}"
-  instance_type               = "${lookup(var.worker_groups[count.index], "instance_type", lookup(local.workers_group_defaults, "instance_type"))}"
-  key_name                    = "${lookup(var.worker_groups[count.index], "key_name", lookup(local.workers_group_defaults, "key_name"))}"
-  ebs_optimized               = "${lookup(var.worker_groups[count.index], "ebs_optimized", lookup(local.ebs_optimized, lookup(var.worker_groups[count.index], "instance_type", lookup(local.workers_group_defaults, "instance_type")), false))}"
-  enable_monitoring           = "${lookup(var.worker_groups[count.index], "enable_monitoring", lookup(local.workers_group_defaults, "enable_monitoring"))}"
-  spot_price                  = "${lookup(var.worker_groups[count.index], "spot_price", lookup(local.workers_group_defaults, "spot_price"))}"
-    volume_size           = "${lookup(var.worker_groups[count.index], "root_volume_size", lookup(local.workers_group_defaults, "root_volume_size"))}"
-    volume_type           = "${lookup(var.worker_groups[count.index], "root_volume_type", lookup(local.workers_group_defaults, "root_volume_type"))}"
-    iops                  = "${lookup(var.worker_groups[count.index], "root_iops", lookup(local.workers_group_defaults, "root_iops"))}"
-  triggers = "${map(
-    "key", "${element(keys(var.tags), count.index)}",
-    "value", "${element(values(var.tags), count.index)}",
-    "propagate_at_launch", "true"
-  )}"
BREAKS HERE
-  version = "~> 1.19"
BREAKS HERE
-  source    = "git::https://github.com/cloudposse/tf_label.git?ref=tags/0.1.0"
-  source = "git::https://github.com/cloudposse/tf_subnets.git?ref=tags/0.1.6"
-
-
-  tags {
-    Name      = "${module.label.id}"
-    Namespace = "${var.namespace}"
-    Stage     = "${var.stage}"
-  }
-
-  tags {
-    Name      = "${module.label.id}"
-    Namespace = "${var.namespace}"
-    Stage     = "${var.stage}"
-  }
BREAKS HERE
-    tectonic_cluo_operator       = "quay.io/coreos/tectonic-cluo-operator:v0.2.0"
-    cluo          = "0.2.0"
BREAKS HERE
-resouce "aws_internet_gateway" "igw" {
BREAKS HERE
-resource "aws_route53_record" "18f_gov_fec-style.18f.gov_18f_gov_a" {
-  name = "fec-style.18f.gov.18f.gov."
BREAKS HERE
-  value = "${module.blue.asg_id}"
-  value = "${module.green.asg_id}"
BREAKS HERE
-  name_prefix     = "alb-nomad-ui"
-  name_prefix = "tgr-nomad-ui"
BREAKS HERE
-
-  vpc_id = "${var.vpc_id}"
-  name   = "${module.default_label.id}"
-  tags   = "${module.default_label.tags}"
-  count             = "${var.http_enabled}"
-  count             = "${var.https_enabled}"
-  source     = "git::https://github.com/cloudposse/terraform-aws-lb-s3-bucket.git?ref=init"
-  count             = "${var.http_enabled}"
-  count             = "${var.https_enabled}"
BREAKS HERE
-      "export PUBLIC_IP_ADDRESS=$(curl http://instance-data/latest/meta-data/public-ipv4)"
BREAKS HERE
-      "Name", "${var.cluster_name}-etcd-${count.index}",
BREAKS HERE
-
BREAKS HERE
-  template = "${file("${path.module}/data/aws-iam/policies/codebuild.json")}"
-    file("${path.module}/data/aws-iam/policies/assume-role/codebuild.json")
-  source        = "${path.module}/data/aws-lambda/assets/unknown.svg"
BREAKS HERE
-  source      = "../../tf-modules/s3-remote-state"
-  source       = "../../tf-modules/s3-full-access-policy"
BREAKS HERE
-  zone_id = "${data.openstack_dns_zone_v2.tectonic.id}"
-  zone_id = "${data.openstack_dns_zone_v2.tectonic.id}"
BREAKS HERE
-  source           = "thojkooi/docker-swarm-mode/digitalocean"
-  version          = "0.1.1"
-  source                     = "github.com/thojkooi/terraform-digitalocean-docker-swarm-firewall"
-  do_token                   = "${var.do_token}"
-  prefix                     = "example-com"
-  cluster_tags               = ["${digitalocean_tag.cluster.id}", "${digitalocean_tag.manager.id}", "${digitalocean_tag.worker.id}"]
-  cluster_droplet_ids        = []
-  allowed_outbound_addresses = ["0.0.0.0/0"]
BREAKS HERE
-  description = "Allow IAM Users to push into ECR"
-  description = "Allow IAM Users to pull from ECR"
BREAKS HERE
-    private_ip_address = "${cidrhost(format("10.%d.0.0/16", floor(count.index / var.agent_count)), count.index - (floor(count.index / var.agent_count) * length(var.locations)) + 5 + var.master_count)}"
BREAKS HERE
-  vpc_id                   = "${data.aws_vpc.default.id}"
-  subnet_ids               = "${data.aws_subnet_ids.all.ids}"
-  ami_id                   = "ami-a23feadf"
-  consul_cluster_tag_key   = "consul-servers"
-  consul_cluster_tag_value = "${local.stack_name}-${local.env_name}-consul-srv"
-  server_sg_id             = "${aws_security_group.sg_nomad_server.id}"
BREAKS HERE
-    prevent_destroy = true
BREAKS HERE
-    "https://d1wh5biaq5z7yu.cloudfront.net/."
BREAKS HERE
-  filename = "${var.config_output_path}/config-map-aws-auth_${var.cluster_name}.yaml"
-    command = "kubectl apply -f ${var.config_output_path}/config-map-aws-auth_${var.cluster_name}.yaml --kubeconfig ${var.config_output_path}/kubeconfig_${var.cluster_name}"
BREAKS HERE
-  project       = "${element(var.firewall_projects, count.index)}"
BREAKS HERE
-  cloudinit = "${libvirt_cloudinit_disk.cloudinit_disk.id}"
BREAKS HERE
-  path = "${data.terraform_remote_state.k8s_cluster.vault_user}-gke-${data.terraform_remote_state.k8s_cluster.environment}"
BREAKS HERE
-resource "random_string" "suffix" {
-  length  = 8
-  special = false
-  upper   = false
-}
-
-  network_name = "pf-test-int-full-${random_string.suffix.result}"
-  name   = "pf-ci-test-full-${random_string.suffix.result}"
BREAKS HERE
-    name = "d20zf20jb1pccz.cloudfront.net."
BREAKS HERE
-  credentials_file_path = "${path.module}/sa-key.json"
-  credentials = "${file(local.credentials_file_path)}"
-  project_id        = "${var.project_id}"
-  name              = "deploy-service-cluster"
-  region            = "${var.region}"
-  network           = "${var.network}"
-  subnetwork        = "${var.subnetwork}"
-  ip_range_pods     = "${var.ip_range_pods}"
-  ip_range_services = "${var.ip_range_services}"
BREAKS HERE
-  default     = "true"
BREAKS HERE
-  cluster_type_output_regional_zones = "${concat(google_container_cluster.primary.*.additional_zones, list(list()))}"
-  cluster_type_output_zonal_zones    = "${concat(slice(var.zones,1,length(var.zones)), list(list()))}"
-    zonal    = "${concat(google_container_cluster.zonal_primary.*.zone, local.cluster_type_output_zonal_zones[0])}"
BREAKS HERE
-    Name    = "OpenShift VPC"
-    Name    = "OpenShift IGW"
-    Name    = "OpenShift Public Subnet"
-    Name    = "OpenShift Public Route Table"
BREAKS HERE
-      "test -e /etc/fstab || touch /etc/fstab",
BREAKS HERE
-  network       = "${var.network}"
BREAKS HERE
-  
-    bucket_prefix = "tooling"
BREAKS HERE
-  statement_id   = "AllowExecutionFromCloudWatch"
-  principal      = "events.amazonaws.com"
-  source_arn     = "${aws_sns_topic_subscription.lambda-subscription.arn}"
BREAKS HERE
-  name                   = "${local.api_gateway_0_authorizer}"
-  name                   = "${local.api_gateway_0_authorizer}"
-  name                   = "${local.api_gateway_1_authorizer}"
-  name                   = "${local.api_gateway_1_authorizer}"
-  name                   = "${local.api_gateway_2_authorizer}"
-  name                   = "${local.api_gateway_2_authorizer}"
-  name                   = "${local.api_gateway_3_authorizer}"
-  name                   = "${local.api_gateway_3_authorizer}"
-  name                   = "${local.api_gateway_4_authorizer}"
-  name                   = "${local.api_gateway_4_authorizer}"
-  name                   = "${local.api_gateway_5_authorizer}"
-  name                   = "${local.api_gateway_5_authorizer}"
-  name                   = "${local.api_gateway_6_authorizer}"
-  name                   = "${local.api_gateway_6_authorizer}"
-  name                   = "${local.api_gateway_7_authorizer}"
-  name                   = "${local.api_gateway_7_authorizer}"
-  name                   = "${local.api_gateway_8_authorizer}"
-  name                   = "${local.api_gateway_8_authorizer}"
-  name                   = "${local.api_gateway_9_authorizer}"
-  name                   = "${local.api_gateway_9_authorizer}"
BREAKS HERE
-    instance_port = 50000
-    target = "HTTP:50000/"
-    instance_port = 50000
-    target = "HTTP:50000/"
BREAKS HERE
-  associate_public_ip_address = false
BREAKS HERE
-
-    // Set explicit name to match the new default name set by the API.
-    // https://github.com/terraform-providers/terraform-provider-google/issues/574
-    device_name = "persistent-disk-0"
BREAKS HERE
-  user_data                   = "${template_file.cloud_config.rendered}"
BREAKS HERE
-
-  geth_attach_script = <<EOF
-#!/bin/bash
-
-sudo docker run --rm -it ${local.quorum_docker_image} attach http://$ip:${local.quorum_rpc_port} $@
-EOF
-  echo '${local.geth_attach_script}' > $script
BREAKS HERE
-  name                      = "${var.asg_name_prefix}${var.stack_item_label}"
-    value               = "${var.asg_name_prefix}${var.stack_item_label}"
-  name                      = "${var.asg_name_prefix}${var.stack_item_label}"
-    value               = "${var.asg_name_prefix}${var.stack_item_label}"
BREAKS HERE
-  docker_machine_options_string = "${format(",%s", join(",", formatlist("%q", var.docker_machine_options)))}"
-    runners_instance_profile    = "${aws_iam_instance_profile.runners.name}"
-
-    docker_machine_options = "${length(var.docker_machine_options) == 0 ? "" : local.docker_machine_options_string}"
-    runners_off_peak_periods          = "${var.runners_off_peak_periods}"
-
-################################################################################
-### docker machine runner role and policies
-################################################################################
-data "template_file" "runners_role_trust_policy" {
-  template = "${length(var.instance_role_runner_json) > 0 ? var.instance_role_runner_json : file("${path.module}/policies/instance-role-trust-policy.json")}"
-}
-
-resource "aws_iam_role" "runners" {
-  name               = "${var.environment}-runners-role"
-  assume_role_policy = "${data.template_file.runners_role_trust_policy.rendered}"
-}
-
-resource "aws_iam_instance_profile" "runners" {
-  name = "${var.environment}-runners-profile"
-  role = "${aws_iam_role.runners.name}"
-}
-
-data "template_file" "cache_policy" {
-  template = "${file("${path.module}/policies/cache.json")}"
-
-  vars {
-    s3_cache_arn = "${aws_s3_bucket.build_cache.arn}"
-  }
-}
-
-resource "aws_iam_policy" "runners" {
-  name        = "${var.environment}-runners-cache-policy"
-  path        = "/"
-  description = "Policy for Runners."
-
-  policy = "${data.template_file.cache_policy.rendered}"
-}
-
-resource "aws_iam_role_policy_attachment" "runners" {
-  role       = "${aws_iam_role.runners.name}"
-  policy_arn = "${aws_iam_policy.runners.arn}"
-}
BREAKS HERE
-    bucket        = "${data.terraform_remote_state.infra_aws_logging.aws_logging_bucket_id}"
BREAKS HERE
-  "${var.create_kyc}}",
BREAKS HERE
-  local_subnet   = "${alicloud_vpc.vpc_region-a.cidr_block}"
-        "alicloud_cen_instance_attachment.attachment_region-b"
-resource "alicloud_instance" "proxy" {
BREAKS HERE
-      "modprobe br_netfilter && echo br_netfilter >> /etc/modules"
BREAKS HERE
-data "aws_region" "default" {
-  current = true
-}
BREAKS HERE
-  vpc_cidr_block = "10.78.0.0/16"
BREAKS HERE
-  value = [ "${aws_subnet.elb_subnet.*.id}"]
-  value = [ "${aws_subnet.elb_subnet.*.az}"]
BREAKS HERE
-    substription_id_b64 = "${base64encode(module.azure.substription_id)}"
-    location = "${module.azure.location}"
BREAKS HERE
-    console_base_address = "https://${var.base_address}"
BREAKS HERE
-    option.resource_record_name => option ...
BREAKS HERE
-variable "name" {}
-variable "envname" {}
-variable "envtype" {}
-## patch baseline vars
-
-## maintenance window vars
-
-
-variable "s3_bucket_name" {}
BREAKS HERE
-  name  = "ship_db_pg_migrate"
BREAKS HERE
-  count="${var.number_nat_gateways}"
-  vpc = true
-  count="${var.number_nat_gateways}"
-  subnet_id = "${var.public_subnets[count.index]}"
-    count = "${var.number_nat_gateways}"
-    route_table_id = "${element(var.private_route_tables,count.index)}"
-    destination_cidr_block = "0.0.0.0/0"
-    nat_gateway_id = "${element(aws_nat_gateway.gateway.*.id,count.index)}"
BREAKS HERE
-  user_data = "${file("boostrap-server-${count.index}.sh")}"
-    Name = "server${count.index}"
BREAKS HERE
-
-resource "aws_instance" "validator" {
-  count          = "${var.backup_enabled ? signum(lookup(var.validator_node_counts, var.aws_region, 0)) : 0}"
-  ami           = "${data.aws_ami.bootnode.id}"
-  instance_type = "t2.micro"
-  tags {
-    Name = "Debug-Validator"
-  subnet_id = "${aws_subnet.quorum_validator.0.id}"
-  vpc_security_group_ids = ["${aws_security_group.allow_all_for_backup_lambda.0.id}", 
-    "${aws_security_group.allow_ssh_for_debugging.0.id}"]
-  count          = "${var.backup_enabled ? signum(lookup(var.observer_node_counts, var.aws_region, 0) ) : 0}"
-  ami           = "${data.aws_ami.bootnode.id}"
-
-    Name = "Debug-Observer"
-  subnet_id = "${aws_subnet.quorum_observer.0.id}"
-  vpc_security_group_ids = ["${aws_security_group.allow_all_for_backup_lambda.0.id}", 
-    "${aws_security_group.allow_ssh_for_debugging.0.id}"]
-  count          = "${var.backup_enabled ? signum(lookup(var.maker_node_counts, var.aws_region, 0)) : 0}"
-  ami           = "${data.aws_ami.bootnode.id}"
-
-    Name = "Debug-Maker"
-
-  key_name = "quorum-cluster-${var.aws_region}-network-${var.network_id}"
-  subnet_id = "${aws_subnet.quorum_maker.0.id}"
-  vpc_security_group_ids = ["${aws_security_group.allow_all_for_backup_lambda.0.id}", 
-    "${aws_security_group.allow_ssh_for_debugging.0.id}"]
-}
BREAKS HERE
-    count = 2
-    image = "../packer/output-virtualbox-iso/test-ubuntu-xenial.box"
-    user_data = "${file("install.sh")}"
BREAKS HERE
-# ALB Security Group: Edit this to restrict access to the application
BREAKS HERE
-    command = "echo \"${var.edge_count == 0 ? "" : join("\n",formatlist("%s ansible_ssh_host=%s ansible_ssh_user=ubuntu", slice(module.master.hostnames,0,var.edge_count), module.edge.public_ip))}\" >> inventory"
BREAKS HERE
-  security_groups = [
-  security_groups = [
-  security_groups = [
BREAKS HERE
-   instance_protocol = "tcp"
-   instance_protocol = "tcp"
-    lb_port = 5671
-    lb_protocol = "http"
-    lb_port = 15671
BREAKS HERE
-
-  recreate_pods = false
BREAKS HERE
-
-  tunnel1_inside_cidr   = "169.254.33.88/30"
-  tunnel2_inside_cidr   = "169.254.33.100/30"
-  vpc_id                       = "${module.vpc.vpc_id}"
-  create_vpn_gateway_attachment = false 
-  tunnel1_inside_cidr   = "169.254.34.88/30"
-  tunnel2_inside_cidr   = "169.254.34.100/30"
-  vpc_id                       = "${module.vpc.vpc_id}"
-  create_vpn_gateway_attachment = false 
-
-  azs             = ["eu-west-1a", "eu-west-1b", "eu-west-1c"]
-  private_subnets = ["10.10.1.0/24", "10.10.2.0/24", "10.10.3.0/24"]
-  public_subnets  = [["10.10.11.0/24", "10.10.12.0/24", "10.10.13.0/24"]]
-  intra_subnets = ["10.10.31.0/24", "10.10.32.0/24", "10.10.33.0/24"]
-  enable_vpn_gateway = true
-
-
-
BREAKS HERE
-        "command": [
-            "CMD-SHELL",
-            "ls"
-        ],
BREAKS HERE
-  
-  
-  
-  
-  
BREAKS HERE
-  key_name   = "${local.lower_name}-worker"
BREAKS HERE
-  roles = [
-    "${ aws_iam_role.master.name }"
-  ]
BREAKS HERE
-      "${aws_s3_bucket.default.arn}",
-      "${aws_s3_bucket.default.arn}/${lookup(var.deployment_arns, element(keys(var.deployment_arns), count.index))}",
BREAKS HERE
-  count = "${lookup(var.maker_node_counts, var.aws_region, 0)}"
BREAKS HERE
-  display_name        = "${var.label_prefix}${var.display_name}-${count.index}"
-  hostname_label      = "${var.hostname_label}-${count.index}"
-    display_name      = "${var.label_prefix}${var.display_name}-${count.index}"
-    hostname_label    = "${var.hostname_label}-${count.index}"
BREAKS HERE
-  source    = "git::https://github.com/cloudposse/terraform-terraform-label.git?ref=tags/0.1.2"
-  source              = "git::https://github.com/cloudposse/terraform-aws-alb-ingress.git?ref=tags/0.2.2"
-  source                    = "git::https://github.com/cloudposse/terraform-aws-ecs-alb-service-task.git?ref=tags/0.2.0"
BREAKS HERE
-  name = "terraform_${var.name}_${count.index}_disk"
-  name = "${var.name}_${count.index}"
-    network_name = "${element(list("terraform-network", ""), replace(replace(var.bridge, "/.+/", "1"), "/^$/", "0"))}"
-hostname: ${var.name}${element(list("", "-${count.index  + 1}"), signum(var.count - 1))}
-  value = "${coalesce("${var.name}.${var.domain}", libvirt_domain.domain.id)}"
BREAKS HERE
-  availability_zones = ["${var.availability_zones}"]
BREAKS HERE
-    kubernetes_ami              = "${element(coalescelist(aws_ami_copy.k8s_base_image.*.name,data.aws_ami.kubernetes_ami.*.name),0)}"
-    kubernetes_ami              = "${element(coalescelist(aws_ami_copy.k8s_base_image.*.name,data.aws_ami.kubernetes_ami.*.name),0)}"
BREAKS HERE
-        "postgres",
BREAKS HERE
-  version = "~> 1.0"
BREAKS HERE
-  count       = "${var.enabled == "true" && var.image_id == "" ? 1 : 0}"
-  image_id                  = "${coalesce(var.image_id, join("", data.aws_ami.eks_worker.*.id))}"
BREAKS HERE
-  alias {
-    name = "gsa-elb-ecs-prod-wild-diggov-1-1458076956.us-east-1.elb.amazonaws.com."
-    zone_id = "Z2FDTNDATAQYW2"
-    evaluate_target_health = false
-  }
BREAKS HERE
-
-resource "aws_api_gateway_base_path_mapping" "stage" {
-  api_id      = "${var.api_id}"
-  stage_name  = "${aws_api_gateway_deployment.stage.stage_name}"
-  domain_name = "${var.domain_name}"
-  base_path   = "${var.base_path}"
-}
BREAKS HERE
-  vpc_security_group_ids = ["${aws_security_group.default.id}", "${aws_security_group.mastersg.id}"]
BREAKS HERE
-  default = "Ubuntu Xenial"
BREAKS HERE
-resource "aws_s3_bucket_policy" "terraform_state" {
-  count  = "${var.s3_state_encryption_enabled ? 1 : 0}"
-  bucket = "${aws_s3_bucket.tf_backend_bucket_encrypted.id}"
-  policy = <<EOF
-{
-  "Version": "2012-10-17",
-  "Id": "RequireEncryption",
-   "Statement": [
-    {
-      "Sid": "RequireEncryptedTransport",
-      "Effect": "Deny",
-      "Action": ["s3:*"],
-      "Resource": ["arn:aws:s3:::${aws_s3_bucket.tf_backend_bucket_encrypted.bucket}/*"],
-      "Condition": {
-        "Bool": {
-          "aws:SecureTransport": "false"
-        }
-      },
-      "Principal": "*"
-    },
-    {
-      "Sid": "RequireEncryptedStorage",
-      "Effect": "Deny",
-      "Action": ["s3:PutObject"],
-      "Resource": ["arn:aws:s3:::${aws_s3_bucket.tf_backend_bucket_encrypted.bucket}/*"],
-      "Condition": {
-        "StringNotEquals": {
-          "s3:x-amz-server-side-encryption": "AES256"
-        }
-      },
-      "Principal": "*"
-  ]
-EOF
BREAKS HERE
-  default     = "https://public.update.core-os.net"
BREAKS HERE
-  s3_bucket         = "${var.lambda_bucket_name}"
-  s3_key            = "lamdba-slack.zip"
-  s3_object_version = "${var.lambda_bucket_version}"
BREAKS HERE
-  name                = "${element(keys(var.app_service_name), count.index)}"
-    service_name                   = "${element(keys(var.app_service_name), count.index)}"
BREAKS HERE
-  subnet_id = "${aws_subnet.sn_ship_install.id}"
BREAKS HERE
-    command = "echo '${data.aws_kms_ciphertext.private_key.ciphertext_blob}' | base64 --decode > '${var.private_key_file_path}'"
BREAKS HERE
-  source     = "git::https://github.com/cloudposse/terraform-terraform-label.git?ref=0.1.2"
-  source     = "git::https://github.com/cloudposse/terraform-terraform-label.git?ref=0.1.2"
-  attributes = ["${compact(concat(var.attributes, list("exec", "role")))}"]
-  execution_role_arn       = "${aws_iam_role.ecs_exec_role.arn}"
-  task_role_arn            = "${aws_iam_role.ecs_exec_role.arn}"
-data "aws_iam_policy_document" "ecs_service_role" {
-resource "aws_iam_role" "ecs_role" {
-  assume_role_policy = "${data.aws_iam_policy_document.ecs_service_role.json}"
-resource "aws_iam_role_policy" "ecs_service_role_policy" {
-  role   = "${aws_iam_role.ecs_role.id}"
-data "aws_iam_policy_document" "ecs_task_exec_role" {
-resource "aws_iam_role" "ecs_exec_role" {
-  assume_role_policy = "${data.aws_iam_policy_document.ecs_task_exec_role.json}"
-data "aws_iam_policy_document" "ecs_exec_role" {
-resource "aws_iam_role_policy" "ecs_exec_role_policy" {
-  policy = "${data.aws_iam_policy_document.ecs_exec_role.json}"
-  role   = "${aws_iam_role.ecs_exec_role.id}"
BREAKS HERE
-    iops: "2500"
BREAKS HERE
-  source = "http://cloud-images.ubuntu.com/minimal/releases/bionic/release/ubuntu-18.04-minimal-cloudimg-amd64.img"
BREAKS HERE
-  user_data       = "${data.template_file.init.rendered}"
BREAKS HERE
-  name = "worker-${ var.name }"
BREAKS HERE
-  custom_suffix    = "${format("consul-slr-%s",replace(replace(timestamp(),"-",""),":",""))}"
BREAKS HERE
-  source    = "git::https://github.com/cloudposse/tf_label.git?ref=tags/0.1.0"
-  tags {
-    Name      = "${module.label.id}"
-    Namespace = "${var.namespace}"
-    Stage     = "${var.stage}"
-  }
-    user_data        = "${join("\n", var.user_data)}"
-    welcome_message  = "${var.stage}"
-    hostname         = "${var.name}.${data.aws_route53_zone.domain.name}"
-    search_domains   = "${data.aws_route53_zone.domain.name}"
-    ssh_user         = "${var.ssh_user}"
-  tags {
-    Name      = "${module.label.id}"
-    Namespace = "${var.namespace}"
-    Stage     = "${var.stage}"
-  }
-  source    = "git::https://github.com/cloudposse/tf_hostname.git?ref=tags/0.1.0"
BREAKS HERE
-   security_groups = [
-   "${aws_security_group.sg_public_lb.id}"]
BREAKS HERE
-# EC2 instance
-#
-  count = "${var.instance_count}"
BREAKS HERE
-    provisioner "local-exec" {
-  k8s_app_namespace = "${var.k8s_app_namespace}"
BREAKS HERE
-  source          = "terraform-google-modules/network/google"
-  version         = "~> 0.4.0"
-  network_name    = "pf-test-int-full-${random_string.suffix.result}"
-  project_id      = "${var.shared_vpc}"
-  name                = "pf-ci-test-full-${random_string.suffix.result}"
-  random_project_id   = "true"
-  count = "${module.project-factory.group_email != "" ? 1 : 0}"
-
BREAKS HERE
-  #version     = "~> 1.20"
-module "vpc" {
-  source          = "github.com/terraform-google-modules/terraform-google-network.git"
-  network_name    = "forseti-shared-vpc"
-  project_id      = "${var.shared_project_id}"
-  shared_vpc_host = "true"
-
-  subnets = [
-    {
-      subnet_name           = "${var.subnetwork_name}"
-      subnet_ip             = "${var.subnet_cidr}"
-      subnet_region         = "${var.region}"
-      subnet_private_access = true
-    }
-  ]
-
-  secondary_ranges = {
-    forseti-subnet-01 = []
-  }
-
-}
-
-module "service-project" {
-  source              = "terraform-google-modules/project-factory/google"
-  version             = "v1.0.0"
-  random_project_id   = "true"
-  name                = "forseti-service"
-  org_id              = "${var.org_id}"
-  billing_account     = "${var.billing_account}"
-  credentials_path    = "${var.credentials_path}"
-  shared_vpc          = "${var.shared_project_id}"
-  shared_vpc_subnets  = [
-    "projects/${var.shared_project_id}/regions/${var.region}/subnetworks/${var.subnetwork_name}",
-  ]
-  activate_apis = [
-    "storage-api.googleapis.com",
-    "compute.googleapis.com",
-    "sqladmin.googleapis.com"
-  ]
-  project_id          = "${module.service-project.project_id}"
-  network             = "${module.vpc.network_self_link}"
-  subnetwork          = "forseti-subnet-01"
BREAKS HERE
-    command = "(test -z \"$(kops get cluster | grep ${var.cluster-name})\" ) || kops --state=s3://${var.kops-state-bucket} delete ig --name ${var.cluster-name} --yes ${var.name}"
BREAKS HERE
-    to_port = 0
-    protocol = "all"
-  engine_version = "5.6.17"
BREAKS HERE
-    command = "${path.module}/grant.sh create reader ${var.reader_context == "" ? "" : "--context ${join(",", split(" ", var.reader_context))}"} ${var.kms_key_arn} ${var.roles_arns[count.index]}"
-    command    = "${path.module}/grant.sh revoke ${var.kms_key_arn} ${var.roles_arns[count.index]}"
-    command = "${path.module}/grant.sh create writer ${var.writer_context == "" ? "" : "--context ${join(",", split(" ", var.writer_context))}"} ${var.kms_key_arn} ${var.roles_arns[count.index]}"
-    command    = "${path.module}/grant.sh revoke ${var.kms_key_arn} ${var.roles_arns[count.index]}"
BREAKS HERE
-resource "null_resource" "export-cluster-rendered-template" {
-  triggers {
-    template = "${data.template_file.cluster_config.rendered}"
-  }
-  //The --unregister flag just deletes the kops configurations stored in s3
-  provisioner "local-exec" {
-    command = "${var.kops_executable_name} delete cluster ${var.k8s_cluster_name} --state s3://${var.s3_bucket_name} --unregister --yes || true"
-  }
-  //generating certs using kops
-}
BREAKS HERE
-    name = "dev-api"
BREAKS HERE
-  tags = "${merge(var.tags, map("kubernetes.io/cluster/${var.cluster_name}", "owned"))}"
-  count = "${var.enabled == "true" ? 1 : 0}"
-  count              = "${var.enabled == "true" ? 1 : 0}"
-  count      = "${var.enabled == "true" ? 1 : 0}"
-  count      = "${var.enabled == "true" ? 1 : 0}"
-  count      = "${var.enabled == "true" ? 1 : 0}"
-  count = "${var.enabled == "true" ? 1 : 0}"
-  iam_instance_profile_name = "${join("", aws_iam_instance_profile.default.*.name)}"
-    aws_iam_role_arn = "${join("", aws_iam_role.default.*.arn)}"
BREAKS HERE
-  zone_id = "${aws_route53_zone.tectonic.zone_id}"
BREAKS HERE
-    bucket_prefix = "AWSLogs/${var.stackname}-deploy_elb"
-    interval      = 5
BREAKS HERE
-    command = "echo \"${var.edge_count == 0 ? "" : join("\n",formatlist("%s ansible_ssh_host=%s ansible_ssh_user=ubuntu", slice(var.master_hostnames,0,var.edge_count), var.edge_public_ip))}\" >> inventory"
BREAKS HERE
-# A device to mount target entry is a key value pair (separated by '.').
-# A device to mount target entry is a key value pair (separated by '.').
-# A device to mount target entry is a key value pair (separated by '.').
-# A device to mount target entry is a key value pair (separated by '.').
BREAKS HERE
-  zone    = "${data.google_compute_zones.available.names[0]}"
BREAKS HERE
-  azs             = ["eu-west-1a", "eu-west-1b", "eu-west-1c"]
BREAKS HERE
-  filename         = "${path.module}/es-cleanup.zip"
BREAKS HERE
-  subnet_id = "${aws_subnet.public_subnet.3.id}"
-  subnet_id = "${aws_subnet.public_subnet.4.id}"
-  subnet_id = "${aws_subnet.public_subnet.5.id}"
BREAKS HERE
-
-  name        = "${replace(format("%.255s", lower(replace("tf-redis-${var.name}-${var.env}-${data.aws_vpc.vpc.tags["Name"]}", "_", "-"))), "/\\s/", "-")}"
BREAKS HERE
-      ap-northeast-1  = "ami-c6293ca8"
-      ap-northeast-2  = "ami-38814956"
-      ap-southeast-1  = "ami-85ee3be6"
-      ap-southeast-2  = "ami-dcddfdbf"
-      eu-central-1    = "ami-175ebf78"
-      eu-west-1       = "ami-1955d16a"
-      sa-east-1       = "ami-a723accb"
-      us-east-1       = "ami-edc7cb87"
-      us-west-1       = "ami-ade597cd"
-      us-west-2       = "ami-934ca4f3"
BREAKS HERE
-  availability_zone = "${var.azs[count.index]}"
-  availability_zone = "${var.azs[count.index]}"
BREAKS HERE
-variable "default_route_table_tags" {
-  description = "Additional tags for the default route table"
-  default     = {}
-}
-
BREAKS HERE
-  name                 = "asg-${aws_launch_configuration.ecs.name}"
BREAKS HERE
-data "aws_region" "current" {}
-  region = "${var.region == "" ? data.aws_region.current.name : var.region}"
BREAKS HERE
-//   - Project: Purely for my own organision, delete or change as you like!
BREAKS HERE
-  user_data = "${file("boostrap.sh")}"
BREAKS HERE
-            $PWD/cfssl/generate_server.sh k8s_master ${digitalocean_droplet.k8s_master.ipv4_address}
-            "sudo systemctl enable kubelet",
-            "sudo systemctl start kubelet",
-resource "null_resource" "deploy_dns_addon" {
-    depends_on = ["null_resource.setup_kubectl"]
-    provisioner "local-exec" {
-        command = <<EOF
-            sed -e "s/\$DNS_SERVICE_IP/10.3.0.10/" < 03-dns-addon.yaml > ./secrets/03-dns-addon.rendered.yaml
-            until kubectl get pods 2>/dev/null; do printf '.'; sleep 5; done
-            kubectl create -f ./secrets/03-dns-addon.rendered.yaml
-EOF
-    }
-}
-
-            sed -e "s/\$EXT_IP1/${digitalocean_droplet.k8s_worker.0.ipv4_address}/" < 04-microbot.yaml > ./secrets/04-microbot.rendered.yaml
-            kubectl create -f ./secrets/04-microbot.rendered.yaml
BREAKS HERE
-  name          = "prometheus-operator"
-  chart         = "stable/prometheus-operator"
-  namespace     = "monitoring"
-  recreate_pods = "true"
-  name          = "prometheus-proxy"
-  namespace     = "monitoring"
-  chart         = "stable/oauth2-proxy"
-  version       = "0.9.1"
-  recreate_pods = true
-  name          = "alertmanager-proxy"
-  namespace     = "monitoring"
-  chart         = "stable/oauth2-proxy"
-  version       = "0.9.1"
-  recreate_pods = true
BREAKS HERE
-  length  = 4
-  name                    = "pf-test-int-full-${random_string.suffix.result}"
-  name                = "pf-test-int-full-${random_string.suffix.result}"
BREAKS HERE
-  name      = "k8s-node-${count.index + 1}"
BREAKS HERE
-  user_data            = "#!/bin/bash\napt-get install nginx\nMYIP=`ifconfig | grep 'addr:10' | awk '{ print $2 }' | cut -d ':' -f2`\necho 'this is: '$MYIP > /var/www/html/index.html"
BREAKS HERE
-  subnets = [ "${ split(",", var.subnet-ids-public) }" ]
-  security_groups = [ "${ var.external-elb-security-group-id }" ]
BREAKS HERE
-  memory_limit_in_mb = "400"
-  jvm_memory_limit = "250"
BREAKS HERE
-### Lambda Support Role
-# ---------------------------------------------------------------------------------------------------------------------
-# CREATE INVOCATION POLICY
-# We need to permit the lambda be used @Edge as well as in the traditional manner.
-# ---------------------------------------------------------------------------------------------------------------------
-data "aws_iam_policy_document" "lambda_policy" {
-resource "aws_iam_role" "lambda_lookup" {
-  name               = "ecs-lambda-lookup-${var.name}"
-  description        = "Role permitting Lambda functions to be invoked from Lambda"
-  assume_role_policy = "${data.aws_iam_policy_document.lambda_policy.json}"
-}
-data "aws_iam_policy_document" "logging_policy_document" {
-    resources = ["${format("arn:aws:logs:%s:%s:log-group:/aws/lambda/%s-lambda-lookup:*",var.region, data.aws_caller_identity.current.account_id, var.name)}"]
-    effect    = "Allow"
-    resources = ["${format("arn:aws:logs:%s:%s:log-group:/aws/lambda/%s-lambda-lookup:*.*",var.region, data.aws_caller_identity.current.account_id, var.name)}"]
-    effect    = "Allow"
-data "aws_iam_policy_document" "support_lambda_ecs_permissions" {
-resource "aws_iam_role_policy" "lambda_log_policy" {
-  role   = "${aws_iam_role.lambda_lookup.name}"
-  policy = "${data.aws_iam_policy_document.logging_policy_document.json}"
-resource "aws_iam_role_policy" "lambda_lookup_policy" {
-  role   = "${aws_iam_role.lambda_lookup.name}"
-  policy = "${data.aws_iam_policy_document.support_lambda_ecs_permissions.json}"
BREAKS HERE
-      subnet_region = "us-east2"
BREAKS HERE
-  zone         = "${element(data.google_compute_zones.all.names, count.index)}"
-locals {
-  controllers_ipv4_public = ["${google_compute_instance.controllers.*.network_interface.0.access_config.0.assigned_nat_ip}"]
-}
-
BREAKS HERE
-    	user = "${var.ssh_user}"
-    	key_file = "${var.ssh_private_key_location}"
-    	agent = false
-  	}
-    	inline = [
-      		"mkdir -p /tmp/terraform/"
-    	]
-  	}
-
-  	provisioner "file" {
-    	source = "../files/bastion/"
-    	destination = "/tmp/terraform"
-  	}
-
-  	provisioner "file" {
-    	source = "${var.ssh_private_key_location}"
-    	destination = "/home/${var.ssh_user}/.ssh/id_rsa"
-  	}
-
-  	provisioner "remote-exec" {
-    	inline = [
-      		"chmod 0600 /home/${var.ssh_user}/.ssh/id_rsa",
-      		"chmod a+x /tmp/terraform/provision.sh",
-      		"/tmp/terraform/provision.sh ${var.ssh_user}"
-    	]
-  	}
-  	provisioner "remote-exec" {
-    	inline = [
-      		"mkdir -p /tmp/terraform/"
-    	]
-  	}
-
-  	provisioner "file" {
-    	source = "../files/gcp/spinnaker-and-jenkins/"
-    	destination = "/tmp/terraform"
-  	}
-
-  	provisioner "file" {
-    	source = "../files/lib/"
-    	destination = "/tmp/terraform"
-  	}
-
-  	provisioner "remote-exec" {
-    	inline = [
-      		"chmod a+x /tmp/terraform/provision.sh",
-      		"chmod a+x /tmp/terraform/create_application.sh",
-      		"sudo /tmp/terraform/provision.sh ${google_compute_network.spinnaker-network.name} ${var.jenkins_admin_username} ${var.jenkins_admin_password}",
-      		"/tmp/terraform/create_application.sh"
-    	]
-  	}
-    	command = "ssh -o IdentitiesOnly=yes -o StrictHostKeyChecking=no -i ${var.ssh_private_key_location} ${var.ssh_user}@${google_compute_instance.bastion.network_interface.0.access_config.0.nat_ip} 'sed -i.bak -e \"s/<INTERNAL_DNS>/${google_compute_instance.spinnaker-and-jenkins.network_interface.0.address}/\" /home/${var.ssh_user}/.ssh/config'"
-  	}
-
-#  	provisioner "remote-exec" {
-#    	inline = [
-#      		"sudo rm -rf /tmp/terraform*"
-#    	]
-#  	}
BREAKS HERE
-  instance_elb_ids              = ["${aws_elb.puppetmaster_bootstrap_elb.id}", "${aws_elb.puppetmaster_internal_elb.id}"]
BREAKS HERE
-  description = "Version of Calico to install for pod networking."
BREAKS HERE
-  count = "${var.provider == "SCW" ? 1 : 0}"
-    host                = "${local.use_bastion ? aws_instance.swarm_manager.private_ip : var.manager_ip}"
BREAKS HERE
-  name        = "${var.name}"
BREAKS HERE
-  name        = "control_plane_credhub"
-  name        = "control_plane_uaa"
-  name        = "control_plane_lb"
-  description = "Allow acess to VMs from LB"
-  name               = "credhub-uaa-lb"
-  name     = "uaa-target-group"
-  name     = "credhub-target-group"
BREAKS HERE
-resource "aws_iam_role_policy_attachment" "read_publishing-api-db-admin_database_backups_iam_role_policy_attachment" {
-  count      = 1
-  policy_arn = "${data.terraform_remote_state.infra_database_backups_bucket.publishing-api_dbadmin_read_database_backups_bucket_policy_arn}"
BREAKS HERE
-    inline = "while ! [ -f /etc/profile.d/forseti_environment.sh ]; do sleep 10; done"
-    inline = "while ! [ -f /etc/profile.d/forseti_environment.sh ]; do sleep 10; done"
BREAKS HERE
-      ap-northeast-1  = "ami-c6293ca8"
-      ap-northeast-2  = "ami-38814956"
-      ap-southeast-1  = "ami-85ee3be6"
-      ap-southeast-2  = "ami-dcddfdbf"
-      eu-central-1    = "ami-175ebf78"
-      eu-west-1       = "ami-1955d16a"
-      sa-east-1       = "ami-a723accb"
-      us-east-1       = "ami-edc7cb87"
-      us-west-1       = "ami-ade597cd"
-      us-west-2       = "ami-934ca4f3"
-      # Just plain ubuntu images here for now
-      # From https://cloud-images.ubuntu.com/locator/ec2/
-      cn-north-1      = "ami-0679b06b"
-      us-gov-west-1   = "ami-30b8da13"
-      # Just plain ubuntu images here for now
-      # From https://cloud-images.ubuntu.com/locator/ec2/
-      cn-north-1      = "ami-92f622ff"
-      us-gov-west-1   = "ami-34df6755"
BREAKS HERE
-    var.dimension_name = coalesce(var.dimension_value, var.autoscaling_group_names[count.index])
-    var.dimension_name = coalesce(var.dimension_value, var.autoscaling_group_names[count.index])
BREAKS HERE
-  enabled   = "${var.enabled}"
BREAKS HERE
-    aws_elastic_search_url = "${var.elasticsearch_http_endpoint}"
BREAKS HERE
-  domain_name           = "tf-${var.domain_name}"
-    "Domain", "${var.domain_name}"
-  domain_name     = "tf-${var.domain_name}"
BREAKS HERE
-  image = "${docker_image.image_id.latest}"
BREAKS HERE
-  key_name = "${var.ssh_key_name}"
-    Name = "bastion host"
-  key_name = "${var.ssh_key_name}"
-    Name = "Jenkins host"
-  key_name = "${var.ssh_key_name}"
-      "/tmp/terraform/create_application.sh ${var.region} ${aws_vpc.main.id} ${var.base_iam_role_name} ${var.vpc_name} ${aws_security_group.example_app.id} ${aws_security_group.vpc_sg.id} ${aws_security_group.mgmt_sg.id}"
-  } 
-  
-    Name = "Spinnaker host"
BREAKS HERE
-    command = "${var.command} 2>${path.module}/stderr.${self.id} >${path.module}/stdout.${self.id}; echo $? >${path.module}/exitstatus.${self.id}"
-    command = "${var.command_when_destroy == "" ? ":" : var.command_when_destroy}"
BREAKS HERE
-  zone = "${element(var.zones, 0)}"
BREAKS HERE
-  ip_set_descriptors = "${var.admin_remote_ipset}"
-  ip_set_descriptors = "${var.blacklisted_ips}"
BREAKS HERE
-  gcp_project = "${var.gcp_project}"
-  gcp_zone    = "${var.gcp_zone}"
-  template = "${file("${path.module}/startup-script-vault.sh")}"
-    consul_cluster_tag_name      = "${var.consul_server_cluster_name}"
-    vault_cluster_tag_name       = "${var.vault_cluster_name}"
-    web_proxy_port               = "${var.web_proxy_port}"
-  gcp_zone         = "${var.gcp_zone}"
BREAKS HERE
-  name                        = "${var.name}"
BREAKS HERE
-  source_security_group_id = "${aws_security_group.sg_prometheus_ingress}"
BREAKS HERE
-    assets          = "${path.module}/../../assets"
BREAKS HERE
-   records = ["aspmx.l.google.com"]
BREAKS HERE
-
-output "kube-api-public-fqdn" {
-  value = "${azurerm_public_ip.controller.*.fqdn[0]}"
-}
BREAKS HERE
-        subnet_id                     = "${azurerm_subnet.public_dmz.id}"
BREAKS HERE
-  value = ["${list("google-${var.extra_disk_name}")}"]
BREAKS HERE
-  key_id    = "${var.kms_key_arn}"
-  //   kms_key_arn = "${aws_kms_key.this.arn}"
-
-  kms_key_arn = "${var.kms_key_arn}"
BREAKS HERE
-variable "admin_role_policy_arn" {
-  description = "Policy ARN to use for admin role"
-  default     = "arn:aws:iam::aws:policy/AdministratorAccess"
-variable "poweruser_role_policy_arn" {
-  description = "Policy ARN to use for poweruser role"
-  default     = "arn:aws:iam::aws:policy/PowerUserAccess"
-variable "readonly_role_policy_arn" {
-  description = "Policy ARN to use for readonly role"
-  default     = "arn:aws:iam::aws:policy/ReadOnlyAccess"
BREAKS HERE
-    dns_ip           = "${var.dns_ip}"
-    docker_version   = "${var.docker_version}"
-    k8s_version      = "${var.k8s_version}"
-    cni_version      = "${var.cni_version}"
-    tags             = "${random_id.instance-prefix.hex}"
-    instance_prefix  = "${random_id.instance-prefix.hex}"
-    pod_network_type = "${var.pod_network_type}"
-    k8s_version       = "${var.k8s_version}"
-    master_ip = "${var.master_ip}"
-  network_ip        = "${var.master_ip}"
-  target_tags       = ["${concat(list("k8s-${var.name}"), var.add_tags)}"]
-  zone              = "${var.zone}"
-  target_tags       = ["${concat(list("k8s-${var.name}"), var.add_tags)}"]
BREAKS HERE
-  count             = "${length(module.vpc.public_subnet_ids)}"
-  availability_zone = "${var.region}a"
BREAKS HERE
-  name_prefix = "govuk.content-data-api"
BREAKS HERE
-    timeout             = "300s"
-      timeout             = "300s"
BREAKS HERE
-  vpc_security_group_ids = ["${aws_security_group.allow_all_for_backup_lambda.*.id}"]
-  vpc_security_group_ids = ["${aws_security_group.allow_all_for_backup_lambda.*.id}"]
-  vpc_security_group_ids = ["${aws_security_group.allow_all_for_backup_lambda.*.id}"]
BREAKS HERE
-  depends_on = [
-    "aws_api_gateway_integration.0_n_auth_regional",
-    "aws_api_gateway_integration.0_w_auth_regional",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.0_n_auth_global",
-    "aws_api_gateway_integration.0_w_auth_global",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.1_n_auth_regional",
-    "aws_api_gateway_integration.1_w_auth_regional",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.1_n_auth_global",
-    "aws_api_gateway_integration.1_w_auth_global",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.2_n_auth_regional",
-    "aws_api_gateway_integration.2_w_auth_regional",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.2_n_auth_global",
-    "aws_api_gateway_integration.2_w_auth_global",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.3_n_auth_regional",
-    "aws_api_gateway_integration.3_w_auth_regional",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.3_n_auth_global",
-    "aws_api_gateway_integration.3_w_auth_global",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.4_n_auth_regional",
-    "aws_api_gateway_integration.4_w_auth_regional",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.4_n_auth_global",
-    "aws_api_gateway_integration.4_w_auth_global",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.5_n_auth_regional",
-    "aws_api_gateway_integration.5_w_auth_regional",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.5_n_auth_global",
-    "aws_api_gateway_integration.5_w_auth_global",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.6_n_auth_regional",
-    "aws_api_gateway_integration.6_w_auth_regional",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.6_n_auth_global",
-    "aws_api_gateway_integration.6_w_auth_global",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.7_n_auth_regional",
-    "aws_api_gateway_integration.7_w_auth_regional",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.7_n_auth_global",
-    "aws_api_gateway_integration.7_w_auth_global",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.8_n_auth_regional",
-    "aws_api_gateway_integration.8_w_auth_regional",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.8_n_auth_global",
-    "aws_api_gateway_integration.8_w_auth_global",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.9_n_auth_regional",
-    "aws_api_gateway_integration.9_w_auth_regional",
-  ]
-
-  depends_on = [
-    "aws_api_gateway_integration.9_n_auth_global",
-    "aws_api_gateway_integration.9_w_auth_global",
-  ]
-
BREAKS HERE
-  name        = "aurora-${var.name}"
-  description = "For Aurora cluster ${var.name}"
-  tags = "${merge(var.tags, map("Name", "aurora-${var.name}"))}"
BREAKS HERE
-resource "aws_security_group" "allow-external-ssh" {
-  name        = "allow-external-ssh"
-  description = "Allow incoming ssh connections from the world."
-  vpc_id      = "${var.vpc-id}"
-
-  ingress {
-    from_port   = 22
-    to_port     = 22
-    protocol    = "tcp"
-    cidr_blocks = ["0.0.0.0/0"]
-  }
-}
-
-resource "aws_security_group" "allow-all-http-outgoing" {
-  name        = "allow-all-http-outgoing"
-  description = "Allow outgoing http connections to the world."
-resource "aws_security_group" "allow-all-http-incoming" {
-  name        = "allow-all-http-incoming"
-  description = "Allow incoming http connections from the world."
-}
-
-resource "aws_security_group" "allow-mysql-egress" {
-  name        = "allow-mysql-egress"
-  description = "Allow outgoing mysql connections to the VPC."
-  vpc_id      = "${var.vpc-id}"
-    from_port   = 3306
-    to_port     = 3306
-    protocol    = "tcp"
-    cidr_blocks = ["${var.vpc-cidr-block}"]
BREAKS HERE
-    namespace = "aws:elasticbeanstalk:cloudwatch:logs:health"
-    name      = "HealthStreamingEnabled"
-    value     = "${var.health_streaming_enabled ? "true" : "false"}"
-  }
-  setting {
-    namespace = "aws:elasticbeanstalk:cloudwatch:logs:health"
-    name      = "DeleteOnTerminate"
-    value     = "${var.health_streaming_delete_on_terminate ? "true" : "false"}"
-  }
-  setting {
-    namespace = "aws:elasticbeanstalk:cloudwatch:logs:health"
-    name      = "RetentionInDays"
-    value     = "${var.health_streaming_retention_in_days}"
-  }
-  setting {
BREAKS HERE
-resource "cloudflare_record" "c2-http-rdr-a1" {
-resource "cloudflare_record" "c2-http-rdr-a2" {
BREAKS HERE
-    "method.response.header.Content-Type" = "'integration.response.header.Content-Type'"
BREAKS HERE
-    bucket        = "elb/${data.terraform_remote_state.infra_aws_logging.aws_logging_bucket_id}"
BREAKS HERE
-  tags {
-    environment = "${var.environment}"
-  }
-
-
-  tags {
-    environment = "${var.environment}"
-  }
BREAKS HERE
-    us-east-1      = "ami-8c1be5f6" # N. Virginia
-    us-east-2      = "ami-c5062ba0" # Ohio
-    us-west-1      = "ami-e689729e" # N. California
-    us-west-2      = "ami-02eada62" # Oregon
-    eu-west-1      = "ami-acd005d5" # Ireland
-    eu-west-2      = "ami-1a7f6d7e" # London
-    eu-central-1   = "ami-c7ee5ca8" # Frankfurt
-    ap-northeast-1 = "ami-0797ea64" # Tokyo
-    ap-northeast-2 = "ami-9bec36f5" # Seoel
-    ap-southeast-1 = "ami-2a69be4c" # Singapore
-    ap-southeast-2 = "ami-8536d6e7" # Sydney
-    ap-south-1     = "ami-4fc58420" # Mumbai
-    ca-central-1   = "ami-fd55ec99" # Canada
BREAKS HERE
-  label_prefix = "${terraform.workspace}"
-  linode_group      = "${var.linode_group}"
-  cluster_name = "${terraform.workspace}"
-  label_prefix = "${terraform.workspace}"
-  linode_group         = "${var.linode_group}"
BREAKS HERE
-
-            sed -e "s/\$DO_ACCESS_TOKEN/${var.do_token}/" < ${path.module}/02-do-secret.yaml > ./secrets/02-do-secret.rendered.yaml
-            until kubectl get pods 2>/dev/null; do printf '.'; sleep 5; done
BREAKS HERE
-    availability_zones      = "${join(",", coalescelist(var.availability_zones, data.aws_availability_zones.available))}"
BREAKS HERE
-  version = "~>1.21.0"
-provider "azuread" {}
BREAKS HERE
-  source      = "../../modules/ec2-nat-instance"
-  # let AWS set IPs for us
-  private_ips = []
-
-  name               = "${var.name}-web"
BREAKS HERE
-  subnets                          = ["${var.subnets_elb}"]
-    healthy_threshold   = "${var.elb_healthy_threshold}"
-    unhealthy_threshold = "${var.elb_unhealthy_threshold}"
-    interval            = "${var.elb_interval}"
-    port                = "${var.elb_healthcheck_port}"
-    healthy_threshold   = "${var.elb_healthy_threshold}"
-    unhealthy_threshold = "${var.elb_unhealthy_threshold}"
-    interval            = "${var.elb_interval}"
-    port                = "${var.elb_healthcheck_port}"
BREAKS HERE
-
-  keys = "${keys(var.env)}"
-    raw = "${format(var.template, join("\n", data.null_data_source.envs.*.outputs.raw))}"
BREAKS HERE
-    count                       = "${var.newrelic_alert_condition? 1 : 0}"
BREAKS HERE
-  tags = "${merge(var.tags, map("Name", var.instance_count > 1 ? format("%s-%d", var.name, count.index+1):var.name))}"
BREAKS HERE
-
BREAKS HERE
-  use_custom_fqdn = "${var.tectonic_azure_use_custom_fqdn}"
BREAKS HERE
-  region     = "${var.region}"
-### VPC
-resource "aws_vpc" "datastore" {
-resource "aws_subnet" "datastore_rds" {
-  cidr_block        = "${cidrsubnet(aws_vpc.datastore.cidr_block, 8, count.index)}"
-  vpc_id            = "${aws_vpc.datastore.id}"
-resource "aws_subnet" "datastore_ecs" {
-  cidr_block              = "${cidrsubnet(aws_vpc.datastore.cidr_block, 8, var.az_count + count.index)}"
-  vpc_id                  = "${aws_vpc.datastore.id}"
-resource "aws_internet_gateway" "datastore" {
-  vpc_id = "${aws_vpc.datastore.id}"
-  route_table_id         = "${aws_vpc.datastore.main_route_table_id}"
-  gateway_id             = "${aws_internet_gateway.datastore.id}"
-### ALB
-resource "aws_alb" "datastore" {
-  name            = "datastore-alb-${var.environment}"
-  subnets         = ["${aws_subnet.datastore_ecs.*.id}"]
-  security_groups = ["${aws_security_group.datastore_alb.id}"]
-}
-resource "aws_alb_target_group" "datastore_hasura" {
-  name        = "datastore-alb-${var.environment}"
-  port        = 8080
-  protocol    = "HTTP"
-  vpc_id      = "${aws_vpc.datastore.id}"
-  target_type = "ip"
-  health_check {
-    path = "/"
-    matcher = "302"
-resource "aws_alb_listener" "datastore" {
-  load_balancer_arn = "${aws_alb.datastore.id}"
-  port              = "443"
-  protocol          = "HTTPS"
-  certificate_arn   = "${data.aws_acm_certificate.datastore.arn}"
-  default_action {
-    target_group_arn = "${aws_alb_target_group.datastore_hasura.id}"
-    type             = "forward"
-resource "aws_security_group" "datastore_alb" {
-  name        = "datastore-alb-${var.environment}"
-  description = "Allow access on port 80 only to ALB"
-  vpc_id      = "${aws_vpc.datastore.id}"
-    protocol    = "tcp"
-    from_port   = 443
-    to_port     = 443
-    cidr_blocks = ["0.0.0.0/0"]
-    from_port = 0
-    to_port   = 0
-    protocol  = "-1"
-data "aws_route53_zone" "datastore" {
-  name         = "${var.domain}."
-resource "aws_route53_record" "datastore" {
-  zone_id = "${data.aws_route53_zone.datastore.zone_id}"
-  name    = "datastore.${var.environment}.${var.domain}"
-  type    = "A"
-  alias {
-    name                   = "${aws_alb.datastore.dns_name}"
-    zone_id                = "${aws_alb.datastore.zone_id}"
-    evaluate_target_health = true
-data "aws_acm_certificate" "datastore" {
-  domain   = "datastore.${var.environment}.${var.domain}"
-  types       = ["AMAZON_ISSUED"] 
-  most_recent = true
-  statuses = ["ISSUED"]
-### ECS
-resource "aws_ecs_cluster" "datastore" {
-  name = "datastore-cluster-${var.environment}"
-resource "aws_ecs_task_definition" "datastore_hasura" {
-  family                   = "hasura-${var.environment}"
-  execution_role_arn       = "${aws_iam_role.datastore_hasura_role.arn}"
-        "cpu": 250,
-        "image": "hasura/graphql-engine:v1.0.0-alpha31",
-        "memory": 512,
-            "awslogs-group": "/ecs/datastore-hasura-${var.environment}",
-            "value": "postgres://${var.rds_username}:${var.rds_password}@${aws_db_instance.datastore.endpoint}/${var.environment}"
-            "value": "{\"type\":\"HS256\", \"key\": \"${var.hasura_jwt_secret}\"}"
-resource "aws_ecs_service" "datastore_hasura" {
-  depends_on      = ["aws_ecs_task_definition.datastore_hasura", "aws_cloudwatch_log_group.datastore_hasura"]
-  name            = "datastore-service-${var.environment}"
-  cluster         = "${aws_ecs_cluster.datastore.id}"
-  task_definition = "${aws_ecs_task_definition.datastore_hasura.arn}"
-  desired_count   = "2"
-    assign_public_ip  = true
-    security_groups   = ["${aws_security_group.datastore_ecs_hasura.id}"]
-    subnets           = ["${aws_subnet.datastore_ecs.*.id}"]
-    target_group_arn = "${aws_alb_target_group.datastore_hasura.id}"
-    "aws_alb_listener.datastore",
-resource "aws_cloudwatch_log_group" "datastore_hasura" {
-  name = "/ecs/datastore-hasura-${var.environment}"
-}
-data "aws_iam_policy_document" "datastore_hasura_log_publishing" {
-  statement {
-    actions = [
-      "logs:CreateLogStream",
-      "logs:PutLogEvents",
-      "logs:PutLogEventsBatch",
-    ]
-    resources = ["arn:aws:logs:${var.region}:*:log-group:/ecs/datastore-hasura-${var.environment}:*"]
-  }
-resource "aws_iam_policy" "datastore_hasura_log_publishing" {
-  name        = "datastore-hasura-log-pub-${var.environment}"
-  path        = "/"
-  description = "Allow publishing to cloudwach"
-
-  policy = "${data.aws_iam_policy_document.datastore_hasura_log_publishing.json}"
-}
-
-data "aws_iam_policy_document" "datastore_hasura_assume_role_policy" {
-  statement {
-    actions = ["sts:AssumeRole"]
-
-    principals {
-      type        = "Service"
-      identifiers = ["ecs-tasks.amazonaws.com"]
-  }
-
-
-resource "aws_iam_role" "datastore_hasura_role" {
-  name               = "datastore-hasura-role-${var.environment}"
-  path               = "/system/"
-  assume_role_policy = "${data.aws_iam_policy_document.datastore_hasura_assume_role_policy.json}"
-resource "aws_iam_role_policy_attachment" "datastore_hasura_role_log_publishing" {
-  role = "${aws_iam_role.datastore_hasura_role.name}"
-  policy_arn = "${aws_iam_policy.datastore_hasura_log_publishing.arn}"
-resource "aws_security_group" "datastore_ecs_hasura" {
-  name        = "datastore-tasks-${var.environment}"
-  description = "allow inbound access from the ALB only"
-  vpc_id      = "${aws_vpc.datastore.id}"
-  ingress {
-    protocol        = "tcp"
-    from_port       = "8080"
-    to_port         = "8080"
-    security_groups = ["${aws_security_group.datastore_alb.id}"]
-  }
-  egress {
-    protocol    = "-1"
-    from_port   = 0
-    to_port     = 0
-    cidr_blocks = ["0.0.0.0/0"]
-# RDS
-resource "aws_db_instance" "datastore" {
-    name                        = "${var.environment}"
-    identifier                  = "datastore-${var.environment}"
-    username                    = "${var.rds_username}"
-    password                    = "${var.rds_password}"
-    port                        = "5432"
-    engine                      = "postgres"
-    engine_version              = "10.5"
-    instance_class              = "db.t2.micro"
-    allocated_storage           = "10"
-    storage_encrypted           = false
-    vpc_security_group_ids      = ["${aws_security_group.datastore_rds.id}"]
-    db_subnet_group_name        = "${aws_db_subnet_group.datastore.name}"
-    parameter_group_name        = "default.postgres10"
-    multi_az                    = true
-    storage_type                = "gp2"
-    publicly_accessible         = false
-    # snapshot_identifier         = "datastore-${var.environment}"
-    allow_major_version_upgrade = false
-    auto_minor_version_upgrade  = false
-    apply_immediately           = true
-    maintenance_window          = "sun:02:00-sun:04:00"
-    skip_final_snapshot         = false
-    # copy_tags_to_snapshot       = "${var.copy_tags_to_snapshot}"
-    backup_retention_period     = 7
-    backup_window               = "04:00-06:00"
-    # tags                        = "${module.label.tags}"
-    final_snapshot_identifier   = "datastore-${var.environment}"
-  }
-  
-resource "aws_security_group" "datastore_rds" {
-  name        = "datastore-rds-${var.environment}"
-  description = "allow inbound access from the hasura tasks only"
-  vpc_id      = "${aws_vpc.datastore.id}"
-  ingress {
-    protocol        = "tcp"
-    from_port       = "5432"
-    to_port         = "5432"
-    security_groups = ["${aws_security_group.datastore_ecs_hasura.id}"]
-  egress {
-    protocol    = "-1"
-    from_port   = 0
-    to_port     = 0
-    cidr_blocks = ["0.0.0.0/0"]
-resource "aws_db_subnet_group" "datastore" {
-  name       = "datastore-${var.environment}"
-  subnet_ids = ["${aws_subnet.datastore_rds.*.id}"]
BREAKS HERE
-variable "target_tag" {
BREAKS HERE
-  alias  = "accepter"
-  region = "${var.accepter_region}"
-data "aws_route_table" "accepter" {
-  count     = "${local.enabled ? local.accepter_subnet_ids_count : 0}"
-  provider  = "aws.accepter"
-  subnet_id = "${element(local.accepter_subnet_ids, count.index)}"
-  accepter_aws_route_table_ids           = "${distinct(sort(data.aws_route_table.accepter.*.route_table_id))}"
-  depends_on                = ["data.aws_route_table.accepter", "aws_vpc_peering_connection_accepter.accepter", "aws_vpc_peering_connection.requester"]
BREAKS HERE
-  name = "${format("%.24s", "${var.cluster["name"]}")}-api-elb"
-  name = "${format("%.17s", "${var.cluster["name"]}")}-monitoring-elb"
-  name = "${format("%.22s", "${var.cluster["name"]}")}-nodes-elb"
BREAKS HERE
-## github.com/coreos-inc/tectonic/commit/0b48144d5332201cf461a309d501b33a00a26f75
-resource "template_folder" "tectonic" {
-  input_path  = "${path.module}/resources/manifests"
-  output_path = "${path.cwd}/generated/tectonic"
-resource "localfile_file" "tectonic" {
-  destination = "${path.cwd}/generated/tectonic.sh"
-resource "localfile_file" "tectonic-rkt" {
-  destination = "${path.cwd}/generated/tectonic-rkt.sh"
BREAKS HERE
-  type = "A"
-
-  alias = {
-    name = "${aws_elb.k8s-api-elb.dns_name}"
-    zone_id = "${aws_elb.k8s-api-elb.zone_id}"
-    evaluate_target_health = false
-  }
-
-  type = "A"
-
-  alias = {
-    name = "${aws_elb.k8s-nodes-elb.dns_name}"
-    zone_id = "${aws_elb.k8s-nodes-elb.zone_id}"
-    evaluate_target_health = false
-  }
BREAKS HERE
-  public_key      = "${file("${path.root}/../../temp_key.pub")}"
BREAKS HERE
-
-  depends_on = ["module.project-factory"]
-}
-
-/******************************************
-  Gsuite Group Configuration
- *****************************************/
-resource "gsuite_group" "group" {
-  count = "${var.create_group ? 1 : 0}"
-
-  description = "${var.name} project group"
-  email       = "${module.gsuite_group.email}"
-  name        = "${local.group_name}"
BREAKS HERE
-authorized_keys: [${var.ssh_key_path == "" ? "${trimspace(file(var.base_configuration["ssh_key_path"]))}" : "${trimspace(file(var.base_configuration["ssh_key_path"]))}, ${trimspace(file(var.ssh_key_path))}"}]
BREAKS HERE
-  tags {
-    Name = "haystack-zookeeper-instance"
-    NodeType = "zookeeper"
BREAKS HERE
-    grafana_storage_class = "${var.k8s_grafana_storage_class}"
-    grafana_storage = "${var.k8s_grafana_storage}"
BREAKS HERE
-  source                = "git::https://github.com/cloudposse/terraform-aws-kops-iam-authenticator-config.git?ref=tags/0.2.1"
BREAKS HERE
-
-module "aws_federation" {
-    source = "../modules/aws_federation"
-
-    env = "${terraform.workspace}"
-    saml_x509_cert = "${var.aws_federation_saml_x509_cert}"
-    saml_idp_domain = "${var.aws_federation_saml_idp_domain}"
-    saml_login_url = "${var.aws_federation_saml_login_url}"
-    saml_logout_url = "${var.aws_federation_saml_logout_url}"
-}
BREAKS HERE
-    tectonic      = "1.7.3-tectonic.1"
BREAKS HERE
-  name               = "${var.platform_name}-public"
-  internal           = false
-  load_balancer_type = "network"
-  subnets            = ["${data.aws_subnet.public.*.id}"]
BREAKS HERE
-  default     = "1.9.4"
-  default     = "v1.6.3"
-  default     = "17.\\*"
BREAKS HERE
-  name            = "${format(local.service_name_fmt, count.index, var.network_name)}"
BREAKS HERE
-    hyperkube                 = "quay.io/coreos/hyperkube:v1.5.4_coreos.0"
-    bootkube                  = "quay.io/coreos/bootkube:v0.3.11"
-    kube_version_operator     = "quay.io/coreos/kube-version-operator:adb6183a1c5082dc42c4b8c01bb3c60daf4ccd44"
-    kubernetes = "1.5.4+tectonic.1"
-    tectonic   = "1.5.4-tectonic.1"
BREAKS HERE
-  condition = "${var.routing_condition}"
BREAKS HERE
-  description = "Additional tags for the nat gateways"
BREAKS HERE
-    from_port       = 443
-    to_port         = 443
-    protocol        = "tcp"
-    cidr_blocks     = ["0.0.0.0/0"]
BREAKS HERE
-      "salt-call --local --file-root=/root/salt/ --output=quiet state.sls_id minimal_package_update default",
BREAKS HERE
-## resource "aws_db_parameter_group" "ship_db_pg_migrate" {
-##   name  = "ship-db-pg-${var.install_version}"
-##   family = "postgres9.5"
-##
-##   parameter {
-##     name = "autovacuum"
-##     value = false
-##   }
-##
-##   # 2 GB. The value should be in KB here
-##   # pgtune generates the same value based on hardware
-##   parameter {
-##     name = "maintenance_work_mem"
-##     value = "2097152"
-##   }
-##
-##   parameter {
-##     name = "synchronous_commit"
-##     value = false
-##   }
-##
-##   parameter {
-##     name = "checkpoint_timeout"
-##     value = "30min"
-##   }
-## }
-##
-## resource "aws_db_instance" "ship_db" {
-##   name                 = "ship_db_${var.install_version}"
-##   allocated_storage    = "${var.db_storage}"
-##   storage_type         = "gp2"
-##   engine               = "postgres"
-##   engine_version       = "9.5"
-##   port                 = "5432"
-##   instance_class       = "${var.in_type_db}"
-##   username             = "${var.db_root_username}"
-##   password             = "${var.db_root_password}"
-##   vpc_security_group_ids = ["${aws_security_group.sg_private_ship_install.id}"]
-##   db_subnet_group_name = "${aws_db_subnet_group.sng_ship_db.id}"
-##   backup_retention_period = 0
-##   multi_az             = false
-##   maintenance_window   = "Sat:04:00-Sat:06:00"
-##   parameter_group_name = "ship-db-pg-${install_version}"
-##
-##   tags {
-##     Name = "ship_db_${var.install_version}"
-##   }
-}
-
-#######################################
-# Database configuration and instance settings
-#######################################
-
-    value = true
-  # 64 MB. the value should be in KB
-    value = "65536"
-    value = true
-  backup_retention_period = 3
-  multi_az             = true
BREAKS HERE
-  firehose_bucket_prefix       = "${aws_cloudwatch_log_group.elasticsearch5_application_log_group.name}"
-  firehose_bucket_prefix       = "${aws_cloudwatch_log_group.elasticsearch5_search_log_group.name}"
-  firehose_bucket_prefix       = "${aws_cloudwatch_log_group.elasticsearch5_index_log_group.name}"
BREAKS HERE
-  stream_name = "${var.kinesis-stream["name"] == "" ? "${var.cluster-name}-spans" : var.kinesis-stream["name"]}"
BREAKS HERE
-    target = "TCP:443"
BREAKS HERE
-    replicas = 2
BREAKS HERE
-  source      = "../../tf-modules/ami-ubuntu"
BREAKS HERE
-resource "null_resource" "aadsync_delay" {
-  // Wait for AAD async global replication
-  provisioner "local-exec" {
-    command = "sleep 120"
-  }
-
-  triggers = {
-    "before" = "${azurerm_azuread_service_principal_password.aks.id}"
-  }
-}
-
-  depends_on = ["null_resource.aadsync_delay"]
-
BREAKS HERE
-    ports    = ["32000-32767"]
BREAKS HERE
-  description = "Policy ARN to use for admin role"
-  description = "Permissions boundary ARN to use for admin role"
-  description = "Permissions boundary ARN to use for admin role"
BREAKS HERE
-  cookbook_team  = "${github_team.npm_lazy_team.id}"
-resource "github_team" "npm_lazy_team" {
-  team_id  = "${github_team.npm_lazy_team.id}"
BREAKS HERE
-  origin_domain_name  = "devs.mozillaindia.org"
-  origin_path         = "/homepage"
-  origin_id           = "gh-pages-dev-mozillaindia-org"
BREAKS HERE
-
-  # user-data needs to download these objects
-  depends_on = ["aws_s3_bucket_object.vault_ca_public_key", "aws_s3_bucket_object.vault_public_key", "aws_s3_bucket_object.vault_private_key"]
BREAKS HERE
-
-resource "random_string" "random_name_string" {
-  length = 4
-  special = false
-  upper = false
-  lower = true
-  number = false
-}
-
-  // size limit for elb name is 28
-  name = "${length("${var.cluster["name"]}") > 24 ? format("%s-%s", substr("${var.cluster["name"]}", 0, 19), random_string.random_name_string.result) : "${var.cluster["name"]}"}-api-elb"
-  // size limit for elb name is 28
-  name = "${length("${var.cluster["name"]}") > 17 ? format("%s-%s", substr("${var.cluster["name"]}", 0, 12), random_string.random_name_string.result) : "${var.cluster["name"]}"}-monitoring-elb"
-  // size limit for elb name is 28
-  name = "${length("${var.cluster["name"]}") > 22 ? format("%s-%s", substr("${var.cluster["name"]}", 0, 17), random_string.random_name_string.result) : "${var.cluster["name"]}"}-nodes-elb"
BREAKS HERE
-  self_link = var.public_subnetwork
-
BREAKS HERE
-variable ingress_tcp_ports {}
BREAKS HERE
-  count = "${var.state == "none" ? 0 : 1}"
BREAKS HERE
-    "google_dns_record_set.controllers",
BREAKS HERE
-variable "key_name" {
-
-  default = "ccloud-tools"
-
-}
-  key_name   = "${var.key_name}"
-    Name = "rest-proxy-${count.index}"
-    Name = "kafka-connect-${count.index}"
-    Name = "ksql-server-${count.index}"
-    Name = "control-center-${count.index}"
-    Name = "bastion-server"
-  name = "rest-proxy-target-group"  
-  name = "rest-proxy"
-    Name = "rest-proxy"
-  name = "kafka-connect-target-group"
-  name = "kafka-connect"
-    Name = "kafka-connect"
-  name = "ksql-server-target-group"  
-  name = "ksql-server"
-    Name = "ksql-server"
-  name = "control-center-target-group"  
-  name = "control-center"
-    Name = "control-center"
-}
BREAKS HERE
-  description = 'Rendered "init snippet" from the template file'
BREAKS HERE
-  name = "${var.prefix}-k8sbook-${var.chap}-sp-aks-${var.cluster_type}"
-resource "null_resource" "aadsync_delay" {
-  // Wait for AAD async global replication
-  provisioner "local-exec" {
-    command = "sleep 90"
-  }
-
-  triggers = {
-    "before" = "${azurerm_azuread_service_principal_password.aks.id}"
-  }
-}
-
-  depends_on = ["null_resource.aadsync_delay"]
-
-    client_secret = "${azurerm_azuread_service_principal_password.aks.value}"
BREAKS HERE
-variable "bucket-prefix" {}
BREAKS HERE
-  version = "1.2.0"
BREAKS HERE
-    "${aws_instance.ms_g_1.id}"
-    "${aws_instance.ms_g_1.id}"
-    "${aws_instance.ms_g_1.id}"
BREAKS HERE
-  required_version = "0.9.8"
-  access_key = "${var.aws_access_key_id}"
-  secret_key = "${var.aws_secret_access_key}"
-  region     = "${var.aws_default_region}"
-resource "aws_security_group" "chef_server" {
-  name        = "chef_server_${var.automate_instance_id}"
-  description = "Terraform Automate Chef Server"
-  vpc_id      = "${var.automate_vpc}"
-
-  tags {
-    Name = "${var.automate_tag}_chef_server_security_group"
-    X-Dept    = "${var.tag_dept}"
-    X-Contact = "${var.tag_contact}"
-  }
-}
-
-# SSH - all
-resource "aws_security_group_rule" "ingress_chef_server_allow_22_tcp_all" {
-  type = "ingress"
-  from_port = 22
-  to_port = 22
-  protocol = "tcp"
-  cidr_blocks = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.chef_server.id}"
-}
-
-# HTTP (nginx)
-resource "aws_security_group_rule" "ingress_chef_server_allow_80_tcp" {
-  type = "ingress"
-  from_port = 80
-  to_port = 80
-  protocol = "tcp"
-  cidr_blocks = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.chef_server.id}"
-# HTTPS (nginx)
-resource "aws_security_group_rule" "ingress_chef_server_allow_443_tcp" {
-  type = "ingress"
-  from_port = 443
-  to_port = 443
-  protocol = "tcp"
-  cidr_blocks = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.chef_server.id}"
-}
-
-# opscode push-jobs
-resource "aws_security_group_rule" "ingress_chef_server_allow_10000-10003_tcp" {
-  type = "ingress"
-  from_port = 10000
-  to_port = 10003
-  protocol = "tcp"
-  cidr_blocks = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.chef_server.id}"
-}
-
-# Allow all Chef
-resource "aws_security_group_rule" "ingress_chef_server_allow_all_chef_automate" {
-  type = "ingress"
-  from_port = 0
-  to_port = 0
-  protocol = "-1"
-  source_security_group_id = "${aws_security_group.chef_automate.id}"
-  security_group_id = "${aws_security_group.chef_server.id}"
-}
-
-# Egress: ALL
-resource "aws_security_group_rule" "egress_chef_server_allow_0-65535_all" {
-  type = "egress"
-  from_port = 0
-  to_port = 0
-  protocol = "-1"
-  cidr_blocks = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.chef_server.id}"
-}
-
-resource "aws_security_group" "chef_automate" {
-  name        = "chef_automate_${var.automate_instance_id}"
-  description = "Terraform Chef Automate Server"
-  vpc_id      = "${var.automate_vpc}"
-
-  tags {
-    Name = "${var.automate_tag}_chef_automate_security_group"
-    X-Dept    = "${var.tag_dept}"
-    X-Contact = "${var.tag_contact}"
-  }
-}
-
-# SSH - all
-resource "aws_security_group_rule" "ingress_chef_automate_allow_22_tcp_all" {
-  type = "ingress"
-  from_port = 22
-  to_port = 22
-  protocol = "tcp"
-  cidr_blocks = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.chef_automate.id}"
-}
-
-# HTTP
-resource "aws_security_group_rule" "ingress_chef_automate_allow_80_tcp" {
-  type = "ingress"
-  from_port = 80
-  to_port = 80
-  protocol = "tcp"
-  cidr_blocks = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.chef_automate.id}"
-}
-
-# HTTPS
-resource "aws_security_group_rule" "ingress_chef_automate_allow_443_tcp" {
-  type = "ingress"
-  from_port = 443
-  to_port = 443
-  protocol = "tcp"
-  cidr_blocks = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.chef_automate.id}"
-}
-
-# Automate GIT
-resource "aws_security_group_rule" "ingress_chef_automate_allow_8989_tcp" {
-  type = "ingress"
-  from_port = 8989
-  to_port = 8989
-  protocol = "tcp"
-  cidr_blocks = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.chef_automate.id}"
-}
-
-# Allow all Chef Server
-resource "aws_security_group_rule" "ingress_chef_automate_allow_all_chef_server" {
-  type = "ingress"
-  from_port = 0
-  to_port = 0
-  protocol = "-1"
-  source_security_group_id = "${aws_security_group.chef_server.id}"
-  security_group_id = "${aws_security_group.chef_automate.id}"
-}
-
-# Egress: ALL
-resource "aws_security_group_rule" "egress_chef_automate_allow_0-65535_all" {
-  type = "egress"
-  from_port = 0
-  to_port = 0
-  protocol = "-1"
-  cidr_blocks = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.chef_automate.id}"
-}
-
-resource "aws_security_group" "build_nodes" {
-  name        = "build_nodes_${var.automate_instance_id}"
-  description = "Terraform Build Nodes"
-  vpc_id      = "${var.automate_vpc}"
-
-  tags {
-    Name = "${var.automate_tag}_build_nodes_security_group"
-    X-Dept    = "${var.tag_dept}"
-    X-Contact = "${var.tag_contact}"
-  }
-}
-
-# Egress: ALL
-resource "aws_security_group_rule" "egress_build_nodes_allow_0-65535_all" {
-  type = "egress"
-  from_port = 0
-  to_port = 0
-  protocol = "-1"
-  cidr_blocks = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.build_nodes.id}"
-}
-
-# SSH - all
-resource "aws_security_group_rule" "ingress_build_nodes_allow_22_tcp_all" {
-  type = "ingress"
-  from_port = 22
-  to_port = 22
-  protocol = "tcp"
-  cidr_blocks = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.build_nodes.id}"
-}
-
-# Allow all Chef Automate
-resource "aws_security_group_rule" "ingress_build_nodes_allow_all_chef_server" {
-  type = "ingress"
-  from_port = 0
-  to_port = 0
-  protocol = "-1"
-  source_security_group_id = "${aws_security_group.chef_automate.id}"
-  security_group_id = "${aws_security_group.build_nodes.id}"
-    user     = "${var.aws_ami_user}"
-    private_key = "${file(".keys/${var.aws_key_pair_name}.pem")}"
-  ami             = "${var.aws_ami_rhel}"
-  instance_type   = "${var.aws_instance_type}"
-  key_name        = "${var.aws_key_pair_name}"
-  subnet_id       = "${var.automate_subnet}"
-  ebs_optimized   = true
-    volume_size = 20
-    volume_type = "gp2"
-    device_name = "/dev/sdb"
-    volume_type = "io1"
-    iops = 5000 # iops = volume_size * 50
-    volume_size = 100
-    Name      = "${format("${var.automate_tag}_chef_server_%02d_${var.automate_instance_id}", count.index + 1)}"
-    source = "mount_data_volume"
-      "sudo bash -ex /tmp/mount_data_volume"
-    source = ".chef/delivery-validator.pub"
-      "curl -L http://chef-installer.chameleon-development.ca -o installer.sh && sudo SVWAIT=30 bash ./installer.sh -c ${aws_instance.chef_server.public_dns}",
-      "sudo chef-server-ctl add-client-key delivery delivery-validator --public-key-path /tmp/pre-delivery-validator.pub"
-  template = "$${delivery_validator}"
-    user     = "${var.aws_ami_user}"
-    private_key = "${file(".keys/${var.aws_key_pair_name}.pem")}"
-  ami             = "${var.aws_ami_rhel}"
-  instance_type   = "${var.aws_instance_type}"
-  key_name        = "${var.aws_key_pair_name}"
-  subnet_id       = "${var.automate_subnet}"
-  ebs_optimized   = true
-    volume_size = 20
-    volume_type = "gp2"
-    device_name = "/dev/sdb"
-    volume_type = "io1"
-    iops = 5000 # iops = volume_size * 50
-    volume_size = 100
-    Name      = "${format("${var.automate_tag}_chef_automate_%02d_${var.automate_instance_id}", count.index + 1)}"
-      "sudo mkdir /etc/chef/"
-    source = "mount_data_volume"
-      "sudo bash -ex /tmp/mount_data_volume"
-  provisioner "chef"  {
-    environment = "_default"
-    run_list = ["chef-services::delivery"]
-    node_name = "${aws_instance.chef_automate.public_dns}"
-    server_url = "https://${aws_instance.chef_server.public_dns}/organizations/delivery"
-    user_name = "delivery-validator"
-    user_key = "${data.template_file.delivery_validator.rendered}"
-    client_options = ["trusted_certs_dir = '/etc/chef/trusted_certs'"]
-    command = "scp -oStrictHostKeyChecking=no -i .keys/${var.aws_key_pair_name}.pem ${var.aws_ami_user}@${aws_instance.chef_automate.public_dns}:/tmp/test.creds ./"
-
-    user     = "${var.aws_ami_user}"
-    private_key = "${file(".keys/${var.aws_key_pair_name}.pem")}"
-  ami             = "${var.aws_ami_rhel}"
-  instance_type   = "${var.aws_build_node_instance_type}"
-  key_name        = "${var.aws_key_pair_name}"
-  subnet_id       = "${var.automate_subnet}"
-  ebs_optimized   = false
-  count = 3
-    volume_size = 100
-    volume_type = "gp2"
-    Name      = "${format("${var.automate_tag}_build_node_%02d_${var.automate_instance_id}", count.index + 1)}"
-    provisioner "chef"  {
-      attributes_json = <<-EOF
-      environment = "_default"
-      node_name = "build-node-${count.index + 1}"
-      fetch_chef_certificates = true
-      run_list = ["chef-services::install_build_nodes"]
-      server_url = "https://${aws_instance.chef_server.public_dns}/organizations/delivery"
-      user_name = "delivery-validator"
-      user_key = "${data.template_file.delivery_validator.rendered}"
-      client_options = ["trusted_certs_dir '/etc/chef/trusted_certs'"]
-    }
BREAKS HERE
-  haystack_index_store_master_count = "${var.haystack_index_store_master_count}"
-  haystack_index_store_instance_count = "${var.haystack_index_store_instance_count}"
-  haystack_index_store_worker_instance_type = "${var.haystack_index_store_worker_instance_type}"
-  haystack_index_store_es_master_instance_type = "${var.haystack_index_store_es_master_instance_type}"
BREAKS HERE
-  value       = ["${null_resource.global_secondary_indexe_names.*.triggers.name}"]
BREAKS HERE
-    expose    = "false"
-    basicauth = "false"
-  default = {
-    memory = ""
-  }
BREAKS HERE
-  tags = "${map("Name", "${var.stackname}-publishing-api-db-admin", "Project", var.stackname, "aws_environment", var.aws_environment, "aws_migration", "publishing-api_db_admin")}"
-  default_tags                  = "${map("Project", var.stackname, "aws_stackname", var.stackname, "aws_environment", var.aws_environment, "aws_migration", "publishing-api_db_admin", "aws_hostname", "publishing-api-db-admin-1")}"
BREAKS HERE
-  count    = "${var.configure_kubectl_session ? 1 : 0}"
-resource "null_resource" "configure_kubectl" {
-    kubeconfig_rendered = "${data.template_file.kubeconfig.rendered}"
BREAKS HERE
-provider "aws" {
-  version = "~> 1.5"
-
-  region  = "us-east-1"
-}
-
-# KEY PAIR FOR ALL INSTANCES
-# ---------------------------------------------------------------------------------------------------------------------
-resource "aws_key_pair" "auth" {
-  key_name   = "quorum-cluster-${var.network_id}"
-  public_key = "${file(var.public_key_path)}"
-}
-
-# ---------------------------------------------------------------------------------------------------------------------
-  aws_key_pair_id = "${aws_key_pair.auth.id}"
-  aws_key_pair_id = "${aws_key_pair.auth.id}"
BREAKS HERE
-  traffic_manager_endpoint_priority   = 300
BREAKS HERE
-    digit = "/\"([[:digit:]]+)\"/"
BREAKS HERE
-  subnet_id     = "${aws_subnet.system.0.id}"
BREAKS HERE
-resource "scaleway_security_group_rule" "ssh_accept_worker" {
BREAKS HERE
-
BREAKS HERE
-  launch_configuration = "${var.create_lc ? element(concat(aws_launch_configuration.this.*.id, list(var.launch_configuration)), 0) : var.launch_configuration}"
-      var.tags,
-      list(map("key", "Name", "value", var.name, "propagate_at_launch", true))
BREAKS HERE
-
-  lifecycle {
-    create_before_destroy = true
-  }
-  name = "20-eth1.network"
-Address=${var.cidr}
-  ssh_authorized_keys = "${var.nat_ssh_public_keys}"
-  networkd = ["${data.ignition_networkd_unit.nat_eth0.id}", "${data.ignition_networkd_unit.nat_eth1.id}"]
-  name            = "${var.name}_nat_gw_${count.index}"
-  image_name      = "CoreOS Stable"
-  flavor_name     = "${lookup(var.nat_instance_flavor_names, var.region)}"
-  security_groups = ["${openstack_networking_secgroup_v2.nat_sg.name}"]
-
-  user_data = "${data.ignition_config.nat.rendered}"
BREAKS HERE
-  name     = "${var.name}"
-  username = "${var.username}"
-  password = "${var.password}"
-  port     = "${var.port}"
BREAKS HERE
-data "aws_caller_identity" "default" {}
-data "aws_region" "default" {}
BREAKS HERE
-
BREAKS HERE
-
-//This is prevent the cyclic dependency
-resource "aws_security_group" "nodes" {
-  name = "nodes.${var.haystack_cluster_name}"
-  vpc_id = "${var.aws_vpc_id}"
-  description = "Security group for nodes"
-
-  ingress {
-    from_port = "${var.reverse_proxy_port}"
-    to_port = "${var.reverse_proxy_port}"
-    protocol = "tcp"
-    security_groups = [
-      "${aws_security_group.nodes-elb.id}"]
-  }
-  ingress {
-    from_port = "${var.graphite_node_port}"
-    to_port = "${var.graphite_node_port}"
-    protocol = "tcp"
-    security_groups = [
-      "${aws_security_group.monitoring-elb.id}"]
-  }
-  ingress {
-    from_port = 22
-    to_port = 22
-    protocol = "tcp"
-    cidr_blocks = [
-      "0.0.0.0/0"]
-  }
-  egress {
-    from_port = 0
-    to_port = 0
-    protocol = "-1"
-    cidr_blocks = [
-      "0.0.0.0/0"]
-  }
-  tags = {
-    Product = "Haystack"
-    Component = "K8s"
-    ClusterName = "${var.haystack_cluster_name}"
-    Role = "${var.haystack_cluster_name}-k8s-nodes"
-    Name = "${var.haystack_cluster_name}-k8s-nodes"
-  }
-
-//master instance security group
-resource "aws_security_group_rule" "all-master-to-master" {
-  security_group_id = "${aws_security_group.masters.id}"
-  source_security_group_id = "${aws_security_group.masters.id}"
-  ingress {
-    from_port = "443"
-    to_port = "443"
-    protocol = "tcp"
-    security_groups = [
-      "${aws_security_group.api-elb.id}"]
-  }
-  ingress {
-    from_port = 0
-    to_port = 65535
-    protocol = "4"
-    security_groups = [
-      "${aws_security_group.nodes.id}"]
-  }
-
-  ingress {
-    from_port = 1
-    to_port = 2379
-    protocol = "tcp"
-    security_groups = [
-      "${aws_security_group.nodes.id}"]
-  }
-  ingress {
-    from_port = 2382
-    to_port = 4001
-    protocol = "tcp"
-    security_groups = [
-      "${aws_security_group.nodes.id}"]
-  }
-
-  ingress {
-    from_port = 4003
-    to_port = 65535
-    protocol = "tcp"
-    security_groups = [
-      "${aws_security_group.nodes.id}"]
-  }
-  ingress {
-    from_port = 1
-    to_port = 65535
-    protocol = "udp"
-    security_groups = [
-      "${aws_security_group.nodes.id}"]
-  }
-  ingress {
-    from_port = 22
-    to_port = 22
-    protocol = "tcp"
-    cidr_blocks = [
-      "0.0.0.0/0"]
-  }
-
-  egress {
-    from_port = 0
-    to_port = 0
-    protocol = "-1"
-    cidr_blocks = [
-      "0.0.0.0/0"]
-  }
BREAKS HERE
-  count = "${length(var.container_properties)}"
BREAKS HERE
-variable kubeadm_token {}
-  bootstrap_file = "bootstrap/master.sh"
-  node_labels    = "${split(",", var.master_as_edge == "true" ? "role=edge" : "")}"
-  assign_floating_ip = "false"
-  bootstrap_file = "bootstrap/node.sh"
-  bootstrap_file = "bootstrap/node.sh"
-  assign_floating_ip = "false"
-  bootstrap_file = "bootstrap/node.sh"
-  source             = "../common/inventory"
-  cluster_prefix     = "${var.cluster_prefix}"
-  domain             = "${ var.use_cloudflare == true ? module.cloudflare.domain_and_subdomain : format("%s.nip.io", element(concat(module.edge.public_ip, module.master.public_ip), 0))}"
-  ssh_user           = "${var.ssh_user}"
-  master_hostnames   = "${module.master.hostnames}"
-  master_public_ip   = "${module.master.public_ip}"
-  master_private_ip  = "${module.master.local_ip_v4}"
-  master_as_edge     = "${var.master_as_edge}"
-  edge_count         = "${var.edge_count}"
-  edge_hostnames     = "${module.edge.hostnames}"
-  edge_public_ip     = "${module.edge.public_ip}"
-  edge_private_ip    = "${module.edge.local_ip_v4}"
-  node_count         = "${var.node_count}"
-  node_hostnames     = "${module.node.hostnames}"
-  node_public_ip     = "${module.node.public_ip}"
-  node_private_ip    = "${module.node.local_ip_v4}"
-  glusternode_count  = "${var.glusternode_count}"
-  gluster_volumetype = "${var.gluster_volumetype}"
-  extra_disk_device  = "${element(concat(module.glusternode.extra_disk_device, list("")),0)}"
BREAKS HERE
-  source     = "git::https://github.com/cloudposse/terraform-null-label.git?ref=tags/0.3.1"
-    environment_variable {
-    }
-
-    environment_variable {
-      "name"  = "AWS_ACCOUNT_ID"
-      "value" = "${signum(length(var.aws_account_id)) == 1 ? var.aws_account_id : data.aws_caller_identity.default.account_id}"
-    }
-
-    environment_variable {
-      "name"  = "IMAGE_REPO_NAME"
-      "value" = "${signum(length(var.image_repo_name)) == 1 ? var.image_repo_name : "UNSET"}"
-    }
-
-    environment_variable {
-      "name"  = "IMAGE_TAG"
-      "value" = "${signum(length(var.image_tag)) == 1 ? var.image_tag : "latest"}"
-    }
-
-    environment_variable {
-      "name"  = "STAGE"
-      "value" = "${var.stage}"
-    }
-
-    environment_variable {
-      "name"  = "GITHUB_TOKEN"
-      "value" = "${var.github_token}"
-    }
BREAKS HERE
-    expose = true
-
-data "docker_network" "bridge" {
-  name = "bridge"
-}
BREAKS HERE
-  name_prefix = "${var.name_prefix}-lb"
-    Name = "${var.name_prefix}-lb"
BREAKS HERE
-  count = "${max(length(var.private_subnets), length(var.elasticache_subnets), length(var.database_subnets))}"
BREAKS HERE
-  load_balancer_name            = "test-alb"
BREAKS HERE
-  ecs_cluster_name = "${element(split("/",var.ecs_cluster_id),3)}"
-  container_image = "${var.container_image == "" ? module.live_task_lookup.image: var.container_image}"
-  # deployment_minimum_healthy_percent sets the minimum % in capacity at depployment
BREAKS HERE
-resource "aws_lambda_function" "es_cleanup_vpc" {
-  count            = "${length(var.subnet_ids) > 0 ? 1 : 0}"
-    security_group_ids = ["${aws_security_group.lambda.*.id}"]
-
-resource "aws_lambda_function" "es_cleanup" {
-  count            = "${length(var.subnet_ids) == 0 ? 1 : 0}"
-  filename         = "${path.module}/es-cleanup.zip"
-  function_name    = "${var.prefix}es-cleanup"
-  description      = "${var.prefix}es-cleanup"
-  timeout          = 300
-  runtime          = "python${var.python_version}"
-  role             = "${aws_iam_role.role.arn}"
-  handler          = "es-cleanup.lambda_handler"
-  source_code_hash = "${data.archive_file.es_cleanup_lambda.output_base64sha256}"
-
-  environment {
-    variables = {
-      es_endpoint  = "${var.es_endpoint}"
-      index        = "${var.index}"
-      delete_after = "${var.delete_after}"
-      index_format = "${var.index_format}"
-      sns_alert    = "${var.sns_alert}"
-    }
-  }
-
-  tags = "${merge(
-            var.tags,
-            map("Scope", "${var.prefix}lambda_function_to_elasticsearch"),
-            )}"
-}
BREAKS HERE
-  name          = "${var.cluster_name}"
BREAKS HERE
-  tags = "${map("Name", "${var.stackname}-calculators-frontend", "Project", var.stackname, "aws_environment", var.aws_environment, "aws_migration", "calculators-frontend")}"
-  default_tags                  = "${map("Project", var.stackname, "aws_stackname", var.stackname, "aws_environment", var.aws_environment, "aws_migration", "calculators-frontend", "aws_hostname", "calculators-frontend-1")}"
BREAKS HERE
-  name                      = "${var.stack_item_label}"
-    value               = "${var.stack_item_label}"
-  name                      = "${var.stack_item_label}"
-    value               = "${var.stack_item_label}"
BREAKS HERE
-    datastore = "${var.vm_disk_datastore}"
BREAKS HERE
-  name        = "tf-sg-ec-${var.name}-${data.aws_vpc.vpc.tags["Name"]}"
-  description = "Terraform-managed ElastiCache security group for ${var.name}-${data.aws_vpc.vpc.tags["Name"]}"
-    Name = "tf-sg-ec-${var.name}-${data.aws_vpc.vpc.tags["Name"]}"
BREAKS HERE
-resource "openstack_compute_keypair_v2" "snapshot-demo-keypair" {
-  name       = "snapshot-demo-keypair"
-  public_key = "${var.SSH_KEY}"
-    key_pair = "snapshot-demo-keypair"
-    depends_on = ["openstack_networking_subnet_v2.subnet_snap"]
-    image_id = "${data.openstack_images_image_v2.centos.id}"
-      private_key = "${file("~/.ssh/id_rsa")}"
-/*resource "openstack_blockstorage_volume_v2" "scratch_volume"*/
-
-/*resource "openstack_compute_instance_v2" "boot-from-volume" {
-  name            = "boot-from-volume"
-  flavor_id       = "3"
-  key_pair        = "my_key_pair_name"
-  security_groups = ["default"]
-
-  block_device {
-    uuid                  = "<image-id>"
-    source_type           = "image"
-    volume_size           = 5
-    boot_index            = 0
-    destination_type      = "volume"
-    delete_on_termination = true
-  }
-
-  network {
-    name = "my_network"
-  }
-}*/
BREAKS HERE
-  function_name    = "${var.prefix}es-cleanup"
-  description      = "${var.prefix}es-cleanup"
-            map("Scope", "${var.prefix}lambda_function_to_elasticsearch"),
BREAKS HERE
-resource "aws_route53_record" "digitalgov_gov_openopps_digitalgov_gov_ns" {
-resource "aws_route53_record" "digitalgov_gov_openopps_digitalgov_gov_ns" {
-resource "aws_route53_record" "digitalgov_gov_openopps_digitalgov_gov_ns" {
-resource "aws_route53_record" "digitalgov_gov_openopps_digitalgov_gov_ns" {
-
-
-
-
BREAKS HERE
-  name              = "nat-gateway-${var.region}"
-  name        = "nat-${var.region}"
-  name    = "nat-${var.region}"
-  name = "nat-${var.region}"
BREAKS HERE
-  target_tags       = ["${compact(concat(list("nat-${var.zone}"), var.tags))}"]
-  source_tags = ["${compact(concat(list("nat-${var.region}"), var.tags))}"]
-  target_tags = ["${compact(concat(list("nat-${var.region}"), var.tags))}"]
BREAKS HERE
-  group_name          = ""
-  create_group        = false
BREAKS HERE
-  identifier             = "${var.prefix}-${var.db_id}"
-  engine_version         = "${var.db_version}"
-  instance_class         = "${var.db_instance_class}"
-  storage_type           = "${var.db_storage_type}"
-  allocated_storage      = "${var.db_storage}"
-  username               = "${var.db_username}"
-  password               = "${var.db_password}"
BREAKS HERE
-// ${var.base_configuration["name_prefix"]}${var.name}${var.count > 1 ? "-${count.index  + 1}" : ""}
-//   name_prefix + name (if count = 1)
-//   name_prefix + name + "-" + index (if count > 1)
-  name             = "${var.base_configuration["name_prefix"]}${var.name}${var.count > 1 ? "-${count.index  + 1}" : ""}-main-disk"
-  base_volume_name = "${var.base_configuration["use_shared_resources"] ? "" : var.base_configuration["name_prefix"]}baseimage"
-  name  = "${var.base_configuration["name_prefix"]}${var.name}${var.count > 1 ? "-${count.index  + 1}" : ""}-hana-disk"
-  name       = "${var.base_configuration["name_prefix"]}${var.name}${var.count > 1 ? "-${count.index  + 1}" : ""}"
BREAKS HERE
-
-  master_auth {
-    username = "${var.gke_username}"
-    password = "${var.gke_password}"
-  }
BREAKS HERE
-  instances = [ "${ aws_instance.etcd.*.id }" ]
BREAKS HERE
-instance MS-B-1
BREAKS HERE
-  cluster_pod_security_policy_enabled        = "${local.cluster_type_output_pod_security_policy_enabled[local.cluster_type] ? false : true}"
BREAKS HERE
-  cluster_type_output_zonal_zones    = "${concat(google_container_cluster.zonal_primary.*.additional_zones, list(list()))}"
BREAKS HERE
-  name = "iscsi-data"
BREAKS HERE
-  count = 2
BREAKS HERE
-  ami                    = "${module.ami.ami_id}"
-  instance_type          = "${var.instance_type}"
-  subnet_id              = "${element(split(",", var.public_subnet_ids), count.index)}"
-  key_name               = "${var.key_name}"
-  vpc_security_group_ids = ["${aws_security_group.bastion.id}"]
BREAKS HERE
-  k8s_nodes_iam-instance-profile_arn = "${module.haystack-k8s.nodes_iam-instance-profile_arn}"
BREAKS HERE
-  instance_class = "db.t1.micro"
BREAKS HERE
-  credentials = "${file(local.credentials_path)}"
-  region      = "${local.region}"
-  project_id        = "${local.project_id}"
-  region            = "${local.region}"
-  network           = "${local.network}"
-  subnetwork        = "${local.subnetwork}"
-  ip_range_pods     = "${local.ip_range_pods}"
-  ip_range_services = "${local.ip_range_services}"
-  kubernetes_version = "1.9.7-gke.11"
BREAKS HERE
-    kube_version_operator        = "quay.io/coreos/kube-version-operator:v1.7.3-kvo.1"
-    tectonic_channel_operator    = "quay.io/coreos/tectonic-channel-operator:0.5.0"
BREAKS HERE
-  zone_id = "${aws_route53_zone.18f_gov_zone.zone_id}"
-  
-
BREAKS HERE
-    cidr_blocks = ["10.0.0.0/8"]
-  # HTTP access
-    cidr_blocks = ["10.0.0.0/8"]
-    cidr_blocks = ["10.0.0.0/8"]
-  # HTTP access
-    to_port     = -1
BREAKS HERE
-variable "service-ip-range" {}
-variable "vpc-cidr" {}
BREAKS HERE
-  source = "../../modules/k8s-addons"
BREAKS HERE
-  shared_vpc_subnets = ["projects/${var.shared_vpc}/regions/${google_compute_subnetwork.subnet-01.region}/subnetworks/${google_compute_subnetwork.subnet-01.name}"]
-resource "google_compute_network" "network" {
-  name                    = "pf-test-full-${random_string.suffix.result}"
-  routing_mode            = "GLOBAL"
-  auto_create_subnetworks = "false"
-  project                 = "${var.shared_vpc}"
-}
-resource "google_compute_subnetwork" "subnet-01" {
-  name          = "subnet-01"
-  ip_cidr_range = "10.10.10.0/24"
-  region        = "us-east4"
-  network       = "${google_compute_network.network.name}"
-  project       = "${var.shared_vpc}"
BREAKS HERE
-  subnets = ["${data.terraform_remote_state.vpc.private_subnet_ids}"]
BREAKS HERE
-    prometheus_operator          = "quay.io/coreos/prometheus-operator:v0.6.0"
-    tectonic_prometheus_operator = "quay.io/coreos/tectonic-prometheus-operator:v1.0.0"
-    prometheus = "v1.5.2"
-    monitoring = "1.0.0"
BREAKS HERE
-  eviction_policy     = "Delete"
BREAKS HERE
-  name                   = "${var.name}-nat-${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"
BREAKS HERE
-
-  name        = "api-elb.haystack-k8s"
-  vpc_id      = "${var.k8s_vpc_id}"
-  tags = {
-    KubernetesCluster = "haystack-k8s"
-    Name              = "api-elb.haystack-k8s"
-}
-
-
-resource "aws_security_group" "nodes-api-elb-haystack-k8s" {
-  name        = "nodes-api-elb.haystack-k8s"
-  vpc_id      = "${var.k8s_vpc_id}"
-  description = "Security group for api ELB"
-    Name              = "nodes-api-elb.haystack-k8s"
-resource "aws_security_group" "masters-haystack-k8s" {
-  name        = "masters.haystack-k8s"
-  vpc_id      = "${var.k8s_vpc_id}"
-  description = "Security group for masters"
-  tags = {
-    KubernetesCluster = "haystack-k8s"
-    Name              = "masters.haystack-k8s"
-}
-
-resource "aws_security_group" "nodes-haystack-k8s" {
-  name        = "nodes.haystack-k8s"
-  vpc_id      = "${var.k8s_vpc_id}"
-  description = "Security group for nodes"
-    Name              = "nodes.haystack-k8s"
-resource "aws_security_group_rule" "all-master-to-master" {
-  type                     = "ingress"
-  security_group_id        = "${aws_security_group.masters-haystack-k8s.id}"
-  source_security_group_id = "${aws_security_group.masters-haystack-k8s.id}"
-  from_port                = 0
-  to_port                  = 0
-  protocol                 = "-1"
-}
-resource "aws_security_group_rule" "all-node-to-node" {
-  type                     = "ingress"
-  security_group_id        = "${aws_security_group.nodes-haystack-k8s.id}"
-  source_security_group_id = "${aws_security_group.nodes-haystack-k8s.id}"
-  from_port                = 0
-  to_port                  = 0
-  protocol                 = "-1"
-}
-
-resource "aws_security_group_rule" "api-elb-egress" {
-  type              = "egress"
-  security_group_id = "${aws_security_group.api-elb-haystack-k8s.id}"
-  from_port         = 0
-  to_port           = 0
-  protocol          = "-1"
-  cidr_blocks       = ["0.0.0.0/0"]
-}
-
-
-resource "aws_security_group_rule" "nodes-api-elb-egress" {
-  type              = "egress"
-  security_group_id = "${aws_security_group.nodes-api-elb-haystack-k8s.id}"
-  from_port         = 0
-  to_port           = 0
-  protocol          = "-1"
-  cidr_blocks       = ["0.0.0.0/0"]
-}
-
-
-
-resource "aws_security_group_rule" "http-nodes-elb-0-0-0-0--0" {
-  type              = "ingress"
-  security_group_id = "${aws_security_group.nodes-api-elb-haystack-k8s.id}"
-  from_port         = 80
-  to_port           = 80
-  protocol          = "tcp"
-  cidr_blocks       = ["0.0.0.0/0"]
-}
-
-resource "aws_security_group_rule" "https-api-elb-0-0-0-0--0" {
-  type              = "ingress"
-  security_group_id = "${aws_security_group.api-elb-haystack-k8s.id}"
-  from_port         = 443
-  to_port           = 443
-  protocol          = "tcp"
-  cidr_blocks       = ["0.0.0.0/0"]
-}
-resource "aws_security_group_rule" "https-elb-to-master" {
-  type                     = "ingress"
-  security_group_id        = "${aws_security_group.masters-haystack-k8s.id}"
-  source_security_group_id = "${aws_security_group.api-elb-haystack-k8s.id}"
-  from_port                = 443
-  to_port                  = 443
-  protocol                 = "tcp"
-}
-resource "aws_security_group_rule" "master-egress" {
-  type              = "egress"
-  security_group_id = "${aws_security_group.masters-haystack-k8s.id}"
-  from_port         = 0
-  to_port           = 0
-  protocol          = "-1"
-  cidr_blocks       = ["0.0.0.0/0"]
-resource "aws_security_group_rule" "node-egress" {
-  type              = "egress"
-  security_group_id = "${aws_security_group.nodes-haystack-k8s.id}"
-  from_port         = 0
-  to_port           = 0
-  protocol          = "-1"
-  cidr_blocks       = ["0.0.0.0/0"]
-}
-resource "aws_security_group_rule" "node-to-master-protocol-ipip" {
-  source_security_group_id = "${aws_security_group.nodes-haystack-k8s.id}"
-  to_port                  = 65535
-  protocol                 = "4"
-}
-
-resource "aws_security_group_rule" "node-to-master-tcp-1-2379" {
-  type                     = "ingress"
-  security_group_id        = "${aws_security_group.masters-haystack-k8s.id}"
-  source_security_group_id = "${aws_security_group.nodes-haystack-k8s.id}"
-  from_port                = 1
-  to_port                  = 2379
-  protocol                 = "tcp"
-}
-
-resource "aws_security_group_rule" "node-to-master-tcp-2382-4001" {
-  type                     = "ingress"
-  security_group_id        = "${aws_security_group.masters-haystack-k8s.id}"
-  source_security_group_id = "${aws_security_group.nodes-haystack-k8s.id}"
-  from_port                = 2382
-  to_port                  = 4001
-  protocol                 = "tcp"
-resource "aws_security_group_rule" "node-to-master-tcp-4003-65535" {
-  type                     = "ingress"
-  security_group_id        = "${aws_security_group.masters-haystack-k8s.id}"
-  source_security_group_id = "${aws_security_group.nodes-haystack-k8s.id}"
-  from_port                = 4003
-  to_port                  = 65535
-  protocol                 = "tcp"
-}
-resource "aws_security_group_rule" "node-to-master-udp-1-65535" {
-  type                     = "ingress"
-  security_group_id        = "${aws_security_group.masters-haystack-k8s.id}"
-  source_security_group_id = "${aws_security_group.nodes-haystack-k8s.id}"
-  from_port                = 1
-  to_port                  = 65535
-  protocol                 = "udp"
-}
-resource "aws_security_group_rule" "ssh-external-to-master-0-0-0-0--0" {
-  type              = "ingress"
-  security_group_id = "${aws_security_group.masters-haystack-k8s.id}"
-  from_port         = 22
-  to_port           = 22
-  protocol          = "tcp"
-  cidr_blocks       = ["0.0.0.0/0"]
-}
-resource "aws_security_group_rule" "ssh-external-to-node-0-0-0-0--0" {
-  type              = "ingress"
-  security_group_id = "${aws_security_group.nodes-haystack-k8s.id}"
-  from_port         = 22
-  to_port           = 22
-  protocol          = "tcp"
-  cidr_blocks       = ["0.0.0.0/0"]
-resource "aws_security_group_rule" "reverse-proxy-app-in-node-0-0-0-0--0" {
-  type              = "ingress"
-  security_group_id = "${aws_security_group.nodes-haystack-k8s.id}"
-  from_port         = "${var.reverse_proxy_port}"
-  to_port           = "${var.reverse_proxy_port}"
-  protocol          = "tcp"
-  cidr_blocks       = ["0.0.0.0/0"]
-}
BREAKS HERE
-      element(local.tag_keys, 0) != "" ? element(local.tag_keys, 0) : "",
-      element(local.tag_values, 0) != "" ? element(local.tag_values, 0) : "",
-      element(local.tag_keys, 1) != "" ? element(local.tag_keys, 1) : "",
-      element(local.tag_values, 1) != "" ? element(local.tag_values, 1) : "",
-      element(local.tag_keys, 2) != "" ? element(local.tag_keys, 2) : "",
-      element(local.tag_values, 2) != "" ? element(local.tag_values, 2) : "",
-      element(local.tag_keys, 3) != "" ? element(local.tag_keys, 3) : "",
-      element(local.tag_values, 3) != "" ? element(local.tag_values, 3) : "",
-      element(local.tag_keys, 4) != "" ? element(local.tag_keys, 4) : "",
-      element(local.tag_values, 4) != "" ? element(local.tag_values, 4) : "",
-      element(local.tag_keys, 5) != "" ? element(local.tag_keys, 5) : "",
-      element(local.tag_values, 5) != "" ? element(local.tag_values, 5) : "",
-      element(local.tag_keys, 6) != "" ? element(local.tag_keys, 6) : "",
-      element(local.tag_values, 6) != "" ? element(local.tag_values, 6) : "",
-      element(local.tag_keys, 7) != "" ? element(local.tag_keys, 7) : "",
-      element(local.tag_values, 7) != "" ? element(local.tag_values, 7) : "",
-      element(local.tag_keys, 8) != "" ? element(local.tag_keys, 8) : "",
-      element(local.tag_values, 8) != "" ? element(local.tag_values, 8) : "",
-    element(local.tag_keys, 9) != "" ? element(local.tag_keys, 9) : "",
-    element(local.tag_values, 9) != "" ? element(local.tag_values, 9) : "",
-    "true"
-  )
-  # Slicing to the length of the map of tags so we dont get blank or repeating tags
-    ), 0, length(local.tag_keys) - 1)
BREAKS HERE
-    allowed_origins = ["${var.domain}"]
BREAKS HERE
-  name_filter  = "4.10.8 docker #1"
BREAKS HERE
-  node_count     = "1"
-      host    = "${element(module.node.*.public_ip, var.node_count)}"
BREAKS HERE
-  source     = "git::https://github.com/cloudposse/tf_label.git?ref=tags/0.2.0"
-  tags        = "${module.label.tags}"
-  source    = "git::https://github.com/cloudposse/tf_hostname.git?ref=tags/0.1.0"
BREAKS HERE
-  workstation_cidr = "${coalesce(var.workstation_cidr, local.workstation_external_cidr)}"
BREAKS HERE
-  count = "${local.count}"
-
BREAKS HERE
-  name                             = "bastion-service-${var.vpc}"
-  name     = "bastion-service-${var.vpc}"
BREAKS HERE
-  default = "travisci/worker:v4.0.1"
BREAKS HERE
-  base_instance_name = "${var.env}-${var.index}-worker-com"
-  base_instance_name = "${var.env}-${var.index}-worker-com-free"
-  base_instance_name = "${var.env}-${var.index}-worker-org"
BREAKS HERE
-      port_range  = "all"
-      port_range  = "all"
-      protocol              = "icmp"
-      port_range            = "all"
-      protocol              = "tcp"
-      port_range            = "all"
BREAKS HERE
-  master_ipv4_cidr_block = "10.0.90.0/28"
-  private_cluster        = true
BREAKS HERE
-/*
-admin subnets:
-  3
-  4
-  5
-    Maybe should figure out a better way to do this...
-
-ec2 subnets:
-  6
-  7
-  8
-*/
-
-
-  associate_public_ip_address=true
-      "sudo /tmp/terraform/provision.sh ${var.ppa_repo_key} ${var.docker_repo_key} ${var.region} ${aws_iam_access_key.spinnaker.id} ${aws_iam_access_key.spinnaker.secret} ${var.internal_dns_zone}"
-# The following two instances are for test purposes only and should be removed before we make this public
-/* TEST Spinnaker instance */
-#resource "aws_instance" "spinnaker_test" {
-#  ami = "ami-84baade5"
-#  instance_type = "${var.spinnaker_instance_type}"
-#  subnet_id = "${aws_subnet.public_subnet.5.id}"
-#  vpc_security_group_ids = ["${aws_security_group.infra_spinnaker.id}", "${aws_security_group.vpc_sg.id}", "${aws_security_group.mgmt_sg.id}"]
-#  iam_instance_profile = "${aws_iam_instance_profile.spinnaker_instance_profile.id}"
-#
-#    Name = "Spinnaker WORKING I HOPE host"
-#}
-
-/* TEST bastion instance */
-resource "aws_instance" "bastion_test" {
-  ami = "${module.tf_kenzan.ami_id}"
-  instance_type = "${var.bastion_instance_type}"
-  subnet_id = "${aws_subnet.public_subnet.3.id}"
-  vpc_security_group_ids = ["${aws_security_group.adm_bastion.id}", "${aws_security_group.vpc_sg.id}", "${aws_security_group.mgmt_sg.id}"]
-  associate_public_ip_address=true
-  key_name = "${var.ssh_key_name}"
-  tags = {
-    Name = "bastion TEST host"
-    created_by = "${var.created_by}"
-  }
-
-  connection {
-    user = "${var.ssh_user}"
-    key_file = "${var.ssh_private_key_location}"
-    agent = false
-  }
-
-  provisioner "file" {
-    source = "${var.ssh_private_key_location}"
-    destination = "/home/${var.ssh_user}/.ssh/id_rsa"
-  }
-
-  provisioner "remote-exec" {
-    inline = [
-      "chmod 0600 /home/${var.ssh_user}/.ssh/id_rsa"
-    ]
-  }
-}
BREAKS HERE
-    : file(var.etcd_client_cert)
-  count    = "${var.experimental_enabled || var.etcd_tls_enabled ? 1 : 0}"
-  count    = "${var.experimental_enabled || var.etcd_tls_enabled ? 1 : 0}"
-  count    = "${var.experimental_enabled || var.etcd_tls_enabled ? 1 : 0}"
BREAKS HERE
-  source                 = "./modules/live_task_lookup/"
-  create                 = "${var.create}"
-  ecs_cluster_id         = "${var.ecs_cluster_id}"
-  ecs_service_name       = "${var.name}"
-  container_name         = "${var.container_name}"
-  lambda_lookup_role_arn = "${module.iam.lambda_lookup_role_arn}"
-  lookup_type            = "${var.live_task_lookup_type}"
BREAKS HERE
-    volume_size = 250
-    volume_size = 52
-  desired_capacity = "5"
-  max_size = "5"
-  min_size = "3"
-    "aws_launch_configuration.worker",
BREAKS HERE
-  count               = "${var.master_count}"
-  name                = "${var.cluster_name}-master-${count.index}"
-  location            = "${var.location}"
-  resource_group_name = "${var.resource_group_name}"
-    load_balancer_backend_address_pools_ids = ["${azurerm_lb_backend_address_pool.api-lb.id}"]
BREAKS HERE
-* You can specify rules based on host header with the `rules_host` variable,
-* path pattern with the `rules_path` variable, or both with the `rules_host_and_path`
-* variable. Rules from the three variables are merged and prioritised in the order
-* `rules_host_and_path`, `rules_host` and `rules_path`.
-*
-* The three variables are map types. The values of the maps define the target group
-* port and protocol where requests are routed when they meet the rule condition, with
-* the format TARGET_GROUP_PROTOCOL:TARGET_GROUP_PORT.
-*
-* The keys of `rules_host` are evaluated against the Host header of the request. The
-* keys of `rules_path` are evaluated against the path of the request. If the 
-* `rules_host_and_path` variable is provided, the key has the format FIELD:VALUE.
-* FIELD must be one of 'path-pattern' for path based routing or 'host-header' for host
-* based routing.
-*
-*```
-* rules_host {
-*   "www.example1.com" = "HTTP:8080"
-*   "www.example2.com" = "HTTPS:9091"
-*   "www.example3.*"   = "HTTP:8080"
-* }
-*
-* rules_host_and_path {
-*  "host-header:www.example1.com" = "HTTP:8080"
-*  "host-header:www.example2.com" = "HTTPS:9091"
-*  "path-pattern:/example3"       = "HTTPS:9091"
-* }
-*```
-*
-  type        = "map"
-  description = "A map with the value of a host-header rule condition and the target group associated."
-  default     = {}
-variable "rules_path" {
-  type        = "map"
-  description = "A map with the value of a path-pattern rule condition and the target group associated"
-  default     = {}
-variable "rules_host_and_path" {
-  type        = "map"
-  description = "A map with the value of a rule with the format FIELD:VALUE and the target group associated. FIELD can be one of 'host-header' or 'path-pattern'"
-  default     = {}
-variable "name" {
-  description = "Prefix of the target group names. The final name is name-PROTOCOL-PORT."
-# Resources
-#--------------------------------------------------------------
-locals {
-  hosts = "${zipmap(formatlist("%s:%s", "host-header", keys(var.rules_host)), values(var.rules_host))}"
-  paths = "${zipmap(formatlist("%s:%s", "path-pattern", keys(var.rules_path)), values(var.rules_path))}"
-  rules = "${merge(var.rules_host_and_path, local.hosts, local.paths)}"
-locals {
-  target_groups = "${distinct(values(local.rules))}"
-  count                = "${length(local.target_groups)}"
-  name                 = "${var.name}-${replace(element(local.target_groups, count.index), ":", "-")}"
-  port                 = "${element(split(":", element(local.target_groups, count.index)), 1)}"
-  protocol             = "${element(split(":", element(local.target_groups, count.index)), 0)}"
-    path                = "/"
-    matcher             = "200-499"
-    protocol            = "${element(split(":", element(local.target_groups, count.index)), 0)}"
-locals {
-  target_groups_arns = "${zipmap(aws_lb_target_group.tg.*.name, aws_lb_target_group.tg.*.arn)}"
-  count        = "${length(keys(local.rules))}"
-  priority     = "${count.index + 1}"
-    target_group_arn = "${lookup(local.target_groups_arns, "${var.name}-${replace(element(values(local.rules), count.index), ":", "-")}")}"
-    field  = "${element(split(":", element(keys(local.rules), count.index)), 0)}"
-    values = ["${element(split(":", element(keys(local.rules), count.index)), 1)}"]
BREAKS HERE
-      port_range       = "22"
-      source_addresses = ["0.0.0.0/0", "::/0"]
-      protocol         = "tcp"
-      port_range       = "80"
-      source_addresses = ["0.0.0.0/0", "::/0"]
-      port_range       = "443"
BREAKS HERE
-  required_version = ">= 0.9.3, != 0.9.5"
-  count = 1
-
-  subnet_id = "${tolist(data.aws_subnet_ids.selected.ids)[count.index]}"
-  count = 1
-
-  subnet_id = "${tolist(data.aws_subnet_ids.selected.ids)[count.index]}"
BREAKS HERE
-  name                        = "${var.cluster_name}-${lookup(var.worker_groups[count.index], "name", count.index)}"
-  associate_public_ip_address = true
BREAKS HERE
-  count = "${length(var.private_subnets)}"
BREAKS HERE
-  count         = 3
-  ami           = "ami-40d28157"
-  instance_type = "t2.micro"
-}
-
-resource "aws_instance" "example" {
BREAKS HERE
-  public_key      = "${file("${path.root}/../temp_key.pub")}"
BREAKS HERE
-  iam_role        = "${aws_iam_role.ecs.name}"
BREAKS HERE
-    template = "${file("${path.module}/templates/key_policy.json.tpl")}"
-    vars {
-      tag_product   = "${var.tag_product}"
-      tag_env       = "${var.tag_env}"
-      key_admin_arn = "${aws_iam_role.role.arn}"
-      account_id    = "${data.aws_caller_identity.current.account_id}"
-    }
-    template = "${file("${path.module}/templates/iam_instance_role_policy.json.tpl")}"
-    vars {
-      tag_product      = "${var.tag_product}"
-      tag_env          = "${var.tag_product}"
-      db_credstash_arn = "${aws_dynamodb_table.db_credstash.arn}"
-    }
-
-  depends_on              = ["null_resource.waiter"]
-  description             = "Credstash space for ${var.tag_product}-${var.tag_env}"
-
-  depends_on    = ["aws_kms_key.credstash"]
-  name               = "${var.tag_product}-${var.tag_env}"
-
-
-  ami                    = "${var.ami_id}"
-  instance_type          = "${var.instance_type}"
-  key_name               = "${var.aws_key_name}"
-  user_data              = "${data.template_file.user_data.rendered}"
-    "${aws_security_group.allow_from_office.id}"
-  subnet_id              = "${var.public_subnet_id}"
-  iam_instance_profile   = "${aws_iam_instance_profile.ec2_profile.name}"
-    instance = "${aws_instance.pritunl.id}"
-    vpc      = true
BREAKS HERE
-    "k8s_rbac_authorization_k8s_io_v1_role_binding.this",
BREAKS HERE
-    template = "${file("${artifacts_dir}/policy.json")}"
BREAKS HERE
-    tectonic_version = "${var.versions["tectonic"]}"
BREAKS HERE
-variable image_id {}
-    id = "${var.image_id}"
BREAKS HERE
-  name = "${element(split("|", "${null_resource.dummy_dependency.id}|${element(concat(google_compute_instance_group_manager.default.*.name, list("unused")), 0)}"), 1)}"
BREAKS HERE
-  identifier                   = "${var.envname}-aurora-node-0"
-  identifier                   = "${var.envname}-aurora-node-${count.index + 1}"
-  cluster_identifier              = "${var.envname}-aurora-cluster"
BREAKS HERE
-   "${aws_instance.ms_b_2.id}"
-   "${aws_instance.ms_b_2.id}"
-   "${aws_instance.ms_b_2.id}"
-   "${aws_instance.ms_b_2.id}"
-   "${aws_instance.ms_b_2.id}"
BREAKS HERE
-  credentials_file_path = "${path.module}/sa-key.json"
BREAKS HERE
-  source     = "git::https://github.com/cloudposse/terraform-terraform-label.git?ref=tags/0.1.6"
-  count    = "${var.enabled == "true" ? 1 : 0}"
-  name     = "${module.label.id}"
-  role_arn = "${join("", aws_iam_role.default.*.arn)}"
-  version  = "${var.kubernetes_version}"
-    security_group_ids = ["${join("", aws_security_group.default.*.id)}"]
-    subnet_ids         = ["${var.subnet_ids}"]
BREAKS HERE
-    image = "quay.io/americanredcross/osm-stats-workers:refactor"
BREAKS HERE
-  availability_zones = ["${var.availability_zones}"]
BREAKS HERE
-resource "aws_route53_zone" "innnovation_toplevel" {
-  name = "innnovation.gov"
-resource "aws_route53_record" "innnovation_gov_apex" {
-  zone_id = "${aws_route53_zone.innnovation_toplevel.zone_id}"
-  name = "innnovation.gov."
-    name = "d2q1i25any8vwy.cloudfront.net."
-resource "aws_route53_record" "innnovation_gov_www" {
-  zone_id = "${aws_route53_zone.innnovation_toplevel.zone_id}"
-  name = "www.innnovation.gov."
-    name = "d2q1i25any8vwy.cloudfront.net."
-resource "aws_route53_record" "demo_innnovation_gov_a" {
-  zone_id = "${aws_route53_zone.innnovation_toplevel.zone_id}"
-  name = "demo.innnovation.gov."
-    name = "d1f2igtqmwwbgm.cloudfront.net."
-output "innnovation_ns" {
-  value="${aws_route53_zone.innnovation_toplevel.name_servers}"
BREAKS HERE
-  source_dir  = "${.path.module}/tmp"
BREAKS HERE
-    nat_gateway_id = "${aws_nat_gateway.backup_lambda.id}"
BREAKS HERE
-  availability_zones = ["${var.availability_zones}"]
BREAKS HERE
-        "VAULT_IMAGE" = "${var.vault_IMAGE}"
BREAKS HERE
-resource "aws_instance" "test_ric03uec_centos7" {
-    Name = "test_ric03uec_centos7_${count.index}_${var.install_version}"
-output "test_ric03uec_centos7" {
-  value = "${formatlist("instance %v has private ip %v", aws_instance.test_ric03uec_centos7.*.id, aws_instance.test_ric03uec_centos7.*.private_ip)}"
BREAKS HERE
-  security_groups = ["${aws_security_group.chef_server.id}"]
-  security_groups = ["${aws_security_group.build_nodes.id}"]
-  security_groups = ["${aws_security_group.chef_automate.id}"]
BREAKS HERE
-  source = "http://schnell.nue.suse.com/sumaform-images/centos7_v2.qcow2"
BREAKS HERE
-  # just for demonstration purposes - not otherwise used
-  private_subnets = ["${cidrsubnet(var.vpc_cidr, 8, 1)}"]
-
-  num_gateway_subnets = "1"
-  gateway_subnet_ids  = "${module.network.private_subnets}"
BREAKS HERE
-  subnets         = ["${aws_default_subnet.default_az1.id}", "${aws_default_subnet.default_az2.id}"]
-  vpc_id               = "${aws_default_vpc.default.id}"
BREAKS HERE
-    database_hostname = "${module.db_instance.this_db_instance_endpoint}"
BREAKS HERE
-    ignition_endpoint  = "${var.tectonic_matchbox_http_endpoint}/ignition"
-    baseurl            = "${var.tectonic_matchbox_http_endpoint}/assets/coreos"
BREAKS HERE
-  log_group_name = "${aws_cloudwatch_log_group.log.name}"
-  iam_role_arn   = "${aws_iam_role.vpc_flow_logs_role.arn}"
-  vpc_id         = "${module.vpc.vpc_id}"
-  traffic_type   = "${var.traffic_type}"
BREAKS HERE
-  asg_tags                  = ["${null_resource.tags_as_list_of_maps.*.triggers}"]
-  # Followed recommendation http://67bricks.com/blog/?p=85 to workaround terraform not supporting short circut evaluation
BREAKS HERE
-variable image_resource_group {
-  image_id            = "/subscriptions/${data.azurerm_client_config.current.subscription_id}/resourceGroups/${var.image_resource_group}/providers/Microsoft.Compute/images/${var.kubenow_image}"
-  image_id            = "/subscriptions/${data.azurerm_client_config.current.subscription_id}/resourceGroups/${var.image_resource_group}/providers/Microsoft.Compute/images/${var.kubenow_image}"
-  image_id            = "/subscriptions/${data.azurerm_client_config.current.subscription_id}/resourceGroups/${var.image_resource_group}/providers/Microsoft.Compute/images/${var.kubenow_image}"
-  image_id            = "/subscriptions/${data.azurerm_client_config.current.subscription_id}/resourceGroups/${var.image_resource_group}/providers/Microsoft.Compute/images/${var.kubenow_image}"
BREAKS HERE
-  consul_ami_id            = "ami-a23feadf"
-  nomad_ami_id             = "ami-a23feadf"
BREAKS HERE
-  logging_es_nodes = "1"
BREAKS HERE
-  private_ip = element(
-    distinct(compact(concat([var.private_ip], var.private_ips))),
-    count.index,
-  )
-  ipv6_address_count = var.ipv6_address_count
-  ipv6_addresses     = var.ipv6_addresses
-      private_ip,
-  private_ip = element(
-    distinct(compact(concat([var.private_ip], var.private_ips))),
-    count.index,
-  )
-  ipv6_address_count = var.ipv6_address_count
-  ipv6_addresses     = var.ipv6_addresses
-      private_ip,
BREAKS HERE
-resource "aws_elb" "lb_g_www" { ##   name = "lb-g-www-${var.install_version}" ##   connection_draining = true
-## resource "aws_elb" "lb_b_www" {
-##  name = "lb-b-www-${var.install_version}"
-##  connection_draining = true
-##  subnets = [
-##    "${var.sn_public_ship_id}"]
-##  security_groups = [
-##    "${aws_security_group.sg_public_lb.id}"]
-##
-##  listener {
-##    lb_port = 443
-##    lb_protocol = "https"
-##    instance_port = 50002
-##    instance_protocol = "http"
-##    ssl_certificate_id = "${var.acm_cert_arn_20170309}"
-##  }
-##
-##  listener {
-##    lb_port = 80
-##    lb_protocol = "http"
-##    instance_port = 50002
-##    instance_protocol = "http"
-##  }
-##
-##  health_check {
-##    healthy_threshold = 2
-##    unhealthy_threshold = 2
-##    timeout = 10
-##    target = "TCP:50002"
-##    interval = 30
-##  }
-##
-##  instances = ["${aws_instance.ms_b.*.id}"]
-## }
-##
-## # APP ELB
-## resource "aws_elb" "lb_b_app" {
-##  name = "lb-b-app-${var.install_version}"
-##  connection_draining = true
-##  subnets = [
-##    "${var.sn_public_ship_id}"]
-##  security_groups = [
-##    "${aws_security_group.sg_public_lb.id}"]
-##
-##  listener {
-##    lb_port = 443
-##    lb_protocol = "ssl"
-##    instance_port = 50001
-##    instance_protocol = "tcp"
-##    ssl_certificate_id = "${var.acm_cert_arn_20170309}"
-##  }
-##
-##  health_check {
-##    healthy_threshold = 2
-##    unhealthy_threshold = 2
-##    timeout = 3
-##    target = "HTTP:50001/"
-##    interval = 5
-##  }
-##
-##  instances = ["${aws_instance.ms_b.*.id}"]
-## }
-##
-## # API ELB
-## resource "aws_elb" "lb_b_api" {
-##  name = "lb-b-api-${var.install_version}"
-##  connection_draining = true
-##  subnets = [
-##    "${var.sn_public_ship_id}"]
-##  security_groups = [
-##    "${aws_security_group.sg_public_lb.id}"]
-##
-##  listener {
-##    lb_port = 443
-##    lb_protocol = "https"
-##    instance_port = 50000
-##    instance_protocol = "http"
-##    ssl_certificate_id = "${var.acm_cert_arn_20170309}"
-##  }
-##
-##  health_check {
-##    healthy_threshold = 2
-##    unhealthy_threshold = 5
-##    timeout = 3
-##    target = "HTTP:50000/"
-##    interval = 5
-##  }
-##
-##  instances = ["${aws_instance.ms_b.*.id}"]
-## }
-##
-## # API INT ELB
-## resource "aws_elb" "lb_b_api_int" {
-##  name = "lb-b-api-int-${var.install_version}"
-##  connection_draining = true
-##  subnets = [
-##    "${var.sn_public_ship_id}"]
-##  security_groups = [
-##    "${aws_security_group.sg_public_lb.id}"]
-##
-##  listener {
-##    lb_port = 443
-##    lb_protocol = "https"
-##    instance_port = 50004
-##    instance_protocol = "http"
-##    ssl_certificate_id = "${var.acm_cert_arn_20170309}"
-##  }
-##
-##  health_check {
-##    healthy_threshold = 2
-##    unhealthy_threshold = 5
-##    timeout = 3
-##    target = "HTTP:50004/"
-##    interval = 5
-##  }
-##
-##  instances = ["${aws_instance.ms_b.*.id}"]
-## }
-##
-## # API CON ELB
-## resource "aws_elb" "lb_b_api_con" {
-##  name = "lb-b-api-con-${var.install_version}"
-##  connection_draining = true
-##  idle_timeout = 150
-##  subnets = [
-##    "${var.sn_public_ship_id}"]
-##  security_groups = [
-##    "${aws_security_group.sg_public_lb.id}"]
-##
-##  listener {
-##    lb_port = 443
-##    lb_protocol = "https"
-##    instance_port = 50005
-##    instance_protocol = "http"
-##    ssl_certificate_id = "${var.acm_cert_arn_20170309}"
-##  }
-##
-##  health_check {
-##    healthy_threshold = 2
-##    unhealthy_threshold = 5
-##    timeout = 3
-##    target = "HTTP:50005/"
-##    interval = 5
-##  }
-##
-##  instances = ["${aws_instance.ms_b.*.id}"]
-## }
BREAKS HERE
-  count    = "3"
-  count    = "3"
-  count    = "3"
-    subnets                     = "${join("\n", formatlist("  - worker-%s", data.aws_availability_zones.available.names))}"
-    min                         = "${length(data.aws_availability_zones.available.names)}"
-    subnets                     = "${join("\n", formatlist("  - worker-%s", data.aws_availability_zones.available.names))}"
BREAKS HERE
-  security_groups             = "${concat([aws_security_group.ecs.id], var.security_group_ids)}"
BREAKS HERE
-    ca_cert = "${base64encode(tls_self_signed_cert.kube-ca.cert_pem)}"
-    ca_cert = "${base64encode(tls_self_signed_cert.kube-ca.cert_pem)}"
BREAKS HERE
-    ecs_config      = "${var.ecs_config}"
-    ecs_logging     = "${var.ecs_logging}"
-    cluster_name    = "${var.cluster}"
-    env_name        = "${var.environment}"
-    custom_userdata = "${var.custom_userdata}"
BREAKS HERE
-name = CouchDB 
BREAKS HERE
-    from_port   = "${var.private_ssh_port}"
-    protocol    = "TCP"
-    to_port     = "${var.private_ssh_port}"
-    cidr_blocks = ["0.0.0.0/0"]
-  }
-
-  egress {
-    from_port   = 443
-    to_port     = 443
-    protocol    = "TCP"
-    cidr_blocks = ["0.0.0.0/0"]
-  }
-
-  egress {
-    from_port   = 80
-    to_port     = 80
BREAKS HERE
-  zone           = "us-east4-b"
BREAKS HERE
-    destination = "/tmp/foundationdb.conf"
BREAKS HERE
-  description = "SG form VMs allocated by CircleCI for Remote Docker and machine executor"
BREAKS HERE
-  security_group_id = "${aws_security_group.prometheus.id}"
BREAKS HERE
-    name = "${lookup(var.docker_volume, "name") != "" ? lookup(var.docker_volume, "name") : ""}"
-      autoprovision = "${lookup(var.docker_volume, "autoprovision") != "" ? lookup(var.docker_volume, "autoprovision") : ""}"
-      scope         = "${lookup(var.docker_volume, "scope") != "" ? lookup(var.docker_volume, "scope") : ""}"
-      driver        = "${lookup(var.docker_volume, "driver") != "" ? lookup(var.docker_volume, "driver") : ""}"
BREAKS HERE
-    "${aws_security_group.sg_private_ship_install.id}"]
-]
BREAKS HERE
-  default     = "10000000"
BREAKS HERE
-variable image_id {}
-    id = "${var.image_id}"
BREAKS HERE
-  type              = "egress"
-  from_port         = "0"
-  to_port           = "65535"
-  protocol          = "all"
-  cidr_blocks       = ["0.0.0.0/0"]
-  ipv6_cidr_blocks  = ["::/0"]
-  name_prefix   = "${var.name}-"
-  image_id      = "${var.ami}"
-  instance_type = "${var.instance_type}"
-  user_data     = "${data.template_file.user_data.rendered}"
-  tag {
-    key                 = "Name"
-    value               = "${var.name}"
-    propagate_at_launch = true
-  }
-
-  tag {
-    key                 = "EIP"
-    value               = "${var.eip}"
-    propagate_at_launch = true
-  }
BREAKS HERE
-    value = off
BREAKS HERE
-resource "template_folder" "bootkube-bootstrap" {
-  input_path  = "${path.module}/resources/bootstrap-manifests"
-  output_path = "${path.cwd}/generated/bootstrap-manifests"
BREAKS HERE
-  master_ipv4_cidr_block = "10.0.90.0/28"
-  private_cluster        = true
BREAKS HERE
-  name           = "${terraform.workspace}-worker-${count.index + 1}"
-    destination = "~/.ssh/${terraform.workspace}.pem"
-      "docker swarm leave",
-  # remove node on destroy
BREAKS HERE
-variable "stack_item_label" {}
-variable "stack_item_fullname" {}
-variable "organization" {}
-variable "vpc_stack_name" {}
-variable "region" {}
-variable "ami" {}
-variable "instance_type" {}
-variable "instance_profile" {}
-variable "key_name" {}
-variable "cluster_max_size" {}
-variable "cluster_min_size" {}
BREAKS HERE
-  prevent_destroy = "false"
BREAKS HERE
-
-    // Set explicit name to match the new default name set by the API.
-    // https://github.com/terraform-providers/terraform-provider-google/issues/574
-    device_name = "persistent-disk-0"
BREAKS HERE
-  encrypted = true
-}
BREAKS HERE
-  db_subnet_group_name = "${aws_subnet.private-subnet.id}"
BREAKS HERE
-    ip_configuration            = ["${var.ip_configuration}"]
BREAKS HERE
-    zone_awareness_enabled   = "${var.zone_awareness}"
BREAKS HERE
-  count                    = "${length(var.allowed_inbound_security_group_ids)}"
-  count                    = "${length(var.allowed_inbound_security_group_ids)}"
-  count                    = "${length(var.allowed_inbound_security_group_ids)}"
-  count                    = "${length(var.allowed_inbound_security_group_ids)}"
-  count                    = "${length(var.allowed_inbound_security_group_ids)}"
-  count                    = "${length(var.allowed_inbound_security_group_ids)}"
-  count                    = "${length(var.allowed_inbound_security_group_ids)}"
-  serf_lan_port   = "${var.serf_lan_port}"
BREAKS HERE
-  type = "string"
-  type = "string"
-  type = "string"
-  default = "redis"
BREAKS HERE
-  tag {
-    key                 = "Name"
-    value               = "${var.cluster_name}-master"
-    propagate_at_launch = true
-  }
-
-  tag {
-    key                 = "kubernetes.io/cluster/${var.cluster_name}"
-    value               = "owned"
-    propagate_at_launch = true
-  }
-
-  tags = ["${var.autoscaling_group_extra_tags}"]
BREAKS HERE
-  source = "terraform-aws-modules/vpn-gateway/aws"
-  vpn_gateway_id      = "${module.vpc.vgw_id}"
-  customer_gateway_id = "${aws_customer_gateway.main.id}"
-  vpc_id              = "${module.vpc.vpc_id}"
-  vpc_subnet_ids      = ["${module.vpc.private_route_table_ids}"]
-  source = "../../"
BREAKS HERE
-    console                   = "quay.io/coreos/tectonic-console:v1.0.3"
-    kube_version_operator     = "quay.io/coreos/kube-version-operator:79aa49c6b225f27e4479a502efeaae4195e1070e"
-    tectonic_channel_operator = "quay.io/coreos/tectonic-channel-operator:3d36eba4ecd9cbc50e1f3773d07e6bc358b00096"
-    ingress_controller        = "gcr.io/google_containers/nginx-ingress-controller:0.8.3"
-    exechealthz               = "gcr.io/google_containers/exechealthz-amd64:1.2"
-    kubernetes = "1.5.3+tectonic.1"
-    tectonic   = "1.5.3-tectonic.1"
BREAKS HERE
-  count  = "${var.enabled == "true" && length(var.groups) > 0 ? 1 : 0}"
-  user   = "${aws_iam_user.default.name}"
-  groups = ["${var.groups}"]
BREAKS HERE
-  count = "${length(var.subnets_elb)}"
-  id    = "${var.subnets_elb[count.index]}"
-  from_port         = "${var.elb_healthcheck_port}"
-  to_port           = "${var.elb_healthcheck_port}"
BREAKS HERE
-  source     = "git::https://github.com/cloudposse/terraform-null-label.git?ref=tags/0.2.1"
-
-  count = "${signum(length(var.replication_source_principal_arn))}"
-      type = "AWS"
-      identifiers = ["${var.replication_source_principal_arn}"]
-  # Support deployment ARNs
-    resources = ["${aws_s3_bucket.default.arn}",
-      "${aws_s3_bucket.default.arn}/*",
-      identifiers = ["${var.deployment_arns}"]
BREAKS HERE
-  source        = "modules/repository"
-  name          = "openvpn"
-  cookbook_team = "${github_team.openvpn.id}"
BREAKS HERE
-  count            = "${length(aws_instance.haproxy_node.*.id)}"
BREAKS HERE
-    from_port = 2300
-    to_port = 2300
BREAKS HERE
-  ip      = "10.138.1.1"
-  ip      = "10.138.1.2"
-  ip      = "10.138.1.3"
BREAKS HERE
-  auth_url    = "${var.auth_url}/v2.0"
-  public_key = "${replace(\"${file(\"bosh.key.pub\")}\",\"\n\",\"\")}"
BREAKS HERE
-  name_prefix = "${var.cluster_name}-"
-  name_prefix = "${var.cluster_name}-"
-  name_prefix = "${var.cluster_name}-"
-  name_prefix        = "${var.cluster_name}-"
BREAKS HERE
-# with the arithmetic of / and % records with all combinations of var.iplist and var.record_names will be crated
BREAKS HERE
-      value               = "${var.cluster_tag_value}"
BREAKS HERE
-  name_prefix          = "${coalesce(var.asg_name, var.name)}-"
BREAKS HERE
-  source     = "git::https://github.com/cloudposse/tf_label.git?ref=tags/0.2.0"
-  source    = "git::https://github.com/cloudposse/tf_hostname.git?ref=tags/0.1.0"
BREAKS HERE
-  account_id = "${local.account_id}"
BREAKS HERE
-      subnet_name   = "subnet-01"
-    subnet-01 = [
-        range_name    = "subnet-01-secondary"
BREAKS HERE
-      ap-northeast-1  = "ami-c7c838a6"
-      ap-northeast-2  = "ami-b477bcda"
-      ap-southeast-1  = "ami-5775a734"
-      ap-southeast-2  = "ami-654e6506"
-      eu-central-1    = "ami-3401ea5b"
-      eu-west-1       = "ami-2237ad51"
-      sa-east-1       = "ami-8360f5ef"
-      us-east-1       = "ami-564df541"
-      us-west-1       = "ami-0b4a0d6b"
-      us-west-2       = "ami-b1a667d1"
-      cn-north-1      = "ami-0679b06b"
-      us-gov-west-1   = "ami-30b8da13"
BREAKS HERE
-    "${aws_instance.ms_3.id}",
-    "${aws_instance.ms_4.id}"
-    "${aws_instance.ms_3.id}",
-    "${aws_instance.ms_4.id}"
-    "${aws_instance.ms_3.id}",
-    "${aws_instance.ms_4.id}"
BREAKS HERE
-    subnets                       = ""                              # A comma delimited string of subnets to place the worker nodes in. i.e. subnet-123,subnet-456,subnet-789
BREAKS HERE
- * ## Run Tests on the VPC Scenario 1 Module
-  cidr        = "10.23.0.0/16"
-module "public-ssh-sg" {
-  name                = "${var.name}"
-  vpc_id              = "${module.vpc.vpc_id}"
-  allowed_cidr_blocks = "0.0.0.0/0"
-module "open-egress-sg" {
-  name   = "${var.name}"
-  vpc_id = "${module.vpc.vpc_id}"
-
-  vpc_security_group_ids = ["${module.public-ssh-sg.id}",
-    "${module.open-egress-sg.id}",
-  ]
BREAKS HERE
-    version = "${ var.coreos-hyperkube-tag }"
-    version = "${ var.coreos-hyperkube-tag }"
BREAKS HERE
-    name                   = "${aws_elb.frontend_elb.dns_name}"
-    zone_id                = "${aws_elb.frontend_elb.zone_id}"
-  source                        = "../../modules/aws/node_group"
-  name                          = "${var.stackname}-frontend"
-  default_tags                  = "${map("Project", var.stackname, "aws_stackname", var.stackname, "aws_environment", var.aws_environment, "aws_migration", "frontend", "aws_hostname", "frontend-1")}"
-  instance_subnet_ids           = "${data.terraform_remote_state.infra_networking.private_subnet_ids}"
-  instance_security_group_ids   = ["${data.terraform_remote_state.infra_security_groups.sg_frontend_id}", "${data.terraform_remote_state.infra_security_groups.sg_management_id}"]
-  instance_type                 = "${var.instance_type}"
-  instance_additional_user_data = "${join("\n", null_resource.user_data.*.triggers.snippet)}"
-  instance_elb_ids_length       = "1"
-  instance_elb_ids              = ["${aws_elb.frontend_elb.id}"]
-  instance_ami_filter_name      = "${var.instance_ami_filter_name}"
-  asg_max_size                  = "${var.asg_size}"
-  asg_min_size                  = "${var.asg_size}"
-  asg_desired_capacity          = "${var.asg_size}"
-  asg_notification_topic_arn    = "${data.terraform_remote_state.infra_monitoring.sns_topic_autoscaling_group_events_arn}"
-  root_block_device_volume_size = "${var.root_block_device_volume_size}"
BREAKS HERE
-      source_addresses = ["0.0.0.0/0"]
-      source_addresses = ["0.0.0.0/0"]
-      source_addresses = ["0.0.0.0/0"]
BREAKS HERE
-variable "zone" {
-  default = "us-central1-a"
-  zone                     = "${var.zone}"
-  min_master_version       = "1.11"
-  zone               = "${var.zone}"
-    auto_upgrade = true
BREAKS HERE
-resource "aws_instance" "etcd_test_04" {
BREAKS HERE
-data "aws_ami" "ubuntu" {
-  most_recent = true
-  filter {
-    name   = "name"
-    values = ["ubuntu/images/hvm-ssd/ubuntu-trusty-14.04-amd64-server-*"]
-  }
-  filter {
-    name   = "virtualization-type"
-    values = ["hvm"]
-  }
-  owners = ["099720109477"] # Canonical
-}
-resource "aws_instance" "backup_lambda" {
-  count = "${var.aws_region =="us-east-1" ?1:0}"
-  source_dest_check = false
-  associate_public_ip_address = true
-  ami           = "${data.aws_ami.ubuntu.id}"
-  instance_type = "t2.micro"
-  key_name = "quorum-cluster-${var.aws_region}-network-${var.network_id}"
-  subnet_id = "${aws_subnet.backup_lambda.id}"
-  vpc_security_group_ids = ["${aws_security_group.allow_all_for_backup_lambda.*.id}"]
-  tags {
-    Name = "quorum-network-${var.network_id}-BackupLambda-NAT-backup_lambda-1"
-    subnet_id = "BackupLambdaAccessInternet-${aws_subnet.backup_lambda.id}"
-  }
-}
-resource "aws_instance" "observer" {
-  count = "${var.aws_region =="us-east-1" && lookup(var.observer_node_counts, var.aws_region, 0) > 0?1:0}"
-  source_dest_check = false
-  associate_public_ip_address = true
-  ami           = "${data.aws_ami.ubuntu.id}"
-  instance_type = "t2.micro"
-  key_name = "quorum-cluster-${var.aws_region}-network-${var.network_id}"
-  subnet_id = "${aws_subnet.quorum_observer.0.id}"
-  vpc_security_group_ids = ["${aws_security_group.allow_all_for_backup_lambda.*.id}"]
-  tags {
-    Name = "quorum-network-${var.network_id}-BackupLambda-NAT-observer-1"
-    subnet_id = "BackupLambdaAccessInternet-${aws_subnet.quorum_observer.0.id}"
-  }
-}
-resource "aws_instance" "validator" {
-  count = "${var.aws_region =="us-east-1" && lookup(var.validator_node_counts, var.aws_region, 0)>0?1:0}"
-  source_dest_check = false
-  associate_public_ip_address = true
-  ami           = "${data.aws_ami.ubuntu.id}"
-  instance_type = "t2.micro"
-  key_name = "quorum-cluster-${var.aws_region}-network-${var.network_id}"
-  subnet_id = "${aws_subnet.quorum_validator.0.id}"
-  vpc_security_group_ids = ["${aws_security_group.allow_all_for_backup_lambda.*.id}"]
-  tags {
-    Name = "quorum-network-${var.network_id}-BackupLambda-NAT-validator-1"
-    subnet_id = "BackupLambdaAccessInternet-${aws_subnet.quorum_validator.0.id}"
-  }
-}
-resource "aws_instance" "maker" {
-  count = "${var.aws_region =="us-east-1" && lookup(var.maker_node_counts, var.aws_region, 0)>0?1:0}"
-  source_dest_check = false
-  associate_public_ip_address = true
-  ami           = "${data.aws_ami.ubuntu.id}"
-  instance_type = "t2.micro"
-  key_name = "quorum-cluster-${var.aws_region}-network-${var.network_id}"
-  subnet_id = "${aws_subnet.quorum_maker.0.id}"
-  vpc_security_group_ids = ["${aws_security_group.allow_all_for_backup_lambda.*.id}"]
-  tags {
-    Name = "quorum-network-${var.network_id}-BackupLambda-NAT-maker-1"
-    subnet_id = "BackupLambdaAccessInternet-${aws_subnet.quorum_maker.0.id}"
-  }
-}
BREAKS HERE
- *  nat_gateway_ami
-
-resource "aws_route_table_association" "az1_private_rta" {
-  subnet_id = "${aws_subnet.az1_private.id}"
-  route_table_id = "${aws_route_table.az1_private_route_table.id}"
-resource "aws_route_table_association" "az2_private_rta" {
-  subnet_id = "${aws_subnet.az2_private.id}"
-  route_table_id = "${aws_route_table.az2_private_route_table.id}"
-  ami = "${var.nat_gateway_ami}"
-
-  ami = "${var.nat_gateway_ami}"
-resource "aws_eip" "az1_nat_eip" {
-  instance = "${aws_instance.az1_private_nat.id}"
-  vpc = true
-resource "aws_eip" "az2_nat_eip" {
-  instance = "${aws_instance.az2_private_nat.id}"
-  vpc = true
BREAKS HERE
-  tags = [
-    {
-      key = "Product"
-      value = "Haystack"
-      propagate_at_launch = true
-    },
-    {
-    },
-    {
-      key = "ClusterName"
-      value = "${var.haystack_cluster_name}"
-      propagate_at_launch = true
-    },
-    {
-    },
-    {
-    },
-    //these tags are required by protokube(kops) to set up kubecfg on that host, change with caution
-    {
-      key = "k8s.io/cluster-autoscaler/node-template/label/kops.k8s.io/instancegroup"
-      value = "${local.k8s_master_1_instance_group_name}"
-      propagate_at_launch = true
-    },
-    {
-      propagate_at_launch = true
-    }
-  ]
-
-
-      key = "Product"
-      value = "Haystack"
-      propagate_at_launch = true
-    },
-    {
-      key = "ClusterName"
-      value = "${var.haystack_cluster_name}"
-      propagate_at_launch = true
-    },
-    {
-
-      key = "Product"
-      value = "Haystack"
-      propagate_at_launch = true
-    },
-    {
-      key = "ClusterName"
-      value = "${var.haystack_cluster_name}"
-      propagate_at_launch = true
-    },
-    {
-
-      key = "Product"
-      value = "Haystack"
-      propagate_at_launch = true
-    },
-    {
-      key = "ClusterName"
-      value = "${var.haystack_cluster_name}"
-      propagate_at_launch = true
-    },
-    {
-
-      key = "Product"
-      value = "Haystack"
-      propagate_at_launch = true
-    },
-    {
-      key = "ClusterName"
-      value = "${var.haystack_cluster_name}"
-      propagate_at_launch = true
-    },
-    {
-  tags = {
-    Product = "Haystack"
-    Component = "K8s"
-    ClusterName = "${var.haystack_cluster_name}"
-    Name = "${var.haystack_cluster_name}-k8s-events-1"
-    Role = "${var.haystack_cluster_role}-k8s-masters"
-    //The below tags are used by protokube(kops) to mount the ebs volume, change with caution
-    KubernetesCluster = "${var.k8s_cluster_name}"
-    "k8s.io/etcd/events" = "1/1,2,3"
-    "k8s.io/role/master" = "1"
-  }
-
-  tags = {
-    Product = "Haystack"
-    Component = "K8s"
-    ClusterName = "${var.haystack_cluster_name}"
-    Name = "${var.haystack_cluster_name}-k8s-main-1"
-    Role = "${var.haystack_cluster_role}-k8s-masters"
-    //The below tags are used by protokube(kops) to mount the ebs volume, change with caution
-    KubernetesCluster = "${var.k8s_cluster_name}"
-    "k8s.io/etcd/main" = "1/1,2,3"
-    "k8s.io/role/master" = "1"
-  }
-
-  tags = {
-    Product = "Haystack"
-    Component = "K8s"
-    ClusterName = "${var.haystack_cluster_name}"
-    Name = "${var.haystack_cluster_name}-k8s-events-2"
-    Role = "${var.haystack_cluster_role}-k8s-masters"
-    //The below tags are used by protokube(kops) to mount the ebs volume, change with caution
-    KubernetesCluster = "${var.k8s_cluster_name}"
-    "k8s.io/etcd/events" = "2/1,2,3"
-    "k8s.io/role/master" = "1"
-  }
-  tags = {
-    Product = "Haystack"
-    Component = "K8s"
-    ClusterName = "${var.haystack_cluster_name}"
-    Name = "${var.haystack_cluster_name}-k8s-main-2"
-    Role = "${var.haystack_cluster_name}-k8s-masters"
-    //The below tags are used by protokube(kops) to mount the ebs volume, change with caution
-    KubernetesCluster = "${var.k8s_cluster_name}"
-    "k8s.io/etcd/main" = "2/1,2,3"
-    "k8s.io/role/master" = "1"
-  }
-  tags = {
-    Product = "Haystack"
-    Component = "K8s"
-    ClusterName = "${var.haystack_cluster_name}"
-    Name = "${var.haystack_cluster_name}-k8s-events-3"
-    Role = "${var.haystack_cluster_role}-k8s-masters"
-    //The below tags are used by protokube(kops) to mount the ebs volume, change with caution
-    KubernetesCluster = "${var.k8s_cluster_name}"
-    "k8s.io/etcd/events" = "3/1,2,3"
-    "k8s.io/role/master" = "1"
-  }
-
-  tags = {
-    Product = "Haystack"
-    Component = "K8s"
-    ClusterName = "${var.haystack_cluster_name}"
-    Name = "${var.haystack_cluster_name}-k8s-main-3"
-    Role = "${var.haystack_cluster_role}-k8s-masters"
-    //The below tags are used by protokube(kops) to mount the ebs volume, change with caution
-    KubernetesCluster = "${var.k8s_cluster_name}"
-    "k8s.io/etcd/main" = "3/1,2,3"
-    "k8s.io/role/master" = "1"
-  }
-}
BREAKS HERE
-    etcd_enabled       = "${var.tectonic_experimental ? "false" : length(compact(var.tectonic_etcd_servers)) != 0 ? "false" : "true"}"
BREAKS HERE
-  tectonic_vmware_vm_masterips = {
-  tectonic_vmware_vm_worker_hostnames = {
-  tectonic_vmware_vm_workerips = {
BREAKS HERE
-  availability_zones = ["${data.aws_availability_zones.available}"]
BREAKS HERE
-  tags                          = "${merge(map("Name", format("tf-elasticache-%s-%s", var.name, lookup(data.aws_vpc.vpc.tags,"Name","")), var.tags))}"
BREAKS HERE
-resource "aws_route53_record" "digitalgov_gov_openopps_digitalgov_gov_cname" {
-  type = "CNAME"
-  ttl = 300
-  records = [ "d11og6pgwhrztr.cloudfront.net." ]
-resource "aws_route53_record" "digitalgov_gov_openopps-staging_digitalgov_gov_txt" {
-resource "aws_route53_record" "digitalgov_gov_mandrill__domainkey_openopps-staging_digitalgov_gov_txt" {
BREAKS HERE
-  replication_group_id          = "${module.label.id}"
BREAKS HERE
-<<<<<<< HEAD
-  count = "${var.enable_public_redshift == false && var.create_vpc && length(var.redshift_subnets) > 0 ? length(var.redshift_subnets) : 0}"
-=======
->>>>>>> Redshift public subnets (#222)
-<<<<<<< HEAD
-  count = "${var.enable_public_redshift && var.create_vpc && length(var.redshift_subnets) > 0 ? length(var.redshift_subnets) : 0}"
-=======
->>>>>>> Redshift public subnets (#222)
BREAKS HERE
-      values = ["arn:${local.aws_partition}:s3:::cg-*"]
BREAKS HERE
-  port     = 4180
-  protocol = "HTTP"
-  vpc_id   = "${data.aws_vpc.vpc.id}"
BREAKS HERE
-  # name                   = "${var.environment_name}-${data.aws_region.current.name}-${var.vpc}-bastion-service"
-  name_prefix            = "${var.environment_name}-${var.vpc}-bastion-service"
BREAKS HERE
-  name = "graceful_shutdown_asg"
-  default_result = "CONTINUE"
-  heartbeat_timeout = "${var.heartbeat_timeout}"
-  lifecycle_transition = "autoscaling:EC2_INSTANCE_TERMINATING"
BREAKS HERE
-  count = "${length(var.subnets_elb)}"
-  id    = "${var.subnets_elb[count.index]}"
-  from_port         = "${var.elb_healthcheck_port}"
-  to_port           = "${var.elb_healthcheck_port}"
BREAKS HERE
-  name = "${var.cluster_name}-${terraform.env}-role"
-resource "aws_iam_instamce_profile" "instance_profile" {
BREAKS HERE
-      "sudo KUBECONFIG=/etc/kubernetes/kubelet.conf kubectl annotate node $${HOSTNAME} --overwrite container-linux-update.v1.coreos.com/last-checked-time=$(date +'%s' -d 'tomorrow')",
-      "chmod +x /tmp/end.sh && sudo /tmp/end.sh",
BREAKS HERE
-  bgp_asn    = "65000"                                                     # required, but I don't think it's used with a static config
-  type       = "ipsec.1"
-  type                = "ipsec.1"
-  static_routes_only  = true
BREAKS HERE
-  logging_es_nodes = "2"
BREAKS HERE
-    value = false
-##     value = true
BREAKS HERE
-  manage_group                = "true"
BREAKS HERE
-variable "availability_zone" {
-  description = "The AZ to start the instance in"
-  default     = ""
-}
-
-  default     = ""
BREAKS HERE
-  name = "codedeploy_s3bucket_access"
BREAKS HERE
-  value       = "${aws_eks_cluster.this.id}"
-#   value       = "${aws_eks_cluster.this.arn}"
-  value       = "${aws_eks_cluster.this.certificate_authority.0.data}"
-  value       = "${aws_eks_cluster.this.endpoint}"
-  value       = "${aws_eks_cluster.this.version}"
BREAKS HERE
-  name_prefix     = "${var.name}.blue"
-  name_prefix     = "${var.name}.green"
BREAKS HERE
-  tags         = ["worker", "${var.env}", "com", "free", "foo"]
BREAKS HERE
-  ip_address = "${aws_instance.vyos_instance.public_ip}"
-resource "aws_vpn_connection" "main" {
-resource "null_resource" "packer_script" {
-    configuration = "${aws_vpn_connection.main.customer_gateway_configuration}"
-    command = "echo '${aws_vpn_connection.main.customer_gateway_configuration}' > vpc-1-vpn-config.vpn.xml"
-    command = "python ../tools/vyos_config.py vpc-1-vpn-config.vyatta ${aws_instance.vyos_instance.private_ip} ${module.vpc_1.vpc_cidr_block} ${cidrhost(module.vpc_1.vpc_cidr_block, 1)} > vpc-1-vpn-config.vyos"
-    command = "rm vpc-1-vpn-config.vpn.xml && vpc-1-vpn-config.vyatta"
BREAKS HERE
-resource "aws_route53_record" "18f_gov_federalist_18f_gov_cname" {
BREAKS HERE
-  # leave swarm on destroy
-  provisioner "remote-exec" {
-    inline = [
-      "sh ~/.kaabah/remove-worker.sh",
-    ]
-    when       = "destroy"
-    on_failure = "continue"
-  }
-
BREAKS HERE
-#
-#  * IAM Role to allow EKS service to manage other AWS services
-#  * EC2 Security Group to allow networking traffic with EKS cluster
-#  * EKS Cluster
-#
-  name = "terraform-eks-cluster"
-  name        = "terraform-eks-cluster"
-    Name = "terraform-eks"
BREAKS HERE
-    ${element(formatlist(", { \"Effect\" : \"Allow\", \"Action\" : [ \"s3:PutObject*\", \"s3:ListBucket\" ], \"Resource\" : [ \"%s/*\", \"%s\" ] }, { \"Effect\" : \"Allow\", \"Action\" : [ \"s3:ListAllMyBuckets\" ], \"Resource\" : [ \"*\" ] }", compact(list(var.s3_bucket_arn))), 0)}
BREAKS HERE
-variable "tectonic_base_domain" {
-  type        = "string"
-  description = "Base address used to access the Tectonic Console, without protocol nor trailing forward slash"
-}
-
BREAKS HERE
-  port                          = 6379
-
BREAKS HERE
-  # provisioner "file" {
-  #   content     = "${file(var.custom_requirements)}"
-  #   destination = "/tmp/requirements.txt"
-  #
-  #   connection {
-  #     type        = "ssh"
-  #     user        = "ubuntu"
-  #     private_key = "${file(var.private_key_path)}"
-  #   }
-  # }
-  # provisioner "file" {
-  #   content     = "${file(var.custom_requirements)}"
-  #   destination = "/tmp/requirements.txt"
-  #
-  #   connection {
-  #     type        = "ssh"
-  #     user        = "ubuntu"
-  #     private_key = "${file(var.private_key_path)}"
-  #   }
-  # }
-  # provisioner "file" {
-  #   content     = "${file(var.custom_requirements)}"
-  #   destination = "/tmp/requirements.txt"
-  #
-  #   connection {
-  #     type        = "ssh"
-  #     user        = "ubuntu"
-  #     private_key = "${file(var.private_key_path)}"
-  #   }
-  # }
BREAKS HERE
-  records = ["${element(var.monitoring_internal_service_names, count.index)}.blue.${data.terraform_remote_state.infra_root_dns_zones.internal_root_domain_name}"]
BREAKS HERE
-  iam_database_authentication_enabled = true
-  enabled_cloudwatch_logs_exports = ["audit", "error", "general", "slowquery"]
-  performance_insights_enabled = true
-  family      = "${local.name}"
-  family = "${local.name}"
BREAKS HERE
-  associate_public_ip_address = false
-
BREAKS HERE
-output "backend_address_pool_id" {
-  value = "${azurerm_lb_backend_address_pool.worker.id}"
-}
-
BREAKS HERE
-  base_volume_id = "${var.base_configuration[var.image]}"
BREAKS HERE
-  source      = "../modules/vsphere_janitor"
-  host_id     = "${module.macstadium_infrastructure.wjb_uuid}"
-  ssh_host    = "${module.macstadium_infrastructure.wjb_ip}"
-  ssh_user    = "${var.ssh_user}"
-  version     = "${var.vsphere_janitor_version}"
-  config_path = "${path.module}/config/vsphere-janitor-production-com"
-  env         = "production-com"
-  index       = "${var.index}"
-  source      = "../modules/vsphere_janitor"
-  host_id     = "${module.macstadium_infrastructure.wjb_uuid}"
-  ssh_host    = "${module.macstadium_infrastructure.wjb_ip}"
-  ssh_user    = "${var.ssh_user}"
-  version     = "${var.vsphere_janitor_version}"
-  config_path = "${path.module}/config/vsphere-janitor-custom-1"
-  env         = "custom-1"
-  index       = "${var.index}"
-  source      = "../modules/vsphere_janitor"
-  host_id     = "${module.macstadium_infrastructure.wjb_uuid}"
-  ssh_host    = "${module.macstadium_infrastructure.wjb_ip}"
-  ssh_user    = "${var.ssh_user}"
-  version     = "${var.vsphere_janitor_version}"
-  config_path = "${path.module}/config/vsphere-janitor-custom-2"
-  env         = "custom-2"
-  index       = "${var.index}"
-  source      = "../modules/vsphere_janitor"
-  host_id     = "${module.macstadium_infrastructure.wjb_uuid}"
-  ssh_host    = "${module.macstadium_infrastructure.wjb_ip}"
-  ssh_user    = "${var.ssh_user}"
-  version     = "${var.vsphere_janitor_version}"
-  config_path = "${path.module}/config/vsphere-janitor-custom-4"
-  env         = "custom-4"
-  index       = "${var.index}"
-  source      = "../modules/vsphere_janitor"
-  host_id     = "${module.macstadium_infrastructure.wjb_uuid}"
-  ssh_host    = "${module.macstadium_infrastructure.wjb_ip}"
-  ssh_user    = "${var.ssh_user}"
-  version     = "${var.vsphere_janitor_version}"
-  config_path = "${path.module}/config/vsphere-janitor-custom-5"
-  env         = "custom-5"
-  index       = "${var.index}"
-  source      = "../modules/vsphere_janitor"
-  host_id     = "${module.macstadium_infrastructure.wjb_uuid}"
-  ssh_host    = "${module.macstadium_infrastructure.wjb_ip}"
-  ssh_user    = "${var.ssh_user}"
-  version     = "${var.vsphere_janitor_version}"
-  config_path = "${path.module}/config/vsphere-janitor-custom-6"
-  env         = "custom-6"
-  index       = "${var.index}"
-  source      = "../modules/vsphere_monitor"
-  host_id     = "${module.macstadium_infrastructure.util_uuid}"
-  ssh_host    = "${module.macstadium_infrastructure.util_ip}"
-  ssh_user    = "${var.ssh_user}"
-  version     = "${var.vsphere_monitor_version}"
-  config_path = "${path.module}/config/vsphere-monitor"
-  index       = "${var.index}"
-  version                                 = "${var.collectd_vsphere_version}"
BREAKS HERE
-  user_data        = "${element(data.ct_config.controller-ignitions.*.rendered, count.index)}"
BREAKS HERE
-  haystack_index_store_instance_count = "${var.haystack_index_store_worker_instance_type}"
BREAKS HERE
-  database_port        = "5432"
BREAKS HERE
-      port_range  = "all"
-      port_range  = "all"
-      protocol              = "icmp"
-      port_range            = "all"
-      protocol              = "tcp"
-      port_range            = "all"
BREAKS HERE
-# Helps debug traefik reverse proxy headers
-# Highly recommended!
-# resource "docker_image" "headerdebug" {
-#   name          = "${data.docker_registry_image.headerdebug.name}"
-#   pull_triggers = ["${data.docker_registry_image.headerdebug.sha256_digest}"]
-# }
-
BREAKS HERE
-  vpc_security_group_ids = ["${aws_security_group.public-ssh.id}",
-    "${aws_security_group.open-egress.id}",
BREAKS HERE
-resource "aws_security_group_rule" "infra_spinnaker_self_referential_rules" {
-  type = "ingress"
-  from_port = 0
-  to_port = 65535
-  protocol = "-1"
-
-  security_group_id = "${aws_security_group.infra_spinnaker.id}"
-  self = true
-}
-
-resource "aws_security_group_rule" "infra_jenkins_new_rule" {
-  type = "ingress"
-  from_port = 80
-  to_port = 80
-  protocol = "tcp"
-
-  security_group_id = "${aws_security_group.infra_jenkins.id}"
-  self = true
-}
-
BREAKS HERE
-    security_group_id = "${aws_security_group.mcws-confluence-public-ec2-sg.id}" 
-
-    security_group_id = "${aws_security_group.mcws-confluence-public-ec2-sg.id}" 
-
-    security_group_id = "${aws_security_group.mcws-confluence-public-ec2-sg.id}" 
-
-    security_group_id = "${aws_security_group.mcws-confluence-public-ec2-sg.id}" 
-
-    security_group_id = "${aws_security_group.mcws-confluence-public-ec2-sg.id}" 
-    security_group_id = "${aws_security_group.mcws-confluence-public-ec2-sg.id}" 
-    ami               = "${lookup(var.aws_amis, var.aws_region)}"
BREAKS HERE
-  scale_up_adjustment   = "${var.autoscaling_scale_down_adjustment}"
-  scale_up_cooldown     = "${var.autoscaling_scale_down_cooldown}"
BREAKS HERE
-# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
-variable "AWS_REGION" {
-  description = "AWS Region where the Lambda function shall run"
-}
BREAKS HERE
-  name           = "swarm-worker-${count.index + 1}"
BREAKS HERE
-provider "aws" {
-  region  = "${var.region}"
-  version = "~> 1.19"
-}
-terraform {
-  backend "s3" {
-    # TODO: Choose a better name?
-    key    = "terraform.tfstate"
-    region = "us-east-1"
-  }
-}
BREAKS HERE
-  default = "v3.2.9"
BREAKS HERE
-      "salt-call --local --file-root=/root/salt/ --output=quiet state.sls default",
BREAKS HERE
-  monitoring_port = "${var.graphite_node_port}"
-  monitoring_elb_dns_name = "${module.elbs.monitoring-elb-dns_name}"
BREAKS HERE
-    "pcc_dev" = "80.146.215.90/32"
-    "thomas"  = "95.90.215.69/32"
-    "shared"  = "10.49.0.0/16"
BREAKS HERE
-{
-name = "CouchDB"
-},
-
BREAKS HERE
-  vpc_id          = "${aws_vpc.this.id}"
-  vpc_id = "${aws_vpc.this.id}"
-  vpc_id = "${aws_vpc.this.id}"
-  vpc_id = "${aws_vpc.this.id}"
-  vpc_id = "${aws_vpc.this.id}"
-  vpc_id = "${aws_vpc.this.id}"
-  vpc_id = "${aws_vpc.this.id}"
-  vpc_id = "${aws_vpc.this.id}"
-  vpc_id                  = "${aws_vpc.this.id}"
-  vpc_id            = "${aws_vpc.this.id}"
-  vpc_id            = "${aws_vpc.this.id}"
-  vpc_id            = "${aws_vpc.this.id}"
-  vpc_id            = "${aws_vpc.this.id}"
-  vpc_id            = "${aws_vpc.this.id}"
-  vpc_id       = "${aws_vpc.this.id}"
-  vpc_id       = "${aws_vpc.this.id}"
-  vpc_id = "${aws_vpc.this.id}"
-  vpc_id         = "${aws_vpc.this.id}"
BREAKS HERE
-
-variable "uploads" {
-  description = "uploads"
-  default     = []
-}
BREAKS HERE
-    source      = "${var.db_records_folder}/"
BREAKS HERE
-            "export MASTER_PRIVATE_IP=\"${digitalocean_droplet.k8s_master.ipv4_address_private}\"",
-            "export MASTER_PUBLIC_IP=\"${digitalocean_droplet.k8s_master.ipv4_address}\"",
-            sed -i '.bak' "s/${digitalocean_droplet.k8s_master.ipv4_address_private}/${digitalocean_droplet.k8s_master.ipv4_address}/" ${path.module}/secrets/admin.conf
-
-            "export NODE_PRIVATE_IP=\"${digitalocean_droplet.k8s_worker.ipv4_address}\"",
BREAKS HERE
-    ip_configuration            = "${local.ip_configurations["${local.ip_configuration_enabled ? "enabled" : "disabled"}"]}"
BREAKS HERE
-  stream_name = "${var.cluster-name}-spans"
BREAKS HERE
-  count = "${var.jumpbox ? 1 : 0}"
-
-  jumpbox_public_key    = "${format("ubuntu:%s", module.ops_manager.ops_manager_ssh_public_key)}"
-  pcf_network_name      = "${module.infra.network}"
BREAKS HERE
-variable "pki-s3-bucket-arn" {}
BREAKS HERE
-  ssl_policy        = "${var.listener_ssl_policy}"
-  certificate_arn   = "${length(var.listener_certificate_domain_name) > 0 ? data.aws_acm_certificate.cert.arn : ""}"
BREAKS HERE
-# Private Subnet ===============================================================
-
-resource "aws_route_table" "private_route_table" {
-  count  = "${length(var.availability_zones)}"
-  vpc_id = "${aws_vpc.vpc.id}"
-
-  route {
-    cidr_block     = "0.0.0.0/0"
-    nat_gateway_id = "${aws_nat_gateway.nat.id}"
-  }
-}
-
-  route_table_id = "${element(aws_route_table.private_route_table.*.id, count.index)}"
-# Public Subnet ===============================================================
-
BREAKS HERE
-[CAVEAT] Tags MUST NOT contain reserved characters '<,>,%,&,\,?,/' or control characters.
BREAKS HERE
-  aws_default_region = "${var.aws_default_region}"
-  account_id = "${var.account_id}"
BREAKS HERE
-  count                = "${local.assume_role_no}"
-  name_prefix          = "bastion-service-host"
-  image_id             = "${data.aws_ami.debian.id}"
-  instance_type        = "${var.bastion_instance_type}"
-  iam_instance_profile = "${aws_iam_instance_profile.bastion_service_profile.arn}"
-
-  iam_instance_profile = "${element(
-    coalesce("${aws_iam_instance_profile.bastion_service_profile.arn}",
-    "${aws_iam_instance_profile.bastion_service_assume_role_profile.arn}"),
-    0)}"
-
BREAKS HERE
-  count      = "${var.generate_ssh_key == "true" ? 1 : 0}"
-    command = "chmod 600 ${local.private_key_filename}"
BREAKS HERE
-  public_key      = "${file("${path.root}/../temp_key.pub")}"
BREAKS HERE
-variable "name" {
-  description = "The prefix for names of network resources"
BREAKS HERE
-variable "count" {
BREAKS HERE
-    name = "everykidinapark.gov."
-    zone_id = "Z1NMJ49ZY4HDU7"
BREAKS HERE
-
-resource "azurerm_role_assignment" "user01" {
-  scope                = "${azurerm_resource_group.shared.id}"
-  role_definition_name = "Azure Kubernetes Service Cluster User Role"
-  principal_id         = "${var.k8sbook_aad_userid_1}"
-}
BREAKS HERE
- 
-    name          = "public"
-    ip_cidr_range = "10.64.0.0/22"
-    name          = "internal"
-    ip_cidr_range = "10.64.4.0/22"
- 
-    name = "internal-any-any-access"
-    name = "adminrouter-firewall"
-    name = "ssh"
BREAKS HERE
-resource "aws_route_table_association" "app" {
-  count          = "${var.amount_app_subnets}"
-  subnet_id      = "${element(module.app_subnets.ids, count.index)}"
-resource "aws_route_table_association" "db" {
-  count          = "${var.amount_db_subnets}"
-  subnet_id      = "${element(module.db_subnets.ids, count.index)}"
BREAKS HERE
-  source     = "git::https://github.com/cloudposse/tf_label.git?ref=tags/0.2.0"
-  namespace  = "${var.namespace}"
-  stage      = "${var.stage}"
-  name       = "public"
-  source     = "git::https://github.com/cloudposse/tf_label.git?ref=tags/0.2.0"
-  tags              = {
-    "Name"          = "${module.public_subnet_label.id}${var.delimiter}${replace(element(var.availability_zones, count.index),"-",var.delimiter)}"
-    "Stage"         = "${module.public_subnet_label.stage}"
-    "Namespace"     = "${module.public_subnet_label.namespace}"
BREAKS HERE
-  count = "${module.master_config.count * module.selfhosted_etcd.if_not_active * signum(module.bootkube.if_active + module.recover_cluster.if_active) * module.setup_etcd.if_active}"
BREAKS HERE
-/**
- * Copyright 2018 Google LLC
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-variable "org_id" {}
-
-variable "folder_id" {
-  default = ""
-}
-
-variable "domain" {}
-
-variable "usage_bucket_name" {
-  default = ""
-}
-
-variable "usage_bucket_prefix" {
-  default = ""
-}
-
-variable "billing_account" {}
-
-variable "group_name" {
-  default = ""
-}
-
-variable "create_group" {
-  default = "false"
-}
-
-variable "group_role" {
-  default = "roles/viewer"
-}
-
-variable "shared_vpc" {
-  default = ""
-}
-
-variable "sa_role" {
-  default = "roles/editor"
-}
-
-variable "sa_group" {
-  default = ""
-}
-
-variable "region" {
-  default = "us-east4"
-}
-
-variable "gsuite_admin_account" {}
-
-variable "random_string_for_testing" {
-  description = "A random string of characters to be appended to resource names to ensure uniqueness"
-}
BREAKS HERE
-provider "aws" {
-  profile = "${var.profile}"
-  region  = "${var.region}"
-}
-      values = ["Ubuntu16.04"]
-  name             = "patch-baseline-CentOS6"
-  description      = "Patch Baseline for CentOS6 Operating System"
-      values = ["CentOS6.5","CentOS6.6","CentOS6.7","CentOS6.8","CentOS6.9"]
-#This is done for Ubuntu. Depending on OS type, available filters and parameters change
-  name             = "patch-baseline-Redhat6"
-  description      = "Patch Baseline for Redhat6 Operating System"
-      values = ["RedhatEnterpriseLinux6.5","RedhatEnterpriseLinux6.6", "RedhatEnterpriseLinux6.7","RedhatEnterpriseLinux6.8","RedhatEnterpriseLinux6.9"]
BREAKS HERE
-  count = "${(1 - var.create_sns_topic) * var.enable}"
-  count = "${var.create_sns_topic * var.enable}"
-  sns_topic_arn      = "${element(compact(concat(aws_sns_topic.this.*.arn, data.aws_sns_topic.this.*.arn, list(var.enable))), 0)}"
-  function_name_base = "${format("%s-%s", var.lambda_function_name, var.sns_topic_name)}"
-  count = "${var.enable}"
-  count = "${var.enable}"
-  count = "${var.enable}"
-  count = "${var.enable}"
-  function_name = "${substr(local.function_name_base, 0, length(local.function_name_base) > 64 ? 64 : length(local.function_name_base))}"
BREAKS HERE
-    "libvirt_volume.centos7_module",
-    "libvirt_volume.opensuse423_module",
-    "libvirt_volume.sles15_module",
-    "libvirt_volume.sles11sp4_module",
-    "libvirt_volume.sles12_module",
-    "libvirt_volume.sles12sp1_module",
-    "libvirt_volume.sles12sp2_module",
-    "libvirt_volume.sles12sp3_module",
-    "libvirt_volume.sles-es7_module"
BREAKS HERE
-    name   = "description"
-    values = ["ELB ${aws_lb.this.arn_suffix}"]
-  }
-
-  filter = {
-  load_balancing_properties = {
-    route53_zone_id      = "${aws_route53_zone.this.zone_id}"
-    route53_name         = "service-web"
-    lb_vpc_id            = "${data.aws_vpc.selected.id}"
-    target_group_port    = "${var.echo_port}"
-    nlb_listener_port    = "${var.echo_port}"
-    deregistration_delay = 0
-  }
BREAKS HERE
-# Asume Role Policy for the ECS Task
-data "aws_iam_policy_document" "ecs_task_asume_role" {
-  assume_role_policy = "${data.aws_iam_policy_document.ecs_task_asume_role.json}"
-  assume_role_policy = "${data.aws_iam_policy_document.ecs_task_asume_role.json}"
BREAKS HERE
-  value = "identity.${module.domain_name.value}"
-  value = "${formatlist("identity.%s",module.domain_names.value)}"
BREAKS HERE
-# - This Firewall Rule may be redundant depnding on the settings of your VPC Network, but if your Network is locked down,
-# - This Firewall Rule may be redundant depnding on the settings of your VPC Network, but if your Network is locked down,
BREAKS HERE
-  location                  = "${var.tectonic_azure_location}"
-  resource_group_name       = "${module.resource_group.name}"
-  tectonic_cluster_name     = "${var.tectonic_cluster_name}"
-  vnet_cidr_block           = "${var.tectonic_azure_vnet_cidr_block}"
BREAKS HERE
-  default     = "consul-nomad-prototype"
-  default     = "consul-nomad-prototype"
BREAKS HERE
-  evaluation_periods  = "1"
-  period    = "${var.period}"
BREAKS HERE
-  create_dns_zone = "${var.tectonic_azure_create_dns_zone}"
BREAKS HERE
-  container_definitions = "${var.custom_container_definitions == "" ? data.template_file.container_definitions.rendered : var.custom_container_definitions}"
-//resource "aws_iam_role_policy" "ecs_task_access_secrets" {
-//  count = "${var.atlantis_github_user_token != "" || var.atlantis_gitlab_user_token != "" ? 1 : 0}"
-//
-//  role       = "${aws_iam_role.ecs_task_execution.id}"
-//  policy = <<EOF
-//{
-//  "Version": "2012-10-17",
-//  "Statement": [
-//    {
-//      "Effect": "Allow",
-//      "Action": [
-//        "ssm:GetParameters",
-//        "secretsmanager:GetSecretValue"
-//      ],
-//      "Resource": [
-//        "arn:aws:ssm:::parameter/*",
-//        "arn:aws:secretsmanager:::secret:*",
-//        "arn:aws:kms:region:aws_account_id:key:key_id"
-//      ]
-//    }
-//  ]
-//}
-//EOF
-//}
-
-data "template_file" "container_definitions" {
-  template = "${file("${path.module}/atlantis-task.json")}"
-
-  vars {
-    name                       = "${var.name}"
-    atlantis_image             = "${local.atlantis_image}"
-    logs_group                 = "${aws_cloudwatch_log_group.atlantis.name}"
-    logs_region                = "${data.aws_region.current.name}"
-    logs_stream_prefix         = "ecs"
-    ATLANTIS_ALLOW_REPO_CONFIG = "${var.allow_repo_config}"
-    ATLANTIS_LOG_LEVEL         = "debug"
-    ATLANTIS_PORT              = "${var.atlantis_port}"
-    ATLANTIS_ATLANTIS_URL      = "${local.atlantis_url}"
-    ATLANTIS_REPO_WHITELIST    = "${join(",", var.atlantis_repo_whitelist)}"
-
-    # When secrets will be supported in ECS Fargate use values from comment field
-    # Ref: https://github.com/aws/amazon-ecs-agent/issues/1209
-    ATLANTIS_GH_USER = "${var.atlantis_github_user}"
-
-    ATLANTIS_GH_TOKEN          = "${element(concat(aws_ssm_parameter.atlantis_github_user_token.*.value, list("")), 0)}" # "${var.atlantis_github_user_token_ssm_parameter_name}"
-    ATLANTIS_GH_WEBHOOK_SECRET = "${aws_ssm_parameter.webhook.value}"                                                    # "${var.webhook_ssm_parameter_name}"
-
-    ATLANTIS_GITLAB_USER           = "${var.atlantis_gitlab_user}"
-    ATLANTIS_GITLAB_TOKEN          = "${element(concat(aws_ssm_parameter.atlantis_gitlab_user_token.*.value, list("")), 0)}" # "${var.atlantis_gitlab_user_token_ssm_parameter_name}"
-    ATLANTIS_GITLAB_WEBHOOK_SECRET = "${aws_ssm_parameter.webhook.value}"                                                    # "${var.webhook_ssm_parameter_name}"
BREAKS HERE
-  role_name        = "ServiceRoleFor${title(element(split(".", var.aws_service), 0))_${join("-", split(" ", lower(var.role_purpose)))}}"
-  role_path        = "/service-role/${var.aws_service}/"
-  role_description = "${var.role_description}"
-  policy_document  = "${data.aws_iam_policy_document.services.json}"
BREAKS HERE
-use_unreleased_updates: ${var.use_unreleased_updates}
BREAKS HERE
-  # output the lists formated
-    command = "echo \"${join("\n",formatlist("%s ansible_ssh_host=%s ansible_ssh_user=ubuntu", var.edge_hostnames, var.edge_public_ip))}\" >> inventory"
-  # only output if master is edge
-    command = "echo \"${var.master_as_edge != true ? "" : join("\n",formatlist("%s ansible_ssh_host=%s ansible_ssh_user=ubuntu", var.master_hostnames, var.master_public_ip))}\" >> inventory"
-    command = "echo \"nodes_count=${1 + var.edge_count + var.node_count} \" >> inventory"
BREAKS HERE
-    console_base_address = "https://${var.domain}"
-    console_callback = "https://${var.domain}/auth/callback"
-    oidc_issuer_url = "https://${var.domain}/identity"
BREAKS HERE
-  private-subnet-id   = "${ module.vnet.controller-subnet-id }"
BREAKS HERE
-  source    = "git::https://github.com/cloudposse/terraform-null-label.git?ref=tags/0.2.1"
-  source     = "git::https://github.com/cloudposse/terraform-null-label.git?ref=tags/0.2.1"
BREAKS HERE
-  value = "${aws_instance.test_1_u1404.private_ip}"
BREAKS HERE
-resource "azurerm_network_interface" "node" {
-  name                = "k8snode${ count.index + 1 }"
-  location            = "${ var.location }"
-  resource_group_name = "${ var.resource_group_name }"
-
-  count = "${ var.node_count }"
-
-  ip_configuration {
-    name                          = "private"
-    subnet_id                     = "${ var.private-subnet-id }"
-    private_ip_address_allocation = "dynamic"
-  }
-}
-
-  name                  = "k8snode${ count.index + 1 }"
-      "sudo /bin/bash -eux /home/ubuntu/prepare_node.sh",
BREAKS HERE
-  name_prefix = "default-"
-}
BREAKS HERE
-    efs_mount_id    = "${var.efs_mount_id}"
BREAKS HERE
-variable "create_log_bucket" {
-  description = "Create the S3 bucket (named with the log_bucket_name var) and attach a policy to allow ALB logging."
-  default     = false
-}
-
-variable "enable_logging" {
-  default     = false
-  description = "Enable the ALB to write log entries to S3."
-}
-
BREAKS HERE
-    value = "30min"
-##     value = "30min"
BREAKS HERE
-  default     = "compute"
-  default = "static"
BREAKS HERE
-  name                = "tf${substr(md5(random_id.cluster.id),0,4)}exhibitor"
-  resource_group_name = "${azurerm_resource_group.dcos.name}"
-  location            = "${azurerm_resource_group.dcos.location}"
-  account_type        = "Standard_LRS"
BREAKS HERE
-resource "null_resource" "etcd_secrets" {
-  count = "${var.tectonic_experimental ? 0 : var.tectonic_etcd_count }"
-
-  connection {
-    type    = "ssh"
-    host    = "${element(module.etcd.ip_address, count.index)}"
-    user    = "core"
-    timeout = "60m"
-  }
-
-  provisioner "file" {
-    content     = "${module.bootkube.etcd_ca_crt_pem}"
-    destination = "$HOME/etcd_ca.crt"
-  }
-
-  provisioner "file" {
-    content     = "${module.bootkube.etcd_server_crt_pem}"
-    destination = "$HOME/etcd_server.crt"
-  }
-
-  provisioner "file" {
-    content     = "${module.bootkube.etcd_server_key_pem}"
-    destination = "$HOME/etcd_server.key"
-  }
-
-  provisioner "file" {
-    content     = "${module.bootkube.etcd_peer_crt_pem}"
-    destination = "$HOME/etcd_peer.crt"
-  }
-
-  provisioner "file" {
-    content     = "${module.bootkube.etcd_peer_key_pem}"
-    destination = "$HOME/etcd_peer.key"
-  }
-
-  provisioner "remote-exec" {
-    inline = [
-      "sudo mkdir -p /etc/ssl/etcd",
-      "sudo mv /home/core/etcd_ca.crt /etc/ssl/etcd/ca.crt",
-      "sudo mv /home/core/etcd_server.crt /etc/ssl/etcd/server.crt",
-      "sudo mv /home/core/etcd_server.key /etc/ssl/etcd/server.key",
-      "sudo mv /home/core/etcd_peer.key /etc/ssl/etcd/peer.key",
-      "sudo mv /home/core/etcd_peer.crt /etc/ssl/etcd/peer.crt",
-    ]
-  }
-}
-
-  # Without depends_on, this remote-exec may start before the kubeconfig copy.  # Terraform only does one task at a time, so it would try to bootstrap  # Kubernetes and Tectonic while no Kubelets are running. Ensure all nodes  # receive a kubeconfig before proceeding with bootkube and tectonic.  #depends_on = ["null_resource.kubeconfig-masters"]
-
BREAKS HERE
-      "mkdir -p kaabah",
-      "set -a && . ./.bash_profile && set +a",
-  provisioner "remote-exec" {
-    inline = [
-      "sudo docker stack rm kaabah",
-      "rm -fr kaabah"
-    ]
-    when       = "destroy"
-    on_failure = "continue"
-  }
-
BREAKS HERE
-    command = "${local.command_when_destroy_chomped == "" ? ":" : local.command_chomped}"
BREAKS HERE
-  roles = ["${aws_iam_role.iam_role.name}"]
BREAKS HERE
-
-variable "allow_rdp_traffic" {
-  description = "This optional variable, when set to true, adds a security rule allowing RDP traffic to flow through to the newly created network. The default value is false."
-  default     = false
-}
-
-variable "allow_ssh_traffic" {
-  description = "This optional variable, when set to true, adds a security rule allowing SSH traffic to flow through to the newly created network. The default value is false."
-  default     = false
-}
-
-variable "sg_name" {
-  description = "Give a name to security group"
-  default     = "acctsecgrp"
-}
BREAKS HERE
-  name                   = "${var.environment_name}-${data.aws_region.current.name}-${var.vpc}-bastion-service"
BREAKS HERE
-    description = "The tier under which the service plan is created. Details can be found at https://docs.microsoft.com/en-us/azure/app-service/overview-hosting-plans"
-    default     = "Isolated"
-    description = "The compute and storage needed for the service plan to be deployed. Details can be found at https://azure.microsoft.com/en-us/pricing/details/app-service/windows/"
-    default     = "S1"
-    description = "The kind of Service Plan to be created. Possible values are Windows/Linux/FunctionApp/App"
-    default     = "Linux"
BREAKS HERE
-  enable_nat_gateway  = true
BREAKS HERE
-    command = <<EOF
-    mkdir lambda && mkdir tmp
-    cp ebs_bckup/ebs_bckup.py tmp/ebs_bckup.py
-    echo "${data.template_file.vars.rendered}" > tmp/vars.ini
-EOF
BREAKS HERE
-  image_id      = "${var.vault_consul_ami == "" ? data.aws_ami.vault_consul.id : var.vault_consul_ami}"
BREAKS HERE
-  subnet_group_name  = "${aws_elasticache_subnet_group.redis_cluster_subnet_group.name}"
-  security_group_ids = ["${var.security_group_ids}"]
BREAKS HERE
-variable "instance_type" {}
-    instance_type        = "${var.instance_type}"
BREAKS HERE
-resource "libvirt_volume" "opensuse422" {
-  name = "${var.name_prefix}opensuse422"
-  source = "http://download.opensuse.org/repositories/home:/SilvioMoioli:/Terraform:/Images/images/opensuse422.x86_64.qcow2"
-  pool = "${var.pool}"
-}
-
-resource "libvirt_volume" "sles11sp3" {
-  name = "${var.name_prefix}sles11sp3"
-  source = "http://download.suse.de/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles11sp3.x86_64.qcow2"
-  pool = "${var.pool}"
-}
-
-resource "libvirt_volume" "sles11sp4" {
-  name = "${var.name_prefix}sles11sp4"
-  source = "http://download.suse.de/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles11sp4.x86_64.qcow2"
-  pool = "${var.pool}"
-}
-
-resource "libvirt_volume" "sles12" {
-  name = "${var.name_prefix}sles12"
-  source = "http://download.suse.de/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12.x86_64.qcow2"
-  pool = "${var.pool}"
-}
-
-resource "libvirt_volume" "sles12sp1" {
-  name = "${var.name_prefix}sles12sp1"
-  source = "http://download.suse.de/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp1.x86_64.qcow2"
-  pool = "${var.pool}"
-}
-
-resource "libvirt_volume" "sles12sp2" {
-  name = "${var.name_prefix}sles12sp2"
-  source = "http://download.suse.de/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp2.x86_64.qcow2"
-  pool = "${var.pool}"
-}
-
-resource "libvirt_volume" "centos7" {
-  name = "${var.name_prefix}centos7"
-  source = "http://w3.nue.suse.com/~smoioli/sumaform-images/centos7_v2.qcow2"
BREAKS HERE
-  source = "https://w3.nue.suse.com/~smoioli/sumaform-images/centos7_v2.qcow2"
BREAKS HERE
-    "scaleway_server.swarm_worker", # Ensure dependency to the workers see https://github.com/kalisio/kaabah/issues/102
BREAKS HERE
-  name    = "${var.route53_fqdn == "" ? local.bastion_host_name-var.service_name.var.dns_domain : var.route53_fqdn}"
BREAKS HERE
-  // Note: For ecs usage, Https outboud traffic to ssm.us-east-1.amazonaws.com and https://ecs.us-east-1.amazonaws.com must be allowed
BREAKS HERE
-  name                   = "real-time-enforcer-log-sink-${random_string.main.result}"
-  org_id                 = "${var.org_id}"
-  destination            = "pubsub.googleapis.com/projects/${var.pubsub_project_id}/topics/${google_pubsub_topic.main.name}"
BREAKS HERE
-    env_vars= "${indent(9,"${var.env_vars}")}"
BREAKS HERE
-// The 'writer' endpoint for the cluster
-output "cluster_endpoint" {
-  value = "${aws_rds_cluster.default.endpoint}"
-// Comma separated list of all DB instance endpoints running in cluster
-output "all_instance_endpoints_list" {
-  value = ["${aws_rds_cluster_instance.cluster_instance_0.endpoint}", "${aws_rds_cluster_instance.cluster_instance_n.*.endpoint}"]
-// A read-only endpoint for the Aurora cluster, automatically load-balanced across replicas
-output "reader_endpoint" {
-  value = "${aws_rds_cluster.default.reader_endpoint}"
-output "password" {
-  value = "${aws_rds_cluster.default.master_password}"
BREAKS HERE
-      "mkdir ~/.kaabah",
-    destination = "~/.kaabah/ca.cert"
-    destination = "~/.kaabah/ca.key"
-    destination = "~/.kaabah/ca.pass"
-    destination = "~/.kaabah"
-      "sh ~/.kaabah/install-worker.sh ${var.docker_version} ${scaleway_server.manager.private_ip} \"10.0.0.0/8\"",
BREAKS HERE
-  type    = "string"
-  default = "East US"
BREAKS HERE
-    DBInstanceIdentifier = "${aws_db_instance.postgresql.name}"
-    DBInstanceIdentifier = "${aws_db_instance.postgresql.name}"
-    DBInstanceIdentifier = "${aws_db_instance.postgresql.name}"
-    DBInstanceIdentifier = "${aws_db_instance.postgresql.name}"
BREAKS HERE
-  name              = "/ecs/quorum"
BREAKS HERE
-  vpc_zone_identifier = ["${data.terraform_remote_state.vpc.public_subnet_ids}"]
BREAKS HERE
-variable vm_lun {
BREAKS HERE
-  namespace = k8s_core_v1_namespace.this.metadata.0.name
-    namespace = var.namespace
BREAKS HERE
-  default     = "1.7.4"
-  default     = "17.06.0"
-  default     = "2.4"
BREAKS HERE
-  depends_on = ["aws_autoscaling_group.quorum_maker", "aws_instances.quorum_maker_node"]
BREAKS HERE
-    default_ttl            = 60
-    default_ttl            = 60
-  output_path = "${path.module}/lambda-headers.zip"
BREAKS HERE
-  image = "rss-bridge/rss-bridge:latest"
-  networks = ["${data.docker_network.bridge.id}"]
BREAKS HERE
-  description = "Whether or not to give bootnodes elastic IPs, maintaining one static IP forever. Disabled by default because regions with more than 5 bootnodes will require personally requesting more EIPs from AWS. WARNING: UNTESTED SINCE MOVING BOOTNODES INTO QUORUM VPC. MAY BE REMOVED IN A FUTURE UPDATE."
BREAKS HERE
-  role_assume_policy         = "${var.lambda_type != true ? data.aws_iam_policy_document.edge.json : data.aws_iam_policy_document.this.json}"
BREAKS HERE
-    template = "${file("${artifacts_dir}/cloud-config.yaml.tmpl")}"
-    template = "${file("${artifacts_dir}/upload-templates/envvars")}"
BREAKS HERE
-  default = "acctvnet"
-  default = "myapp-rg"
-  type = "map"
-  default = false  
-  default = false  
-
-  default = "acctsecgrp"
-}
BREAKS HERE
-  name = "${var.shared["env"]}-smoke.${data.aws_route53_zone.reticulum-zone.name}"
-  name = "assets-smoke-${var.shared["env"]}.${data.aws_route53_zone.reticulum-zone.name}"
BREAKS HERE
-  source   = "../../modules/networking"
-  region   = "${var.aws_region}"
-  env_name = "${var.env_name}"
BREAKS HERE
-      ANSIBLE_HOST_KEY_CHECKING=False ansible-playbook -i ${var.ansible_inventory_path} --private-key ${var.private_key_path} ${path.module}/ansible/play.yml
BREAKS HERE
-  aws_nodes_subnet_ids = "${var.aws_nodes_subnet}"
-  aws_zone = "${var.aws_zone}"
-  aws_utility_subnet_ids = "${var.aws_utilities_subnet}"
-  aws_region = "${var.aws_region}"
-  aws_region = "${var.aws_region}"
BREAKS HERE
-locals {
-  credentials_path = "${path.module}/${var.credentials_path_relative}"
-}
-  credentials = "${file(local.credentials_path)}"
BREAKS HERE
-  network            = "${replace(data.google_compute_network.gke_network.self_link, "https://www.googleapis.com/compute/v1/", "")}"
BREAKS HERE
-  name                 = "bastion-service-asg"
BREAKS HERE
-    resources = "${local.account_arns}"
-
-  actions = ["sts:AssumeRole"]
BREAKS HERE
-  type = "string"
-Note: This field MUST be set manually prior to creating the cluster.
-  type = "string"
-Note: This field MUST be set manually prior to creating the cluster.
BREAKS HERE
-  default     = "* */2 * * *"
BREAKS HERE
-* ## Project: infr-prometheus
-variable "elb_external_cert" {
-    zone_id                = "${aws_elb.prometheus.prometheus_external_elb.zone_id}"
BREAKS HERE
-variable "target_tags" {
-  default = "gke-dev"
BREAKS HERE
-    cidr_block     = "0.0.0.0/0"
BREAKS HERE
-    source_image = "${data.google_compute_image.my_image.self_link}"
BREAKS HERE
-
-variable "resource_tags" {
-  type        = "string"
-  description = "Map of tags to apply to taggable resources in this module.  By default the taggable resources are tagged with the name defined above and this map is merged in"
-  type        = "map"
-  default     = {}
-}
BREAKS HERE
-  k8s_cluster_name = "${var.k8s_cluster_name}"
-  k8s_cluster_name = "${var.k8s_cluster_name}"
-  k8s_cluster_name = "${var.k8s_cluster_name}"
BREAKS HERE
-  ethstats_docker_image         = "puppeth/ethstats:latest"
-  ethstats_port                 = 3000
-    name   = "name"
-    values = ["amzn2-ami-hvm-*"]
-    name   = "virtualization-type"
-    values = ["hvm"]
-  owners = ["137112412989"] # amazon
-  rsa_bits  = "2048"
-  key_name   = "${local.default_bastion_resource_name}"
-  content  = "${tls_private_key.ssh.private_key_pem}"
-  ami                         = "${data.aws_ami.this.id}"
-  instance_type               = "t2.large"
-  vpc_security_group_ids      = ["${aws_security_group.quorum.id}", "${aws_security_group.bastion.id}"]
-  subnet_id                   = "${var.bastion_public_subnet_id}"
-  key_name                    = "${aws_key_pair.ssh.key_name}"
-  iam_instance_profile        = "${aws_iam_instance_profile.bastion.name}"
-    bastion             = "${aws_instance.bastion.public_dns}"
-    script              = "${md5(local_file.bootstrap.content)}"
-      host        = "${aws_instance.bastion.public_ip}"
-      user        = "ec2-user"
-      timeout     = "10m"
BREAKS HERE
-  count = "${var.create && (var.sqs_queue_with_kms == false) ? 1 : 0}"
-  count = "${var.create && var.sqs_queue_with_kms ? 1 : 0}"
BREAKS HERE
-    name      = "${format("icp-worker-${random_id.rand.hex}-%02d", count.index+1)}"
-    name      = "icp-master-${random_id.rand.hex}"
BREAKS HERE
-  default     = ""
BREAKS HERE
- ami = "${var.ami_us_east_1_win19}"
- availability_zone = "${var.avl-zone}"
- instance_type = "${var.in_type_bld}"
- key_name = "${var.aws_key_name}"
- subnet_id = "${aws_subnet.sn_admiral_setup.id}"
- count = 1
- vpc_security_group_ids = [
-   "${aws_security_group.sg_public_admiral_setup.id}"]
- root_block_device {
-   volume_type = "gp2"
-   volume_size = 100
-   delete_on_termination = true
- }
- tags = {
-   Name = "admiral_vijayreddy1991_win19_${count.index}_${var.install_version}"
- }
- value = "${formatlist("instance %v has private ip %v", aws_instance.admiral_vijayreddy1991_win19.*.id, aws_instance.admiral_vijayreddy1991_win19.*.private_ip)}"
BREAKS HERE
-    "kubernetes.default.svc.${var.cluster_dns_fqdn}",
BREAKS HERE
-    #   # DEFAULT VALUES are derived from default_load_balancing_properties_*  #
-    # lb_listener_arn is the ALB listener arn for HTTP  # lb_listener_arn = ""
-    # lb_listener_arn_https is the ALB listener arn for HTTPS  # lb_listener_arn_https = ""
-    # lb_vpc_id is the vpc_id for the target_group to reside in  # lb_vpc_id = ""
-    # route53_zone_id is the zone to add a subdomain to  # route53_zone_id = ""
-    # health_uri is the health uri to be checked by the ALB   # health_uri = "/ping"
-    # unhealthy_threshold is the health uri to be checked by the ALB   # unhealthy_threshold = "3"
-    # Do we create listener rules for https  # https_enabled = true
-    # Do we want to create a subdomain for the service inside the Route53 zone  # create_route53_record = true
-    #   # DEFAULT VALUES are derived from default_capacity_properties_*  #
-    # desired_capacity is the desired amount of tasks for a service, when autoscaling is used desired_capacity is only used initially  # after that autoscaling determins the amount of tasks   # desired_capacity = "2"
-    # desired_min_capacity is used when autoscaling is used, it sets the minimum of tasks to be available for this service  # desired_min_capacity = "2"
-    # desired_max_capacity is used when autoscaling is used, it sets the maximum of tasks to be available for this service  # desired_max_capacity = "5"
-    # deployment_maximum_percent sets the maximum deployment size of the current capacity, 200% means double the amount of current tasks  # will be active in case a deployment is happening  # deployment_maximum_percent = "200"
-    # deployment_minimum_healthy_percent sets the minimum deployment size of the current capacity, 0% means no tasks need to be running at the moment of  # a deployment switch  # deployment_minimum_healthy_percent = "0"
BREAKS HERE
-/**
- * ## Cloud Dev Workspace on AWS
- * 
- * Run a dev workspace on a single EC2 instance.
- * This instance will be part of a single-node autoscaling group
- * that shares an EBS volume to store data.
- *
- * Note that there is a peculiarity with the EBS volume in that it
- * requires some manual setup the very first time to make it available
- * for use (unless a snapshot id is supplied):
- *
- * parted --script /dev/xvdf -- mklabel msdos
- * parted --script /dev/xvdf -- mkpart primary 0 -1
- * mkfs -t ext4 -F /dev/xvdf1
- * e2label /dev/xvdf1 data
- *
- * After running the above code to initialise the EBS, terminate the instance
- * and the autoscaling group will bring up a new instance that will be running
- * gitlab once it is done initialising.
- * 
- */
-
-variable "name_prefix" {
-  description = ""
-  type        = "string"
-}
-
-variable "region" {
-  default = "us-east-2"
-  type    = "string"
-}
-
-variable "instance_type" {
-  description = "the type of EC2 instance"
-  default     = "t2.small"
-  type        = "string"
-}
-
-variable "ssh_pubkey" {
-  description = "The path to the SSH pub key to use"
-  default     = "./id_rsa.pub"
-}
-
-variable "dns_zone_name" {
-  description = "The name of the DNS zone on Route53, to create records in for the workspace"
-  type        = "string"
-}
-
-  name_prefix         = "${var.name_prefix}-workspace"
-  cidr                = "192.168.0.0/24"
-  public_subnet_cidrs = ["192.168.0.0/24"]
-  key_name   = "${var.name_prefix}"
-  name_prefix = "${var.name_prefix}-workspace-data"
-resource "aws_security_group" "workspace" {
-  name        = "${var.name_prefix}-cloud-workspace"
-  description = "Security group for the cloud workspace for ${var.name_prefix}"
-  security_group_id = "${aws_security_group.workspace.id}"
-}
-
-module "sandstorm-rule" {
-  source            = "../../modules/single-port-sg"
-  port              = 6080
-  description       = "Allow ingress for sandcats HTTP, port 6080 (TCP)"
-  cidr_blocks       = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.workspace.id}"
-}
-
-module "https-rule" {
-  source            = "../../modules/single-port-sg"
-  port              = 443
-  description       = "Allow ingress for HTTPS, port 443 (TCP)"
-  cidr_blocks       = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.workspace.id}"
-  security_group_id = "${aws_security_group.workspace.id}"
-  security_groups = ["${aws_security_group.workspace.id}"]
-  iam_instance_profile        = "${module.workspace-data.iam_profile}"
-  tags = {
-    Name = "${var.name_prefix}-workspace"
-  }
-
-# install saltstack and bootstrap CM formula
-wget -O - https://raw.githubusercontent.com/fpco/bootstrap-salt-formula/master/simple-bootstrap.sh | sh
-
-#####
-mkdir /home/ubuntu/bin
-wget https://releases.hashicorp.com/terraform/0.10.2/terraform_0.10.2_linux_amd64.zip
-wget https://releases.hashicorp.com/terraform/0.11.1/terraform_0.11.1_linux_amd64.zip
-
-unzip terraform_0.11.1_linux_amd64.zip -d /home/ubuntu/bin/terraform-0.11.1
-unzip terraform_0.10.2_linux_amd64.zip -d /home/ubuntu/bin/terraform-0.10.2
-
-mkdir /data/sandstorm
-ln -sf /data/sandstorm /opt/sandstorm
-echo "install sandstorm with: curl https://install.sandstorm.io | bash"
-
-##################
-## Outputs
-
-// region deployed to
-output "region" {
-  value = "${var.region}"
-}
-
-// name of the Gitlab autoscaling group
-output "workspace_dns" {
-  value = "${aws_route53_record.workspace.name}"
-}
BREAKS HERE
-    asia-south {
BREAKS HERE
-      rule_no = 5000
-      from_port = 0
-      to_port = 1
BREAKS HERE
-        "sudo kubectl apply -f kube-flannel-rbac.yml",
-        "sudo kubectl apply -f kube-flannel.yml"
BREAKS HERE
-  subnet_id              = "${var.etcd_subnets[count.index]}"
BREAKS HERE
-    command = "mkdir -p ${path.module}/lambda && mkdir -p  ${path.module}/tmp && cp  ${path.module}/ebs_bckup/ebs_bckup.py  ${path.module}/tmp/ebs_bckup.py && echo ${data.template_file.vars.rendered} >  ${path.module}/tmp/vars.ini"
-  output_path = "lambda/${var.stack_prefix}-${var.unique_name}.zip"
-  filename          = "lambda/${var.stack_prefix}-${var.unique_name}.zip"
BREAKS HERE
-  name    = "${google_compute_instance_group_manager.default.name}"
BREAKS HERE
-  key_path = "${file(${var.key_path})}"
-  user_data = "${file("bootstrap.sh")}" 
BREAKS HERE
-  labels {
-    "traefik.port"                                     = 80
-    "traefik.frontend.passHostHeader"                  = "false"
-    "traefik.enable"                                   = "true"
-    "traefik.frontend.headers.SSLTemporaryRedirect"    = "true"
-    "traefik.frontend.headers.STSIncludeSubdomains"    = "false"
-    "traefik.frontend.headers.contentTypeNosniff"      = "true"
-    "traefik.frontend.headers.browserXSSFilter"        = "true"
-    "traefik.frontend.headers.STSSeconds"              = "2592000"
-    "traefik.frontend.headers.customFrameOptionsValue" = "${var.xfo_allow}"
-    "traefik.frontend.headers.customResponseHeaders"   = "${var.xpoweredby}"
-    "traefik.frontend.rule"                            = "Host:pics.${var.domain},pics.in.${var.domain}"
-  }
BREAKS HERE
-  tags = "${merge(var.tags, map("Name", format("%s-%d", var.name, count.index+1)))}"
BREAKS HERE
-  network_name    = "${var.name}"
-  name                = "${var.name}"
BREAKS HERE
-    name = "master"
-        "sudo apt-get update && sudo apt-get install -y kubelet=${var.k8s_version}-00 kubeadm=${var.k8s_version}-00 kubectl=${var.k8s_version}-00 kubernetes-cni vim git",
-        "kubeadm init --kubernetes-version v${var.k8s_version} --apiserver-advertise-address ${self.ipv4_address_private} --pod-network-cidr 10.244.0.0/16 --token ${var.k8s_token}",
-    name = "minion-${count.index + 1}"
-        "sudo apt-get update && sudo apt-get install -y kubelet=${var.k8s_version}-00 kubeadm=${var.k8s_version}-00 kubectl=${var.k8s_version}-00 kubernetes-cni vim git",
BREAKS HERE
-  user_data                   = "${data.ignition_config.main.rendered}"
BREAKS HERE
-  source    = "git::https://github.com/cloudposse/tf_label.git?ref=tags/0.2.0"
-
-  tags        = "${module.label.tags}"
-
-
-
-  source    = "git::https://github.com/cloudposse/tf_hostname.git?ref=tags/0.1.0"
-  source    = "git::https://github.com/cloudposse/tf_hostname.git?ref=tags/0.1.0"
BREAKS HERE
-  name_filter  = "4.10.8 std #1"
BREAKS HERE
-      "while fuser /var/lib/dpkg/lock >/dev/null 2>&1; do sleep 1; done",
BREAKS HERE
-  value = "${tokend_signer_rule.kyc_recovery_creator}"
BREAKS HERE
-
-module "kops_state" {
-    source = "../modules/kops_state"
-
-    cluster_name = "${local.cluster_name}"
-}
BREAKS HERE
-data "aws_ebs_snapshot" "data_disk_snapshot" {
-  most_recent = true
-
-  filter {
-    name   = "tag:Name"
-    values = ["${var.name_prefix}-mirror-data-volume-snapshot"]
-  }
-}
-
-    snapshot_id = "${var.data_volume_snapshot_id == "auto" ? data.aws_ebs_snapshot.data_disk_snapshot.id : var.data_volume_snapshot_id}"
BREAKS HERE
-  cidr_block = "10.0.${ count.index }.0/24"
BREAKS HERE
-      binary_log_enabled = true
BREAKS HERE
-    hostname              = "${element(var.hostnames, count.index)}"
-    intial_cluster        = "${join(",", formatlist("%s=http://%s:2380", var.hostnames, var.vpn_ips))}"
-    listen_client_urls    = "http://${element(var.vpn_ips, count.index)}:2379"
-    advertise_client_urls = "http://${element(var.vpn_ips, count.index)}:2379"
-    listen_peer_urls      = "http://${element(var.vpn_ips, count.index)}:2380"
-    list = "${join(",", formatlist("http://%s:2379", var.vpn_ips))}"
BREAKS HERE
-    k8s_dns_service_ip  = "${cidrhost(var.service_cidr, 10)}"
-    k8s_etcd_service_ip = "${cidrhost(var.service_cidr, 15)}"
BREAKS HERE
- *        subnet_ids           = "1,2"
- *        availability_zones   = "a,b"
-  description = "Comma separated list of subnet IDs"
-  description = "Comma separated list of AZs"
-  availability_zones   = ["${split(",", var.availability_zones)}"]
-  vpc_zone_identifier  = ["${split(",", var.subnet_ids)}"]
BREAKS HERE
-      user        = "admin"
-      user        = "admin"
-      user        = "admin"
-      user        = "admin"
-      user        = "admin"
-      user        = "admin"
-      user        = "admin"
-      user        = "admin"
-      user        = "admin"
-      user        = "admin"
-      user        = "admin"
-      user        = "admin"
-      user        = "admin"
-      user        = "admin"
-      user        = "admin"
BREAKS HERE
-  template = "$${ip_count ? ip_count : subnet_count}"
BREAKS HERE
-  vnet = "${lookup(var.vm_info,"vnet")}"
-    substription_id_b64 = "${base64encode(module.azure.substription_id)}"
-    location = "${module.azure.location}"
BREAKS HERE
-  name_prefix = "${var.stack_item_label}-asg-"
-    Name        = "${var.stack_item_label}-asg"
BREAKS HERE
-  key_name = "${var.public_ssh_key_name}"
-  public_key = "${var.public_ssh_key}"
-  key_name = "${var.public_ssh_key_name}"
-      if [ 1 -eq ${var.provision_through_private_ip} ]
BREAKS HERE
-  ami                    = "${lookup(var.ovpn_ami_ids, var.vpc_aws_region)}"
BREAKS HERE
-
-module "flux" {
-  source = "../../common/flux"
-
-  gitops_ssh_url      = "${var.gitops_ssh_url}"
-  gitops_ssh_key      = "${var.gitops_ssh_key}"
-  flux_recreate       = "${var.flux_recreate}"
-  kubeconfig_complete = "${module.aks-gitops.kubeconfig_done}"
-  flux_clone_dir      = "${var.cluster_name}-flux"
-  gitops_path            = "${var.gitops_path}"
-}
-
-module "kubediff" {
-    source = "../../common/kubediff"
-
-    kubeconfig_complete       = "${module.aks-gitops.kubeconfig_done}"
-    gitops_ssh_url            = "${var.gitops_ssh_url}"
-}
BREAKS HERE
-  name               = "rds-enhanced-monitoring-${var.envname}"
BREAKS HERE
-  logging_es_nodes = "1"
BREAKS HERE
-  associate_public_ip_address = true
-  subnet_id = "${ element( split(",", var.subnet-ids), 0 ) }"
BREAKS HERE
-  subnet_id = "${tolist(data.aws_subnet_ids.selected.ids)[0]}"
-  availability_zone = "${tolist(var.azs)[0]}"
-  subnet_id = "${tolist(data.aws_subnet_ids.selected.ids)[0]}"
-  availability_zone = "${tolist(var.azs)[0]}"
-  availability_zone = "${tolist(var.azs)[0]}"
BREAKS HERE
-  version = "~> 2.7"
BREAKS HERE
-}
-
-resource "random_string" "suffix" {
-  length  = 8
-  special = false
-  upper   = false
-  network_name = "pf-test-int-full-${random_string.suffix.result}"
-      subnet_name   = "pf-test-subnet-01"
-    pf-test-subnet-01 = [
-        range_name    = "pf-test-subnet-01-secondary"
-  name              = "pf-ci-test-full-name-${random_string.suffix.result}"
-  project_id        = "pf-ci-test-full-id-${random_string.suffix.result}"
BREAKS HERE
-# Create a public IP
-resource "azurerm_public_ip" "myterraformpublicip" {
-  name                         = "myPublicIP-${var.instance_type}"
- location = "${var.region}"
-  resource_group_name          = "${var.resource_group_name}"
-  public_ip_address_allocation = "dynamic"
-  domain_name_label            = "salttest" # parameterize this (?)
-}
-
-# Create network interface
-resource "azurerm_network_interface" "myterraformnic" {
-  name                      = "myNIC-${var.instance_type}"
- location = "${var.region}"
-  resource_group_name       = "${var.resource_group_name}"
-  network_security_group_id = "${var.security_group_id}"
-
-  ip_configuration {
-    name                          = "myNicConfiguration--${var.instance_type}"
-    subnet_id                     = "${var.subnet_id}"
-    private_ip_address_allocation = "dynamic"
-    public_ip_address_id          = "${azurerm_public_ip.myterraformpublicip.id}"
-  }
-}
-  network_interface_ids = [
-    "${azurerm_network_interface.myterraformnic.id}"]
-output "ip" {
-  value = "${azurerm_public_ip.myterraformpublicip.fqdn}"
-}
BREAKS HERE
-  source     = "git::https://github.com/cloudposse/terraform-aws-ecr.git?ref=tags/0.2.9"
BREAKS HERE
-    "aws_alb_listener.consul_http",
BREAKS HERE
-      "Sid": "zookeeperRoute53ListZones",
-        "ec2:DescribeInstances"
-      "Resource": [
-        "*"
-      ]
-  user_data = "${data.template_file.zookeeper_user_data.rendered}"
BREAKS HERE
-      "amzn2-ami-hvm-*"]
-      "hvm"]
-    "137112412989"]
-    "${aws_security_group.bastion-ethstats.id}"]
-yum -y install docker
BREAKS HERE
-      identifiers = "ecs-tasks.amazonaws.com"
BREAKS HERE
-data "aws_vpc" "cluster_vpc" {
-  id = "${var.vpc_id}"
-}
-
-  security_groups             = ["${concat(list(aws_security_group.master_sec_group.id), var.extra_sg_ids)}"]
-resource "aws_security_group" "master_sec_group" {
-  vpc_id = "${data.aws_vpc.cluster_vpc.id}"
-
-  tags = "${merge(map(
-      "Name", "${var.cluster_name}_master_sg",
-      "KubernetesCluster", "${var.cluster_name}"
-    ), var.extra_tags)}"
-
-  ingress {
-    protocol  = -1
-    self      = true
-    from_port = 0
-    to_port   = 0
-  }
-
-  ingress {
-    protocol    = "tcp"
-    cidr_blocks = ["0.0.0.0/0"]
-    from_port   = 22
-    to_port     = 22
-  }
-
-  ingress {
-    protocol    = "tcp"
-    cidr_blocks = ["0.0.0.0/0"]
-    from_port   = 443
-    to_port     = 443
-  }
-
-  ingress {
-    protocol    = "tcp"
-    cidr_blocks = ["0.0.0.0/0"]
-    from_port   = 10255
-    to_port     = 10255
-  }
-
-  egress {
-    from_port   = 0
-    to_port     = 0
-    protocol    = "-1"
-    self        = true
-    cidr_blocks = ["0.0.0.0/0"]
-  }
-}
-
BREAKS HERE
-    centos7_image_id = "${openstack_images_image_v2.centos7_image.*.id}"
-    opensuse423_image_id = "${openstack_images_image_v2.opensuse423_image.*.id}"
-    sles15_image_id = "${openstack_images_image_v2.sles15_image.*.id}"
-    sles11sp4_image_id = "${openstack_images_image_v2.sles11sp4_image.*.id}"
-    sles12_image_id = "${openstack_images_image_v2.sles12_image.*.id}"
-    sles12sp1_image_id = "${openstack_images_image_v2.sles12sp1_image.*.id}"
-    sles12sp2_image_id = "${openstack_images_image_v2.sles12sp2_image.*.id}"
-    sles12sp3_image_id = "${openstack_images_image_v2.sles12sp3_image.*.id}"
-    sles-es7_image_id = "${openstack_images_image_v2.sles-es7_image.*.id}"
BREAKS HERE
-
-  # These two blocks below exist to provision a script that uses an AWS SDK to sign a request
-  provisioner "file" {
-    source      = "${path.module}/auth-signature-scripts"
-    destination = "/tmp"
-  }
-
-  connection {
-    type        = "ssh"
-    user        = "${var.ssh_username}"
-    private_key = "${file("${var.ssh_key_path}")}"
-  }
BREAKS HERE
-      source_addresses = ["10.0.0.0/8", "::/0"]
-      source_addresses = ["10.0.0.0/8", "::/0"]
-      source_addresses = ["10.0.0.0/8", "::/0"]
BREAKS HERE
-  value = "${ var.subnet_id != "" ? var.subnet_id : aws_subnet.created.id }"
BREAKS HERE
-  parameter_group_name = "ship-db-pg-${install_version}"
-##   parameter_group_name = "ship-db-pg-${install_version}"
BREAKS HERE
-      "arn:aws:s3:::${aws_s3_bucket.database_backups.id}/content-data-api-postgresql/*-content_data_api.gz",
-      "arn:aws:s3:::${aws_s3_bucket.database_backups.id}/content-data-api-postgresql/*-content_performance_manager.gz",
BREAKS HERE
-  name = "${format("%.32s", "${var.cluster["name"]}-monitoring-elb")}"
BREAKS HERE
-  subnet_id = "${ element( split(",", var.subnet-ids), 0 ) }"
BREAKS HERE
-  provider = "google-beta"
BREAKS HERE
-    dedicated_master_type    = "${var.instance_count >= 10 ? (var.dedicated_master_type ? var.dedicated_master_type : var.instance_type) : ""}"
BREAKS HERE
-  name        = "AWS-Demo"
-  description = "AWS-Demo"
BREAKS HERE
-  node_version_zonal          = "${var.node_version != "" && ! var.regional ? var.node_version : local.kubernetes_version_zonal}"
BREAKS HERE
-      "arn:aws:s3:::govuk-production-database-backups/content-data-api-postgresql/*-content_data_api.gz",
BREAKS HERE
-# DEPLOY A GKE PUBLIC CLUSTER IN GOOGLE CLOUD
-  # We can remove the following parameters after Yori's PR is released:
-  # https://github.com/terraform-providers/terraform-provider-helm/pull/210
-  client_key = "${pathexpand("~/.helm/key.pem")}"
-
-  client_certificate = "${pathexpand("~/.helm/cert.pem")}"
-  ca_certificate     = "${pathexpand("~/.helm/ca.pem")}"
-
-# DEPLOY A GKE REGIONAL PUBLIC CLUSTER IN GOOGLE CLOUD
-    name      = "${var.iam_user}"
-    command = "kubergrunt helm deploy --service-account default --resource-namespace default --tiller-namespace kube-system ${local.tls_algorithm_config} --tls-subject-json '${jsonencode(var.tls_subject)}' --client-tls-subject-json '${jsonencode(var.client_tls_subject)}' --helm-home ${pathexpand("~/.helm")} --tiller-version v2.11.0 --rbac-user ${var.iam_user}"
BREAKS HERE
-  enable_dns_hostname = true
-  version = "5.6.17"
BREAKS HERE
-  key          = "${var.guardduty_assets}/iplist.txt"
-  activate    = false
BREAKS HERE
-resource "aws_instance" "default" {
-  count = 2
-  user_data = "${file("bootstrap-server${count.index+1}.sh")}"
-    Name = "server${count.index+1}"
BREAKS HERE
-  value = "${aws_db_instance.main.engine}://${aws_db_instance.main.username}:${aws_db_instance.main.password}@${aws_db_instance.main.endpoint}/${aws_db_instance.main.database}"
BREAKS HERE
-  # Admin Backend
BREAKS HERE
-        command = "if ping -c 1 -W 1 $MASTER_IP; then ssh -o 'StrictHostKeyChecking no' -i $KEY_FILE USER@$MASTER_IP 'if [[ -f /tmp/icp_worker_scaler.sh ]]; then chmod a+x /tmp/icp_worker_scaler.sh; /tmp/icp_worker_scaler.sh a ${var.icp_edition} ${self.network.0.fixed_ip_v4}; fi'; fi"
BREAKS HERE
-  name                   = "${var.environment_name}-${data.aws_region.current.name}-${var.vpc}-bastion-service"
-  description            = "Bastion service"
-  revoke_rules_on_delete = true
-  vpc_id                 = "${var.vpc}"
-  tags                   = "${var.tags}"
BREAKS HERE
-  subnet_ids = ["${var.sn_ship_install}", "${var.sn_ship_backup}"]
BREAKS HERE
-  description = "Extra tags that will be added to VPC, DHCP Options, Internet Gateway, Subnets and Routing Table."
BREAKS HERE
-resource "aws_subnet" "k8s" {
-  cidr_block              = "${element(var.subnet_cidr_blocks, count.index)}"
-  map_public_ip_on_launch = "${var.use_public_subnets}"
-resource "aws_route_table_association" "k8s" {
-  route_table_id = "${element(var.route_table_ids, var.use_public_subnets == 1 ? 0 : count.index)}"
-  subnet_id      = "${element(aws_subnet.k8s.*.id, count.index)}"
BREAKS HERE
-      user        = "ubuntu"
-      host        = "${module.forseti-install-simple.forseti-server-vm-ip}"
-      private_key = "${tls_private_key.main.private_key_pem}"
-      user        = "ubuntu"
-      host        = "${module.forseti-install-simple.forseti-client-vm-ip}"
-      private_key = "${tls_private_key.main.private_key_pem}"
BREAKS HERE
-  subnet_id = "${element(data.aws_subnet_ids.selected.ids, 0)}"
-  subnet_id = "${element(data.aws_subnet_ids.selected.ids, 0)}"
-  subnet_id = "${element(data.aws_subnet_ids.selected.ids, 0)}"
BREAKS HERE
-
-  filter {
-    name   = "owner-alias"
-    values = ["amazon"]
-  }
BREAKS HERE
-  credentials             = "${aws_iam_role.api_gateway_0.arn}"
-  credentials             = "${aws_iam_role.api_gateway_0.arn}"
-  credentials             = "${aws_iam_role.api_gateway_1.arn}"
-  credentials             = "${aws_iam_role.api_gateway_1.arn}"
-  credentials             = "${aws_iam_role.api_gateway_2.arn}"
-  credentials             = "${aws_iam_role.api_gateway_2.arn}"
-  credentials             = "${aws_iam_role.api_gateway_3.arn}"
-  credentials             = "${aws_iam_role.api_gateway_3.arn}"
-  credentials             = "${aws_iam_role.api_gateway_4.arn}"
-  credentials             = "${aws_iam_role.api_gateway_4.arn}"
-  credentials             = "${aws_iam_role.api_gateway_5.arn}"
-  credentials             = "${aws_iam_role.api_gateway_5.arn}"
-  credentials             = "${aws_iam_role.api_gateway_6.arn}"
-  credentials             = "${aws_iam_role.api_gateway_6.arn}"
-  credentials             = "${aws_iam_role.api_gateway_7.arn}"
-  credentials             = "${aws_iam_role.api_gateway_7.arn}"
-  credentials             = "${aws_iam_role.api_gateway_8.arn}"
-  credentials             = "${aws_iam_role.api_gateway_8.arn}"
-  credentials             = "${aws_iam_role.api_gateway_9.arn}"
-  credentials             = "${aws_iam_role.api_gateway_9.arn}"
BREAKS HERE
-  engine_version         = "5.6.27"
BREAKS HERE
-  tags = {
-    Environment = "${terraform.workspace}"
-  }
-}
-
-resource "aws_route53_zone" "this" {
-  name = "some.zonename.com"
-}
-
-data "http" "icanhazip" {
-  url = "http://ipv4.icanhazip.com"
-}
-
-resource "aws_security_group_rule" "allow_user" {
-  type              = "ingress"
-  from_port         = "0"
-  to_port           = "65535"
-  protocol          = "tcp"
-  cidr_blocks       = ["${format("%s/%s",trimspace(data.http.icanhazip.body), "32")}"]
-  security_group_id = "${data.aws_security_group.selected.id}"
-}
-
-resource "aws_lb" "this" {
-  name               = "${terraform.workspace}-service-nlb"
-  internal           = false
-  load_balancer_type = "network"
-  subnets            = ["${data.aws_subnet.selected.id}"]
-
-  enable_deletion_protection       = false
-  enable_cross_zone_load_balancing = true
-
-  tags = {
-    Environment = "${terraform.workspace}"
-  }
-}
-
-variable "echo_port" {
-  default = "1025"
-  tags = {
-    Environment = "${terraform.workspace}"
-  }
BREAKS HERE
-  rule = "${aws_cloudwatch_event_rule.0.name}"
-  rule = "${aws_cloudwatch_event_rule.0.name}"
-  rule = "${aws_cloudwatch_event_rule.0.name}"
-  rule = "${aws_cloudwatch_event_rule.0.name}"
-  rule = "${aws_cloudwatch_event_rule.0.name}"
-  rule = "${aws_cloudwatch_event_rule.0.name}"
-  rule = "${aws_cloudwatch_event_rule.0.name}"
-  rule = "${aws_cloudwatch_event_rule.0.name}"
-  rule = "${aws_cloudwatch_event_rule.0.name}"
BREAKS HERE
-  azs         = ["${slice(data.aws_availability_zones.available.names, 0, 3)}"]
BREAKS HERE
-  count = "${length(var.floating_ips) > 0 ? 0 : var.count}"
-  count = "${length(var.floating_ips) > 0 ? 0 : var.count}"
-  depends_on = ["openstack_compute_floatingip_associate_v2.module_floating_ip_association", "openstack_compute_floatingip_associate_v2.external_floating_ip_association"]
-    host = "${element(concat(openstack_networking_floatingip_v2.floating_ip.*.address, var.floating_ips), count.index)}"
BREAKS HERE
-variable use_cloudflare {}
-variable cloudflare_domain {}
-    command = "echo \"domain=${ var.use_cloudflare == true ? format("%s", var.cloudflare_domain) : format("%s.nip.io", element(concat(var.edge_public_ip, var.master_public_ip), 0))}\" >> inventory"
BREAKS HERE
-
-  lifecycle {
-    create_before_destroy = true
-  }
-  base_instance_name = "travis-job-${element(var.warmer_pool_images, count.index)}-"
-  instance_template  = "${google_compute_instance_template.warmer_pool_org.*.self_link}"
-  name               = "warmer-pool-org-${length(var.warmer_pool_images)}"
BREAKS HERE
-    Name     = "core_public_subnet_${replace(data.aws_availability_zones.available.names[count.index], -,_)}"
-    Name     = "core_private_subnet_${replace(data.aws_availability_zones.available.names[count.index], -,_)}"
-    Name     = "core_private_route_table_${replace(data.aws_availability_zones.available.names[count.index], -,_)}"
BREAKS HERE
-data "template_file" "airsonic-properties-file" {
-  template = "${file("${path.module}/conf/airsonic.properties.tpl")}"
-  vars {
-    smtp-password = "${var.airsonic-smtp-password}"
-    # db-password   = "${var.airsonic-db-password}"
-  }
-}
BREAKS HERE
-    listen-client-urls: http://etcd${ count.index+1 }.${ var.internal-tld }:2379
-    listen-peer-urls: http://etcd${ count.index+1 }.${ var.internal-tld }:2380
BREAKS HERE
-    "${aws_security_group.public-ssh.id}",
-    "${aws_security_group.open-egress.id}",
BREAKS HERE
-  name                = "${var.cluster_name}-workers"
-  location            = "${var.location}"
-  resource_group_name = "${var.resource_group_name}"
-  managed             = true
BREAKS HERE
-    #   # DEFAULT VALUES are derived from default_load_balancing_properties_*  #  # lb_listener_arn is the ALB listener arn for HTTP  # lb_listener_arn = ""  # lb_listener_arn_https is the ALB listener arn for HTTPS  # lb_listener_arn_https = ""  # lb_vpc_id is the vpc_id for the target_group to reside in  # lb_vpc_id = ""  # route53_zone_id is the zone to add a subdomain to  # route53_zone_id = ""  # health_uri is the health uri to be checked by the ALB   # health_uri = "/ping"  # unhealthy_threshold is the health uri to be checked by the ALB   # unhealthy_threshold = "3"  # Do we create listener rules for https  # https_enabled = true  # Do we want to create a subdomain for the service inside the Route53 zone  # create_route53_record = true
-    #   # DEFAULT VALUES are derived from default_capacity_properties_*  #  # desired_capacity is the desired amount of tasks for a service, when autoscaling is used desired_capacity is only used initially  # after that autoscaling determins the amount of tasks   # desired_capacity = "2"  # desired_min_capacity is used when autoscaling is used, it sets the minimum of tasks to be available for this service  # desired_min_capacity = "2"  # desired_max_capacity is used when autoscaling is used, it sets the maximum of tasks to be available for this service  # desired_max_capacity = "5"  # deployment_maximum_percent sets the maximum deployment size of the current capacity, 200% means double the amount of current tasks  # will be active in case a deployment is happening  # deployment_maximum_percent = "200"  # deployment_minimum_healthy_percent sets the minimum deployment size of the current capacity, 0% means no tasks need to be running at the moment of  # a deployment switch  # deployment_minimum_healthy_percent = "0"
BREAKS HERE
-(optional) If set to `true`, TLS secure communication for self-provisioned etcd. will be used.
-If set, the variables `tectonic_etcd_servers`, `tectonic_etcd_client_cert_path`, and `tectonic_etcd_client_key_path` must also be set.
BREAKS HERE
-    worker_role_arn = "${join("", distinct(concat(data.template_file.launch_template_worker_role_arns.*.rendered, data.template_file.worker_role_arns.*.rendered)))}"
BREAKS HERE
-    subnet_id = "BackupLambdaAccessInternet-${aws_subnet.quorum_observer.id}"
-    subnet_id = "BackupLambdaAccessInternet-${aws_subnet.quorum_validator.id}"
-    subnet_id = "BackupLambdaAccessInternet-${aws_subnet.quorum_maker.id}"
BREAKS HERE
-  default = "10"
-  default = "10"
BREAKS HERE
-  records = ["openopps.usajobs.gov"]
BREAKS HERE
-variable "elb_certname" {
-  domain   = "${var.elb_certname}"
-    instance_protocol = "https"
BREAKS HERE
-  lambda_filename              = "${path.module}/../../lambda/RDSLogsToS3/RDSLogsToS3.zip"
-  lambda_filename              = "${path.module}/../../lambda/RDSLogsToS3/RDSLogsToS3.zip"
BREAKS HERE
-  source         = "../../modules/networking"
-  region         = "${var.aws_region}"
-  env_name       = "${var.env_name}"
-  unique_postfix = "${random_pet.unicorn.id}"
BREAKS HERE
-    aws_hostname    = "apt"
BREAKS HERE
-  count             = "${length(keys(var.listener_action))}"
-  port              = "${element(split(":", element(keys(var.listener_action), count.index)), 1)}"
-  protocol          = "${element(split(":", element(keys(var.listener_action), count.index)), 0)}"
-  ssl_policy        = "${element(split(":", element(keys(var.listener_action), count.index)), 0) == "HTTPS" ? var.listener_ssl_policy : ""}"
-  certificate_arn   = "${element(split(":", element(keys(var.listener_action), count.index)), 0) == "HTTPS" ? data.aws_acm_certificate.cert.0.arn : ""}"
-    target_group_arn = "${lookup(local.target_groups_arns, "${element(values(var.listener_action), count.index)}")}"
-  listener_arn    = "${aws_lb_listener.listener.arn}"
-  count           = "${var.listener_secondary_certificate_domain_name == "" ? 0 : 1}"
BREAKS HERE
-  # source = "git::git@github.com:gruntwork-io/terraform-google-gke.git//modules/gke-service-account?ref=v0.0.4"
BREAKS HERE
-  instance_type = "${var.in_type_core}"
BREAKS HERE
-  name_prefix = "${var.stack_name}-${var.datacenter_name}${var.unique_postfix}"
BREAKS HERE
-  logging_enabled = false
BREAKS HERE
-  --ignore-preflight-errors=KubeletVersion
BREAKS HERE
-  name    = "${lookup(var.subnets[count.index], "subnet_name")}"
-  region  = "${lookup(var.subnets[count.index], "subnet_region")}"
BREAKS HERE
-    name = "mariadb-parameters"
-  name                 = "mariadb"
-  parameter_group_name = "mariadb-parameters"
BREAKS HERE
-  }
-  approval_rule {
-    approve_after_days = 7
-    
- patch_filter {
-      key    = "PRODUCT"
-      values = ["AmazonLinux2"]
-   }
BREAKS HERE
-  count                  = "${length(compact(split(",", var.internal_subnets)))}"
-}
BREAKS HERE
-  name            = "${var.stackname}-draft-content-store-external"
-  name            = "${var.stackname}-draft-content-store-internal"
-    name                   = "${aws_elb.draft-content-store_external_elb.dns_name}"
-    zone_id                = "${aws_elb.draft-content-store_external_elb.zone_id}"
BREAKS HERE
-    etcd_initial_cluster = "${join(",", formatlist("%s=https://%s:2380", null_resource.repeat.*.triggers.name, null_resource.repeat.*.triggers.domain))}"
-    k8s_dns_service_ip   = "${cidrhost(var.service_cidr, 10)}"
BREAKS HERE
-    etcd_operator_image    = "${var.container_images["etcd_operator"]}"
-    kenc_image             = "${var.container_images["kenc"]}"
BREAKS HERE
-    command = "sleep 120"
BREAKS HERE
-resource "aws_route53_record" "18f_gov_agile_18f_gov_a" {
BREAKS HERE
-# Apache Zookeeper Elastic Network Interfaces (for the ASG).
BREAKS HERE
-variable region {
-provider google {
-  region = "${var.region}"
-  source         = "../../"
-  name           = "group-https-lb"
-  target_tags    = ["${module.mig1.target_tags}", "${module.mig2.target_tags}", "${module.mig3.target_tags}"]
-  url_map        = "${google_compute_url_map.https-content.self_link}"
-  create_url_map = false
-  ssl            = true
-  private_key    = "${tls_private_key.example.private_key_pem}"
-  certificate    = "${tls_self_signed_cert.example.cert_pem}"
-  name            = "https-content"
BREAKS HERE
-  name_prefix = "quorum-bootnode-net-${var.network_id}-node-${count.index}"
-    public_ip = "${var.use_elastic_bootnode_ips ? element(aws_eip.bootnodes.*.public_ip, count.index) : false}"
-    eip_id = "${var.use_elastic_bootnode_ips ? element(aws_eip.bootnodes.*.id, count.index) : false}"
BREAKS HERE
-resource "aws_security_group" "sg_public_bbs" {
-  name = "sg_public_bbs_${var.install_version}"
-  description = "BBS instance security group"
-  vpc_id = "${aws_vpc.vpc.id}"
-
-  ingress {
-    from_port = -1
-    to_port = -1
-    protocol = "icmp"
-    cidr_blocks = [
-      "0.0.0.0/0"]
-  }
-
-  ingress {
-    from_port = 7990
-    to_port = 7990
-    protocol = "tcp"
-    cidr_blocks = [
-      "0.0.0.0/0"]
-  }
-
-  ingress {
-    from_port = 7999
-    to_port = 7999
-    protocol = "tcp"
-    cidr_blocks = [
-      "0.0.0.0/0"]
-  }
-
-  ingress {
-    from_port = "22"
-    to_port = "22"
-    protocol = "tcp"
-    cidr_blocks = [
-      "0.0.0.0/0"]
-  }
-
-  ## Allow all outbound traffic
-  egress {
-    from_port = 0
-    to_port = 0
-    protocol = "-1"
-    cidr_blocks = [
-      "0.0.0.0/0"]
-  }
-
-  tags {
-    Name = "sg_public_bbs_${var.install_version}"
-  }
-}
-
-resource "aws_instance" "rcbbs-2" {
-  ami = "${var.ami_us_east_1_ubuntu1604}"
-  availability_zone = "${var.avl-zone}"
-  instance_type = "${var.in_type_scm}"
-  key_name = "${var.aws_key_name}"
-  subnet_id = "${aws_subnet.sn_public.id}"
-
-  vpc_security_group_ids = [
-    "${aws_security_group.sg_public_bbs.id}"]
-
-  root_block_device {
-    volume_type = "gp2"
-    volume_size = 30
-    delete_on_termination = true
-  }
-
-  tags = {
-    Name = "rcbbs_2_${var.install_version}"
-  }
BREAKS HERE
-variable "etcd-ips" { default = "10.0.0.10,10.0.0.11,10.0.0.12" }
BREAKS HERE
-  region  = "${var.region}"
BREAKS HERE
-* listeners and target groups.
-* Listeners and target groups are defined with map variables,
-* where the key is the name of the resource, and the value is
-* the port and protocol of the resource, specified as `PROTOCOL:PORT`.
-*
-* Listeners are associated to target groups with the `listener_target_groups`
-* map, that uses the name of the resources.
-*
-* When using a listener on HTTPS, we can specify the certificate with
-* the `listener_certificates` variable, where the value is the domain name
-* of the certificate as registered on AWS. The value is used by an
-* `aws_acm_certificate` data resource to find the certificate ARN.
-* listeners = {
-*   "myapp-http-80"   = "HTTP:80"
-*   "myapp-https-443" = "HTTPS:443"
-* }
-*
-* target_groups = {
-*   "http-80" = "HTTP:80"
-* }
-*
-* listener_target_groups = {
-*   "myapp-http-80"   = "http-80"
-*   "myapp-https-443" = "http-80"
-* }
-*
-* listener_certificates = {
-*   "myapp-https-443" = "mydomain.com"
-*
-variable "vpc_id" {
-  type        = "string"
-  description = "The ID of the VPC in which the target groups are created."
-}
-
-variable "listeners" {
-  type        = "map"
-  description = "A map of Load Balancer Listener resources."
-variable "listener_certificates" {
-  description = "A map of Load Balancer Listener certificate domain names."
-variable "target_groups" {
-  type        = "map"
-  description = "A map of Load Balancer Target group resources."
-variable "listener_target_groups" {
-  type        = "map"
-  description = "A map matching Load Balancer Listeners and Target groups"
-  count    = "${length(keys(var.listener_certificates))}"
-  domain   = "${element(values(var.listener_certificates), count.index)}"
-locals {
-  listener_certificate_arns = "${zipmap(keys(var.listener_certificates), data.aws_acm_certificate.cert.*.arn)}"
-}
-
-    prefix  = "lb/${var.name}"
-  count             = "${length(keys(var.listeners))}"
-  port              = "${element(split(":", lookup(var.listeners, element(keys(var.listeners), count.index))), 1)}"
-  protocol          = "${element(split(":", lookup(var.listeners, element(keys(var.listeners), count.index))), 0)}"
-  ssl_policy        = "ELBSecurityPolicy-2015-05"
-  certificate_arn   = "${lookup(local.listener_certificate_arns, element(keys(var.listeners), count.index), "")}"
-    target_group_arn = "${matchkeys(values(local.target_groups_arns), keys(local.target_groups_arns), lookup(var.listener_target_groups, element(keys(var.listeners), count.index)))}"
-resource "aws_lb_target_group" "tg" {
-  count                = "${length(keys(var.target_groups))}"
-  name                 = "${element(keys(var.target_groups), count.index)}"
-  port                 = "${element(split(":", lookup(var.target_groups, element(keys(var.target_groups), count.index))), 1)}"
-  protocol             = "${element(split(":", lookup(var.target_groups, element(keys(var.target_groups), count.index))), 0)}"
-    protocol            = "${element(split(":", lookup(var.target_groups, element(keys(var.target_groups), count.index))), 0)}"
-  target_groups_arns = "${zipmap(aws_lb_target_group.tg.*.name, aws_lb_target_group.tg.*.arn)}"
BREAKS HERE
-  member  = "serviceAccount:${google_service_account.vault_cluster_admin.email}"
BREAKS HERE
-resource "aws_s3_bucket" "sumo-prod-media" {
BREAKS HERE
-  source = "../../vpc"
-  source = "../../vpn-gateway"
BREAKS HERE
-      agent       = "false"
-      agent       = "false"
-      agent       = "false"
-      agent       = "false"
-      agent       = "false"
-      agent       = "false"
-      agent       = "false"
-      agent       = "false"
-      agent       = "false"
-      agent       = "false"
-      agent       = "false"
-      agent       = "false"
-      agent       = "false"
-      agent       = "false"
-      agent       = "false"
BREAKS HERE
-  sns_topic_arn = "${element(compact(concat(aws_sns_topic.this.*.arn, data.aws_sns_topic.this.*.arn, list(""))), 0)}"
BREAKS HERE
-  security_group_ids = ["${module.etcd.secgroup_id}"]
-  security_group_ids = ["${module.master_nodes.secgroup_master_id}"]
-  security_group_ids = ["${module.worker_nodes.secgroup_node_id}"]
BREAKS HERE
-data "aws_acm_certificate" "elb_internal_cert" {
-  domain   = "${var.elb_internal_certname}"
-  statuses = ["ISSUED"]
-}
-
-aesource "aws_elb" "prometheus_external_elb" {
-  instance_elb_ids_length       = "2"
-  instance_elb_ids              = ["${aws_elb.prometheus_internal_elb.id}", "${aws_elb.prometheus_external_elb.id}"]
BREAKS HERE
-  rrdatas      = ["${google_compute_instance.jumpbox.network_interface.0.access_config.0.assigned_nat_ip}"]
BREAKS HERE
-  owners     = ["496014204152"]
-resource "null_resource" "kops_full_cluster-spec_file" {
-  triggers {
-    content = "${data.template_file.kops_full_cluster-spec_file.rendered}"
-  }
-
-  provisioner "local-exec" {
-    command = <<-EOC
-      tee ${path.cwd}/kops-cluster.yaml <<EOF
-      ${data.template_file.kops_full_cluster-spec_file.rendered}
-      EOF
-      EOC
-  }
BREAKS HERE
-      "salt-call --force-color --local --output=quiet state.sls default,terraform-resource",
BREAKS HERE
-locals {
-  # Mapping from the node type that we selected and the max number of pods that it can run
-  # Taken from https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-06-05/amazon-eks-nodegroup.yaml
-  max_pod_per_node = {
-    c4.large    = 29
-    c4.xlarge   = 58
-    c4.2xlarge  = 58
-    c4.4xlarge  = 234
-    c4.8xlarge  = 234
-    c5.large    = 29
-    c5.xlarge   = 58
-    c5.2xlarge  = 58
-    c5.4xlarge  = 234
-    c5.9xlarge  = 234
-    c5.18xlarge = 737
-    i3.large    = 29
-    i3.xlarge   = 58
-    i3.2xlarge  = 58
-    i3.4xlarge  = 234
-    i3.8xlarge  = 234
-    i3.16xlarge = 737
-    m3.medium   = 12
-    m3.large    = 29
-    m3.xlarge   = 58
-    m3.2xlarge  = 118
-    m4.large    = 20
-    m4.xlarge   = 58
-    m4.2xlarge  = 58
-    m4.4xlarge  = 234
-    m4.10xlarge = 234
-    m5.large    = 29
-    m5.xlarge   = 58
-    m5.2xlarge  = 58
-    m5.4xlarge  = 234
-    m5.12xlarge = 234
-    m5.24xlarge = 737
-    p2.xlarge   = 58
-    p2.8xlarge  = 234
-    p2.16xlarge = 234
-    p3.2xlarge  = 58
-    p3.8xlarge  = 234
-    p3.16xlarge = 234
-    r3.xlarge   = 58
-    r3.2xlarge  = 58
-    r3.4xlarge  = 234
-    r3.8xlarge  = 234
-    r4.large    = 29
-    r4.xlarge   = 58
-    r4.2xlarge  = 58
-    r4.4xlarge  = 234
-    r4.8xlarge  = 234
-    r4.16xlarge = 737
-    t2.small    = 8
-    t2.medium   = 17
-    t2.large    = 35
-    t2.xlarge   = 44
-    t2.2xlarge  = 44
-    x1.16xlarge = 234
-    x1.32xlarge = 234
-  }
-
-  asg_tags = ["${null_resource.tags_as_list_of_maps.*.triggers}"]
-
-  # More information: https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-06-05/amazon-eks-nodegroup.yaml
-  workers_userdata = <<USERDATA
-#!/bin/bash -xe
-
-CA_CERTIFICATE_DIRECTORY=/etc/kubernetes/pki
-CA_CERTIFICATE_FILE_PATH=$CA_CERTIFICATE_DIRECTORY/ca.crt
-mkdir -p $CA_CERTIFICATE_DIRECTORY
-echo "${aws_eks_cluster.this.certificate_authority.0.data}" | base64 -d >  $CA_CERTIFICATE_FILE_PATH
-INTERNAL_IP=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)
-sed -i s,MASTER_ENDPOINT,${aws_eks_cluster.this.endpoint},g /var/lib/kubelet/kubeconfig
-sed -i s,CLUSTER_NAME,${var.cluster_name},g /var/lib/kubelet/kubeconfig
-sed -i s,REGION,${data.aws_region.current.name},g /etc/systemd/system/kubelet.service
-sed -i s,MAX_PODS,${lookup(local.max_pod_per_node, var.workers_instance_type)},g /etc/systemd/system/kubelet.service
-sed -i s,MASTER_ENDPOINT,${aws_eks_cluster.this.endpoint},g /etc/systemd/system/kubelet.service
-sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kubelet.service
-DNS_CLUSTER_IP=10.100.0.10
-if [[ $INTERNAL_IP == 10.* ]] ; then DNS_CLUSTER_IP=172.20.0.10; fi
-sed -i s,DNS_CLUSTER_IP,$DNS_CLUSTER_IP,g /etc/systemd/system/kubelet.service
-sed -i s,CERTIFICATE_AUTHORITY_FILE,$CA_CERTIFICATE_FILE_PATH,g /var/lib/kubelet/kubeconfig
-sed -i s,CLIENT_CA_FILE,$CA_CERTIFICATE_FILE_PATH,g  /etc/systemd/system/kubelet.service
-systemctl daemon-reload
-systemctl restart kubelet kube-proxy
-USERDATA
-
-  config_map_aws_auth = <<CONFIGMAPAWSAUTH
-apiVersion: v1
-kind: ConfigMap
-metadata:
-  name: aws-auth
-  namespace: kube-system
-data:
-  mapRoles: |
-    - rolearn: ${aws_iam_role.workers.arn}
-      username: system:node:{{EC2PrivateDNSName}}
-      groups:
-        - system:bootstrappers
-        - system:nodes
-CONFIGMAPAWSAUTH
-
-  kubeconfig = <<KUBECONFIG
-
-apiVersion: v1
-clusters:
-- cluster:
-    server: ${aws_eks_cluster.this.endpoint}
-    certificate-authority-data: ${aws_eks_cluster.this.certificate_authority.0.data}
-  name: kubernetes
-contexts:
-- context:
-    cluster: kubernetes
-    user: aws
-  name: aws
-current-context: aws
-kind: Config
-preferences: {}
-users:
-- name: aws
-  user:
-    exec:
-      apiVersion: client.authentication.k8s.io/v1alpha1
-      command: heptio-authenticator-aws
-      args:
-        - "token"
-        - "-i"
-        - "${var.cluster_name}"
-KUBECONFIG
-}
BREAKS HERE
-  count = "${var.count}"
-  depends_on = ["openstack_compute_floatingip_associate_v2.module_floating_ip_association"]
-    host = "${element(openstack_networking_floatingip_v2.floating_ip.*.address, count.index)}"
BREAKS HERE
-  description = "DNS Server to be useddd by Virtual Machine(s). Multiple DNS servers can be seperated by whitespace. Example: `\"192.168.1.1 192.168.2.1\"`"
BREAKS HERE
-        source = "./01-master.sh"
-        destination = "/tmp/01-master.sh"
-            "chmod +x /tmp/01-master.sh",
-            "sudo -E /tmp/01-master.sh"
-        source = "./02-worker.sh"
-        destination = "/tmp/02-worker.sh"
-            "chmod +x /tmp/02-worker.sh",
-            "sudo -E /tmp/02-worker.sh"
-            sed -e "s/\$EXT_IP1/${digitalocean_droplet.k8s_worker.0.ipv4_address}/" < ${path.module}/04-microbot.yaml > ./secrets/04-microbot.rendered.yaml
-            kubectl create -f ./secrets/04-microbot.rendered.yaml
-            sed -e "s/\$DO_ACCESS_TOKEN/${var.do_token}/" < ${path.module}/05-do-secret.yaml > ./secrets/05-do-secret.rendered.yaml
-            kubectl create -f ./secrets/05-do-secret.rendered.yaml
BREAKS HERE
-&& helm init --upgrade --service-account tiller
BREAKS HERE
-  variables {
-    "depends" = "${md5(join(",", var.depends_on))}"
-  }
BREAKS HERE
-  description = "S3 bucket for storing ALB access logs. Setting this means the module will try to create the bucket."
BREAKS HERE
-  name          = "${var.name}"
BREAKS HERE
-               "config:PutEvaluations"
BREAKS HERE
-# Create a new tag
-resource "digitalocean_tag" "cluster_tag" {
-  name = "${var.cluster_tag}"
-}
-
-  name       = "SSH Key for Terraform"
-  tags               = ["${digitalocean_tag.cluster_tag.id}"]
-  tags               = ["${digitalocean_tag.cluster_tag.id}"]
-  tags               = ["${digitalocean_tag.cluster_tag.id}"]
-  tags               = ["${digitalocean_tag.cluster_tag.id}"]
-  tags               = ["${digitalocean_tag.cluster_tag.id}"]
BREAKS HERE
-  tags = "${merge(map("Name", "${var.name}-intra"), var.tags, var.intra_route_table_tags)}"
-  tags = "${merge(map("Name", format("%s-intra-%s", var.name, element(var.azs, count.index))), var.tags, var.intra_subnet_tags)}"
BREAKS HERE
-variable "kube-api-internal-ip" {
BREAKS HERE
-      "ansible --version",
BREAKS HERE
-  records = [
-    "0 0 2379 etcd1.k8s",
-    "0 0 2379 etcd2.k8s",
-    "0 0 2379 etcd3.k8s"
-  ]
-  records = [
-    "0 0 2380 etcd1.k8s",
-    "0 0 2380 etcd2.k8s",
-    "0 0 2380 etcd3.k8s"
-  ]
BREAKS HERE
-  source = "github.com/gruntwork-io/terraform-kubernetes-helm.git//modules/k8s-tiller?ref=v0.3.0"
BREAKS HERE
-  domain_name               = "*.${var.cluster["domain"]}"
-  subject_alternative_names = [ "*.${var.cluster["name"]}.${var.cluster["domain"]}" ]
BREAKS HERE
-  description = "Allows internal ELB traffic"
BREAKS HERE
-      "apt-get install -yq apt-transport-https ufw nfs-common",
BREAKS HERE
-    internal  = 1111
-    external  = 1111
-    ip        = "${var.ips["tun0"]}"
BREAKS HERE
-resource "aws_security_group_rule" "nfs-out" {
BREAKS HERE
-    privileged_mode = "${var.build.build_privileged_override}"
-    privileged_mode = "${var.build.build_privileged_override}"
BREAKS HERE
-  source    = "git::https://github.com/cloudposse/tf_label.git?ref=tags/0.1.0"
BREAKS HERE
-    name              = "education_standards"
BREAKS HERE
-  name          = "${var.cluster_name}-nodes"
BREAKS HERE
-  role     = "member"
BREAKS HERE
-  name                 = "saml-github.webops"
BREAKS HERE
-  source              = "../../modules/aws/rds_instance"
-  name                = "${var.stackname}-content-data-api-postgresql-primary"
-  engine_name         = "postgres"
-  engine_version      = "9.6"
-  default_tags        = "${map("Project", var.stackname, "aws_stackname", var.stackname, "aws_environment", var.aws_environment, "aws_migration", "content_data_api_postgresql_primary")}"
-  subnet_ids          = "${data.terraform_remote_state.infra_networking.private_subnet_rds_ids}"
-  username            = "${var.username}"
-  password            = "${var.password}"
-  allocated_storage   = "1024"
-  instance_class      = "db.m4.large"
-  instance_name       = "${var.stackname}-content-data-api-postgresql-primary"
-  multi_az            = "${var.multi_az}"
-  security_group_ids  = ["${data.terraform_remote_state.infra_security_groups.sg_content-data-api-postgresql-primary_id}"]
-  event_sns_topic_arn = "${data.terraform_remote_state.infra_monitoring.sns_topic_rds_events_arn}"
-  skip_final_snapshot = "${var.skip_final_snapshot}"
-  snapshot_identifier = "${var.snapshot_identifier}"
BREAKS HERE
-authorized_keys: ${file(var.base_configuration["ssh_key_path"])}
BREAKS HERE
-  ca_endpoints       = "${jsonencode(sort(formatlist("%s/%s/ca", var.vault_base_url, vault_mount.pki.path)))}"
-  ca_chain_endpoints = "${jsonencode(sort(formatlist("%s/%s/ca_chain", var.vault_base_url, vault_mount.pki.path)))}"
-
-  crl_distribution_points = "${jsonencode(sort(formatlist("%s/%s/crl", var.vault_base_url, vault_mount.pki.path)))}"
-  "issuing_certificates": ${local.ca_endpoints},
-  "crl_distribution_points": ${local.crl_distribution_points},
BREAKS HERE
-  source                      = "../module"
-  kubernetes_version          = "1.8.4"
-  sg_allow_ssh                = "${aws_security_group.allow_ssh.id}"
-  sg_allow_http_s             = "${aws_security_group.allow_http.id}"
-  cluster_name                = "cluster1"
-  cluster_fqdn                = "cluster1.${aws_route53_zone.k8s_zone.name}"
-  route53_zone_id             = "${aws_route53_zone.k8s_zone.id}"
-  kops_s3_bucket_arn          = "${aws_s3_bucket.kops.arn}"
-  kops_s3_bucket_id           = "${aws_s3_bucket.kops.id}"
-  vpc_id                      = "${aws_vpc.main_vpc.id}"
-  instance_key_name           = "default-key"
-  node_asg_desired            = 1
-  node_asg_min                = 1
-  node_asg_max                = 1
-  master_instance_type        = "t2.small"
-  node_instance_type          = "t2.small"
-  internet_gateway_id         = "${aws_internet_gateway.public.id}"
-  public_subnet_cidr_blocks   = ["${local.cluster1_public_subnet_cidr_blocks}"]
-  kops_dns_mode               = "private"
-  source                      = "../module"
-  kubernetes_version          = "1.7.10"
-  sg_allow_ssh                = "${aws_security_group.allow_ssh.id}"
-  sg_allow_http_s             = "${aws_security_group.allow_http.id}"
-  cluster_name                = "cluster2"
-  cluster_fqdn                = "cluster2.${aws_route53_zone.k8s_zone.name}"
-  route53_zone_id             = "${aws_route53_zone.k8s_zone.id}"
-  kops_s3_bucket_arn          = "${aws_s3_bucket.kops.arn}"
-  kops_s3_bucket_id           = "${aws_s3_bucket.kops.id}"
-  vpc_id                      = "${aws_vpc.main_vpc.id}"
-  instance_key_name           = "default-key"
-  node_asg_desired            = 1
-  node_asg_min                = 1
-  node_asg_max                = 1
-  master_instance_type        = "t2.small"
-  node_instance_type          = "t2.small"
-  internet_gateway_id         = "${aws_internet_gateway.public.id}"
-  public_subnet_cidr_blocks   = ["${local.cluster2_public_subnet_cidr_blocks}"]
-  private_subnet_ids          = ["${aws_subnet.nat_private.*.id}"]
-  kops_dns_mode               = "private"
BREAKS HERE
-  private_key_pem = "${element(tls_private_key.client.*.private_key_pem, count.index)}"
-    common_name  = "${element(var.vpn_users, count.index)}"
-  cert_request_pem      = "${element(tls_cert_request.client.*.cert_request_pem, count.index)}"
-  ca_private_key_pem    = "${element(tls_private_key.client.*.private_key_pem, count.index)}"
-  content   = "${element(tls_private_key.client.*.private_key_pem, count.index)}"
-  filename  = "${var.algo_config}/keys/${element(var.vpn_users, count.index)}.key.pem"
-    command = "chmod 0600 ${var.algo_config}/keys/${element(var.vpn_users, count.index)}.key.pem"
-  content   = "${element(tls_locally_signed_cert.client.*.cert_pem, count.index)}"
-  filename  = "${var.algo_config}/keys/${element(var.vpn_users, count.index)}.crt.pem"
-chmod 0600 ${element(var.vpn_users, count.index)}.crt.pem
-cp -f ${element(var.vpn_users, count.index)}.crt.pem \
-      .for_crl/${element(var.vpn_users, count.index)}.crt.pem
-
-  # provisioner "local-exec" {
-  #   command     = "cp -f ${element(var.vpn_users, count.index)}.crt.pem .deleted-${element(var.vpn_users, count.index)}.crt.pem"
-  #   when        = "destroy"
-  #   working_dir = "${var.algo_config}/keys/"
-  # }
-  content   = "${element(tls_private_key.client.*.private_key_pem, count.index)}"
-  filename  = "${var.algo_config}/${element(var.vpn_users, count.index)}.ssh.pem"
-    command = "chmod 0600 ${var.algo_config}/${element(var.vpn_users, count.index)}.ssh.pem"
BREAKS HERE
-
-  ingress {
-    from_port = "443"
-    to_port = "443"
-    protocol = "tcp"
-    cidr_blocks = ["0.0.0.0/0"]
-  }
-  security_groups = ["${aws_security_group.farspark-alb.id}"]
-    domain_name = "${aws_alb.farspark-alb.dns_name}"
BREAKS HERE
-    etcd_operator                   = "quay.io/coreos/etcd-operator:v0.4.0"
BREAKS HERE
-  assume_role_policy = "${file("./files/lambdarole.json")}"
-  policy = "${file("./files/lambdapolicy.json")}"
-    template = "${file("./files/vars.ini.template")}"
BREAKS HERE
-    iops        = "${var.root_volume_type == "io1" ? var.root_volume_iops : 100}"
BREAKS HERE
-    mkdir ${path.module}/lambda && mkdir ${path.module}/tmp
-    cp ${path.module}/ebs_bckup/ebs_bckup.py ${path.module}/tmp/ebs_bckup.py
-  source_dir  = "tmp"
BREAKS HERE
-  target_tags = ["${var.network_name}-nat-${var.zone}"]
BREAKS HERE
-    "${ aws_security_group.etcd.id }",
BREAKS HERE
-  name                = "es-cleanup-execution-schedule"
-  target_id = "lambda-es-cleanup"
BREAKS HERE
-  name     = "forseti_security"
BREAKS HERE
-data "aws_iam_role" "default" {
-  count = "${signum(length(var.roles)) == 1 ? length(var.roles) : 0}"
-  name  = "${element(var.roles, count.index)}"
-data "aws_iam_policy_document" "login" {
-  statement {
-    sid     = "ECRGetAuthorizationToken"
-    effect  = "Allow"
-    actions = ["ecr:GetAuthorizationToken"]
-    resources = ["*"]
-  }
-data "aws_iam_policy_document" "write" {
-    sid    = "ECRGetAuthorizationToken"
-    actions = [
-      "ecr:InitiateLayerUpload",
-      "ecr:UploadLayerPart",
-      "ecr:CompleteLayerUpload",
-      "ecr:PutImage",
-    ]
-
-    resources = ["${aws_ecr_repository.default.arn}"]
-  }
-}
-data "aws_iam_policy_document" "read" {
-  statement {
-    sid    = "ECRGetAuthorizationToken"
-    effect = "Allow"
-      "ecr:BatchCheckLayerAvailability",
-      "ecr:GetRepositoryPolicy",
-      "ecr:BatchGetImage",
-
-    resources = ["${aws_ecr_repository.default.arn}"]
-data "aws_iam_policy_document" "default_ecr" {
-  count = "${signum(length(var.roles)) == 1 ? 0 : 1}"
-    sid    = "ecr"
-        "${aws_iam_role.default.arn}",
-      "ecr:GetDownloadUrlForLayer",
-      "ecr:BatchGetImage",
-      "ecr:PutImage",
-      "ecr:InitiateLayerUpload",
-      "ecr:UploadLayerPart",
-      "ecr:CompleteLayerUpload",
-data "aws_iam_policy_document" "resource" {
-  count = "${signum(length(var.roles))}"
-
-    sid    = "ecr"
-        "${data.aws_iam_role.default.*.arn}",
-      "ecr:GetDownloadUrlForLayer",
-      "ecr:BatchGetImage",
-      "ecr:BatchCheckLayerAvailability",
-      "ecr:PutImage",
-module "label" {
-  source     = "git::https://github.com/cloudposse/terraform-null-label.git?ref=tags/0.3.1"
-  namespace  = "${var.namespace}"
-  stage      = "${var.stage}"
-  name       = "${var.name}"
-  delimiter  = "${var.delimiter}"
-  attributes = "${var.attributes}"
-  tags       = "${var.tags}"
-}
-resource "aws_ecr_repository" "default" {
-  name = "${var.use_fullname == "true" ? module.label.id : module.label.name}"
-  count      = "${signum(length(var.roles))}"
-
-resource "aws_ecr_repository_policy" "default_ecr" {
-  count      = "${signum(length(var.roles)) == 1 ? 0 : 1}"
-  repository = "${aws_ecr_repository.default.name}"
-  policy     = "${data.aws_iam_policy_document.default_ecr.json}"
-}
-
-resource "aws_iam_policy" "login" {
-  name        = "${module.label.id}${var.delimiter}login"
-  description = "Allow IAM Users to call ecr:GetAuthorizationToken"
-  policy      = "${data.aws_iam_policy_document.login.json}"
-}
-
-resource "aws_iam_policy" "read" {
-  name        = "${module.label.id}${var.delimiter}read"
-  description = "Allow IAM Users to pull from ECR"
-  policy      = "${data.aws_iam_policy_document.read.json}"
-}
-
-resource "aws_iam_policy" "write" {
-  name        = "${module.label.id}${var.delimiter}write"
-  description = "Allow IAM Users to push into ECR"
-  policy      = "${data.aws_iam_policy_document.write.json}"
-}
-
-resource "aws_iam_role" "default" {
-  count              = "${signum(length(var.roles)) == 1 ? 0 : 1}"
-  name               = "${module.label.id}"
-  assume_role_policy = "${data.aws_iam_policy_document.assume_role.json}"
-}
-
-resource "aws_iam_role_policy_attachment" "default_ecr" {
-  count      = "${signum(length(var.roles)) == 1 ? 0 : 1}"
-  role       = "${aws_iam_role.default.name}"
-  policy_arn = "${aws_iam_policy.login.arn}"
-}
-
-resource "aws_iam_role_policy_attachment" "default" {
-  count      = "${signum(length(var.roles)) == 1 ? length(var.roles) : 0}"
-  role       = "${element(var.roles, count.index)}"
-  policy_arn = "${aws_iam_policy.login.arn}"
-}
-
-resource "aws_iam_instance_profile" "default" {
-  count = "${signum(length(var.roles)) == 1 ? 0 : 1}"
-  name  = "${module.label.id}"
-  role  = "${aws_iam_role.default.name}"
-}
-
-resource "aws_ecr_lifecycle_policy" "default" {
-  repository = "${aws_ecr_repository.default.name}"
-
-  policy = <<EOF
-{
-  "rules": [{
-    "rulePriority": 1,
-    "description": "Rotate images when reach ${var.max_image_count} images stored",
-    "selection": {
-      "tagStatus": "tagged",
-      "tagPrefixList": ["${var.stage}"],
-      "countType": "imageCountMoreThan",
-      "countNumber": ${var.max_image_count}
-    },
-    "action": {
-      "type": "expire"
-    }
-  }]
-}
-EOF
-}
BREAKS HERE
-  name                = "tf${substr(md5(random_id.cluster.id),0,4)}exhibitor"
-  resource_group_name = "${azurerm_resource_group.dcos.name}"
-  location            = "${azurerm_resource_group.dcos.location}"
-  account_type        = "Standard_LRS"
BREAKS HERE
-  count                   = "${length(var.aws_azs[var.aws_region])}"
BREAKS HERE
-    description = "Determines the kind of CosmosDB to create. Can either be 'GlobalDocumentDB' or 'MongoDB'."
-    type        = "string"
-    default     = "GlobalDocumentDB"
-    description = "Determines if automatic failover is enabled for the created CosmosDB."
-    default     = false
-    description = "The Consistency Level to use for this CosmosDB Account. Can be either 'BoundedStaleness', 'Eventual', 'Session', 'Strong' or 'ConsistentPrefix'."
-    type        = "string"
-    default     = "Session"
-    description = "The name of the Azure region to host replicated data."
-    type        = "string"
-}
BREAKS HERE
-  count = "${length(var.allowed_security_groups)}"
BREAKS HERE
-# Copyright 2017 The Gardener Authors.
BREAKS HERE
-  name        = "tf-redis-${var.name}-${data.aws_vpc.vpc.tags["Name"]}"
-  family      = "redis${replace(var.redis_version, "/\\.[\\d+]$/","")}" # Strip the patch version from redis_version var
-  name       = "tf-redis-${var.name}-${data.aws_vpc.vpc.tags["Name"]}"
BREAKS HERE
-  name                   = "${var.prefix}-${var.db_name}"
BREAKS HERE
-    host = "${element(openstack_networking_floatingip_v2.floating_ip.*.address, count.index)}"
BREAKS HERE
-  vpc_zone_identifier = "${var.worker_subnet_ids}"
BREAKS HERE
-# provider "aws" {
-#   alias      = "foreign_acct"
-#   region     = "${var.aws_foreign_acct_region != "" ? var.aws_foreign_acct_region : var.aws_region}"
-#   access_key = "${var.aws_foreign_acct_access_key != "" ? var.aws_foreign_acct_access_key : var.aws_access_key}"
-#   secret_key = "${var.aws_foreign_acct_secret_key != "" ? var.aws_foreign_acct_secret_key : var.aws_secret_key}"
-#   version    = "~>1.52.0"
-# }
-output "multi_account" {
-  value = "${local.multi_account}"
-}
-
-output "foreign_access_key_var" {
-  value = "${var.aws_foreign_acct_access_key}"
-}
-
-output "foreign_parsed_access" {
-  value = "${var.aws_foreign_acct_access_key != "" ? var.aws_foreign_acct_access_key : var.aws_access_key}"
-}
-
-# # provider "http" {}
-# # Get account metadata of the primary account (databricks account)
-# data "aws_caller_identity" "current" {}
-# # Set up bucket policy:
-# data "aws_s3_bucket" "target_s3_bucket" {
-#   provider = "aws.foreign_acct"
-#   bucket = "${var.s3_bucket_name}"
-# }
-# data "template_file" "bucket_policy" {
-#   template = "${file("${path.module}/policies/bucket_policy.template.json")}"
-#   vars = {
-#     db_aws_account_id     = "${data.aws_caller_identity.current.account_id}"
-#     s3_cross_account_role = "${aws_iam_role.databricks_to_s3_role.id}"
-#     target_bucket_name    = "${data.aws_s3_bucket.target_s3_bucket.id}"
-#   }
-# }
-# resource "aws_s3_bucket_policy" "bucket_policy" {
-#   provider = "aws.foreign_acct"
-#   bucket = "${data.aws_s3_bucket.target_s3_bucket.id}"
-#   policy = "${data.template_file.bucket_policy.rendered}"
-# }
-# # Need to explicitally specify the instance profile for the role as well
-# resource "aws_iam_instance_profile" "role_instance_profile" {
-#   # provider = "aws.db_acct"
-#   name     = "${aws_iam_role.databricks_to_s3_role.name}"
-#   role     = "${aws_iam_role.databricks_to_s3_role.name}"
-# }
-# # Attach an inline policy to the role
-# resource "aws_iam_policy" "databricks_to_s3_policy" {
-#   # provider = "aws.db_acct"
-#   name     = "${var.custom_iam_role_name}-policy"
-#   policy   = "${data.template_file.databricks_to_s3_policy_config.rendered}"
-# }
-# resource "aws_iam_role_policy_attachment" "attach_policy_to_role" {
-#   # provider   = "aws.db_acct"
-#   role       = "${aws_iam_role.databricks_to_s3_role.id}"
-#   policy_arn = "${aws_iam_policy.databricks_to_s3_policy.arn}"
-# }
-# # Interpolate the role inline policy config template
-# data "template_file" "databricks_to_s3_policy_config" {
-#   template = "${file("${path.module}/policies/role_policy.template.json")}"
-#   vars = {
-#     target_bucket_name = "${data.aws_s3_bucket.target_s3_bucket.id}"
-#   }
-# }
-# # Set up pass through from the workspace role:
-# # New policy gets added to the existing Databricks EC2 role:
-# resource "aws_iam_policy" "pass_through_policy" {
-#   # provider = "aws.db_acct"
-#   name     = "${var.databricks_deployment_role}-policy"
-#   policy   = "${data.template_file.pass_through_policy_config.rendered}"
-# }
-# resource "aws_iam_role_policy_attachment" "attach_pass_through_policy_to_databricks_bucket_role" {
-#   # provider   = "aws.db_acct"
-#   role       = "${var.databricks_deployment_role}"
-#   policy_arn = "${aws_iam_policy.pass_through_policy.arn}"
-# }
-# data "template_file" "pass_through_policy_config" {
-#   template = "${file("${path.module}/policies/pass_through_policy.template.json")}"
-#   vars = {
-#     aws_account_id_databricks = "${data.aws_caller_identity.current.account_id}"
-#     iam_role_for_s3_access    = "${aws_iam_role.databricks_to_s3_role.name}"
-#     foo                       = "hello"
-#   }
-# }
-# # User must now enter the IAM Role to Databricks, etc:
-# data "aws_iam_instance_profile" "databricks_to_s3_role_instance_profile" {
-#   # provider = "aws.db_acct"
-#   name     = "${aws_iam_role.databricks_to_s3_role.id}"
-# }
BREAKS HERE
-  description = "A list of Options to apply."
BREAKS HERE
-    image_re = "${var.image_re}"
BREAKS HERE
-  count          = "${var.provider == "SCALEWAY" ? length(split(",",var.ssh_ip_whitelist)) : 0}"
BREAKS HERE
-  name                    = "ha-network"
-  routing_mode            = "REGIONAL"
-  name          = "ha-subnet"
-  description   = "Subnetwork for HA node"
-  ip_cidr_range = "${var.ip_cidr_range}"
-  name          = "ha-firewall-allow-internal"
-  name    = "ha-firewall-allow-icmp"
-  name    = "ha-firewall-allow-tcp"
BREAKS HERE
-          "awslogs-stream-prefix": "gateway"
-    "name": "gateway",
BREAKS HERE
-  subnet_id     = "${lookup(var.maker_node_counts, var.aws_region, 0)>0?aws_subnet.quorum_maker.0.id:lookup(var.observer_node_counts, var.aws_region, 0)?aws_subnet.quorum_observer.0.id:lookup(var.validator_node_counts, var.aws_region, 0)?aws_subnet.quorum_validator.0.id:aws_subnet.quorum_maker.0.id}"
BREAKS HERE
-  account_id   = "tf-gke-${substr(var.name, 0, min(20, length(var.name)))}"
BREAKS HERE
-resource "aws_route53_record" "wildcard_sys_dns" {
-  name    = "*.pks.${var.env_name}.${var.dns_suffix}"
-  type    = "CNAME"
-  ttl     = 300
-  records = ["${aws_lb.pks_api.dns_name}"]
BREAKS HERE
-  version = ">= 1.10.0"
BREAKS HERE
-  origin_path         = "/"
BREAKS HERE
-  bucket = "quorum-node-counts-network-${var.network_id}"
BREAKS HERE
-  count           = "${length(compact(data.null_data_source.values.*.inputs.ssl_arn_index))}"
BREAKS HERE
-  count  = "${var.create_internal_zone}"
-  name   = "${var.root_domain_internal_name}"
-  vpc_id = "${data.terraform_remote_state.infra_vpc.vpc_id}"
BREAKS HERE
-  count = "${var.kinesis-stream["enabled"]?1:0}"
-  count = "${local.count}"
BREAKS HERE
-  access_key = ""
-  secret_key = ""
-  region     = "${var.region}"
-  name  = "beanstalk-ec2-user"
-  roles = ["${aws_iam_role.beanstalk_ec2.name}"]
-  role = "${aws_iam_role.test_role.id}"
-        "s3:*",
BREAKS HERE
-  name = "${var.cluster["name"]}-monitoring-elb"
-}
BREAKS HERE
-(optional) If set to `true`, TLS secure communication for etcd will be used.
-Note: This is variable has no effect if `tectonic_experimental` is set to `true`.
BREAKS HERE
-  parameter_group_name = "ship-db-pg"
BREAKS HERE
-resource "aws_db_parameter_group" "ship_db_pg_migrate" {
-## resource "aws_db_parameter_group" "ship_db_pg_migrate" {
BREAKS HERE
-  name = "${element(var.images, count.index)}"
BREAKS HERE
-  name    = "real-time-enforcer-events-topic"
-  name    = "real-time-enforcer-events-subscription"
BREAKS HERE
-  records = ["${aws_eip.bastion-1a.public_ip}", "${aws_eip.bastion-1b.public_ip}"]
BREAKS HERE
-  subnet_name              = "pf-test-subnet-${var.random_string_for_testing}"
-  shared_vpc_subnet_name   = "${module.vpc.subnets_names[0]}"
-  shared_vpc_subnet_region = "${module.vpc.subnets_regions[0]}"
-  shared_vpc_subnets       = ["projects/${var.shared_vpc}/regions/${local.shared_vpc_subnet_region}/subnetworks/${local.shared_vpc_subnet_name}"]
-      subnet_name   = "${local.subnet_name}"
-    "${local.subnet_name}" = [
-        range_name    = "${local.subnet_name}-secondary"
BREAKS HERE
-    aws_region  = "${var.aws_region}"
-    gitlab_url  = "${var.runners_gitlab_url}"
-    environment = "${var.environment}"
-
-    runners_vpc_id              = "${var.vpc_id}"
-    runners_subnet_id           = "${var.subnet_id_runners}"
-    runners_aws_zone            = "${var.aws_zone}"
-    runners_instance_type       = "${var.docker_machine_instance_type}"
-    runners_spot_price_bid      = "${var.docker_machine_spot_price_bid}"
-    runners_security_group_name = "${aws_security_group.docker_machine.name}"
-    runners_monitoring          = "${var.runners_monitoring}"
-
-    docker_machine_options = "${length(var.docker_machine_options) == 0 ? "" : local.docker_machine_options_string}"
-
BREAKS HERE
-  # If not present in the map, it uses "" that means no reservation id
-  hardware_reservation_id = "${lookup(var.reservation_ids, format("worker-%v", count.index), "")}"
BREAKS HERE
-  name              = "${cloudwatch_prefix}/var/log/dmesg"
-  name              = "${cloudwatch_prefix}/var/log/docker"
-  name              = "${cloudwatch_prefix}/var/log/ecs/ecs-agent.log"
-  name              = "${cloudwatch_prefix}/var/log/ecs/ecs-init.log"
-  name              = "${cloudwatch_prefix}/var/log/ecs/audit.log"
-  name              = "${cloudwatch_prefix}/var/log/messages"
BREAKS HERE
-
-# Help debug all 3 subnets
-// resource "aws_instance" "validator" {
-//   count          = "${var.backup_enabled ? signum(lookup(var.validator_node_counts, var.aws_region, 0)) : 0}"
-//   ami           = "${data.aws_ami.bootnode.id}"
-//   instance_type = "t2.micro"
-
-//   tags {
-//     Name = "Debug-Validator"
-//   }
-
-//   key_name = "quorum-cluster-${var.aws_region}-network-${var.network_id}"
-//   subnet_id = "${aws_subnet.quorum_validator.0.id}"
-//   vpc_security_group_ids = ["${aws_security_group.allow_all_for_backup_lambda.0.id}", 
-//     "${aws_security_group.allow_ssh_for_debugging.0.id}"]
-// }
-
-// data "aws_ami" "ubuntu" {
-//   most_recent = true
-
-//   filter {
-//     name   = "name"
-//     values = ["ubuntu/images/hvm-ssd/ubuntu-trusty-14.04-amd64-server-*"]
-//   }
-
-//   filter {
-//     name   = "virtualization-type"
-//     values = ["hvm"]
-//   }
-
-//   owners = ["099720109477"] # Canonical
-// }
-
-// resource "aws_instance" "backup_lambda" {
-//   count = "${var.aws_region =="us-east-1" ?1:0}"
-//   source_dest_check = false
-//   associate_public_ip_address = true
-//   ami           = "${data.aws_ami.ubuntu.id}"
-//   instance_type = "t2.micro"
-//   key_name = "quorum-cluster-${var.aws_region}-network-${var.network_id}"
-//   subnet_id = "${aws_subnet.backup_lambda_private.id}"
-//   vpc_security_group_ids = ["${aws_security_group.allow_all_for_backup_lambda.*.id}"]
-//   tags {
-//     Name = "quorum-network-${var.network_id}-BackupLambda-NAT-backup_lambda-1"
-//     subnet_id = "BackupLambdaAccessInternet-${aws_subnet.backup_lambda_private.id}"
-//   }
-// }
-
-// resource "aws_instance" "observer" {
-//   count = "${var.aws_region =="us-east-1" && lookup(var.observer_node_counts, var.aws_region, 0) > 0?1:0}"
-//   source_dest_check = false
-//   associate_public_ip_address = true
-//   ami           = "${data.aws_ami.ubuntu.id}"
-//   instance_type = "t2.micro"
-//   key_name = "quorum-cluster-${var.aws_region}-network-${var.network_id}"
-//   subnet_id = "${aws_subnet.quorum_observer.0.id}"
-//   vpc_security_group_ids = ["${aws_security_group.allow_all_for_backup_lambda.*.id}"]
-//   tags {
-//     Name = "quorum-network-${var.network_id}-BackupLambda-NAT-observer-1"
-//     subnet_id = "BackupLambdaAccessInternet-${aws_subnet.quorum_observer.0.id}"
-//   }
-// }
-
-// resource "aws_instance" "validator" {
-//   count = "${var.aws_region =="us-east-1" && lookup(var.validator_node_counts, var.aws_region, 0)>0?1:0}"
-//   source_dest_check = false
-//   associate_public_ip_address = true
-//   ami           = "${data.aws_ami.ubuntu.id}"
-//   instance_type = "t2.micro"
-//   key_name = "quorum-cluster-${var.aws_region}-network-${var.network_id}"
-//   subnet_id = "${aws_subnet.quorum_validator.0.id}"
-//   vpc_security_group_ids = ["${aws_security_group.allow_all_for_backup_lambda.*.id}"]
-//   tags {
-//     Name = "quorum-network-${var.network_id}-BackupLambda-NAT-validator-1"
-//     subnet_id = "BackupLambdaAccessInternet-${aws_subnet.quorum_validator.0.id}"
-//   }
-// }
BREAKS HERE
-  name_regex = "^ebs-kubernetes-baseimage-${element(split(".",var.k8s_version),0)}.${element(split(".",var.k8s_version),1)}-*"
-  default_master_sg  = "${length(var.extra_master_securitygroups) > 0 ? format("additionalSecurityGroups:\n %s",indent(1,join("\n",formatlist(" - %s",var.extra_master_securitygroups)))) : ""}"
-  default_worker_sg  = "${length(var.extra_worker_securitygroups) > 0 ? format("additionalSecurityGroups:\n %s",indent(1,join("\n",formatlist(" - %s",var.extra_worker_securitygroups)))) : ""}"
-  spot_price         = "${var.spot_price != "" ? format("maxPrice: \"%s\"", var.spot_price) : ""}"
-  ngws_per_az        = "${zipmap(data.aws_subnet.ngw_subnets.*.availability_zone, data.aws_nat_gateway.ngws.*.id)}"
-    kubernetes_ami              = "${data.aws_ami.kubernetes_ami.name}"
-    kubernetes_ami              = "${data.aws_ami.kubernetes_ami.name}"
-
-    name    = "${element(formatlist("master-%s", data.aws_availability_zones.available.names),count.index)}"
-    ig-name = "${element(formatlist("master-%s", data.aws_availability_zones.available.names),count.index)}"
-
BREAKS HERE
-      "Name", "private-${data.aws_availability_zones.azs.names[count.index]}",
-      "Name", "worker-${ "${length(var.worker_azs)}" > 0 ? 
BREAKS HERE
-  source     = "git::https://github.com/cloudposse/tf_label.git?ref=tags/0.2.0"
-  source             = "git::https://github.com/cloudposse/tf_codebuild.git?ref=tags/0.5.0"
-
BREAKS HERE
-    source      = "${ path.module }/../../.secrets/${ var.name }/node${ count.index + 1 }.${ var.internal-tld }.pem"
-    source      = "${ path.module }/../../.secrets/${ var.name }/node${ count.index + 1 }.${ var.internal-tld }-key.pem"
BREAKS HERE
-  value = "postgres://${aws_db_instance.main.username}:${aws_db_instance.main.password}@${aws_db_instance.main.endpoint}"
BREAKS HERE
-  instance_type                 = "t2.medium"
BREAKS HERE
-  dns_names = "${concat(var.std_api_domains,module.api_domains.value, list("*.${var.domain_name}"))}"
BREAKS HERE
-
-  default = 26280
-  type    = "string"
BREAKS HERE
-  count = "${var.az_count}"
-  availability_zone = "${data.aws_availability_zones.zones.names[(count.index % (data.aws_availability_zones.zones.count + 1))]}"
-    Name = "${var.name}-${data.aws_availability_zones.zones.names[(count.index % (data.aws_availability_zones.zones.count + 1))]}-${count.index}"
-  count          = "${var.az_count}"
BREAKS HERE
-    Name = "etcd${ count.index + 1 }"
-    builtWith = "terraform"
BREAKS HERE
-  version = "=1.21.0"
-  version = "=0.1.0"
BREAKS HERE
-    enabled  = "${list(var.ip_configuration)}"
-    disabled = "${list()}"
-    ip_configuration            = "${local.ip_configurations["${local.ip_configuration_enabled ? "enabled" : "disabled"}"]}"
BREAKS HERE
-# TODO pull via a prefix query
-# https://github.com/terraform-providers/terraform-provider-azurerm/issues/577
-
-  name                = "elasticsearch5-2017-11-17T013212"
-  name                = "kibana5-2017-11-23T074321"
BREAKS HERE
-  protocol          = "HTTP"
-  port              = "443"
BREAKS HERE
-    tsa_host_key           = "${file("${var.web_session_signing_key_path}")}"
BREAKS HERE
-  subnet_id     = "${var.public_subnet_ids[count.index]}"
-  subnet_id      = "${var.private_subnet_ids[count.index]}"
BREAKS HERE
-  cidr_block           = "100.127.0.0/16"
BREAKS HERE
-
BREAKS HERE
-  count   = "${var.module_enabled ? 1 : 0}"
-  project = "${var.project}"
-  name    = "${random_id.rand.dec}"
-}
-
-resource "random_id" "rand" {
-  byte_length = 8
-  prefix      = "default-"
-}
BREAKS HERE
-resource "aws_instance" "in_cs_1" {
-resource "aws_instance" "in_cs_2" {
-resource "aws_instance" "in_ms_1" {
-resource "aws_instance" "in_ms_2" {
BREAKS HERE
-
-  worker_security_group_id  = "${coalesce(join("", aws_security_group.workers.*.id), var.cluster_security_group_id)}"
BREAKS HERE
-resource "aws_route" "inte/net_access" {
BREAKS HERE
-  function_name = "${var.function_name}"
-  s3_bucket     = "${var.s3_bucket}"
-  s3_key        = "${var.s3_key}"
-  runtime       = "${var.runtime}"
-  timeout       = "${var.timeout}"
-  environment   = "${var.environment}"
BREAKS HERE
-  source = "git::git@github.com:gruntwork-io/terraform-google-network.git//modules/vpc-network?ref=v0.0.2"
-  name    = "${var.cluster_name}-network-${random_string.suffix.result}"
-  project = "${var.project}"
-  region  = "${var.region}"
BREAKS HERE
-
BREAKS HERE
-resource "aws_iam_role_policy" "nodes-haystack-k8s-policy" {
-  name = "nodes.haystack-k8s"
-  role = "${aws_iam_role.nodes-haystack-k8s-role.name}"
-  policy = "${file("${path.module}/data/aws_iam_role_policy_nodes.haystack-k8s_policy")}"
-}
BREAKS HERE
-  source                = "github.com/GoogleCloudPlatform/terraform-google-managed-instance-group"
BREAKS HERE
-    "Name"              = "k8s cluster ${var.cluster_name} ${element(local.az_letters, count.index)}"
BREAKS HERE
-  bucket = "${var.tectonic_cluster_name}.${data.aws_region.current.name}.${var.tectonic_base_domain}"
-  acl    = "private"
BREAKS HERE
-variable "subscription_id" {
-  type = "string"
-}
-
-variable "tenant_id" {
-  type = "string"
-}
-
-}
BREAKS HERE
-    filename = "${path.module}/files/userdata.template"
-        cluster_name           = "${var.cluster_name}"
-        environment_id         = "${var.environment_id}"
-        environment_access_key = "${var.environment_access_key}"
-        environment_secret_key = "${var.environment_secret_key}"
-        server_hostname        = "${var.server_hostname}"
BREAKS HERE
-    iops        = "${var.root_volume_type == "io1" ? var.root_volume_iops : 100}"
BREAKS HERE
-    security_groups = ["${split(\",\", var.security_groups)}"]
BREAKS HERE
-  name = "${var.cluster_name}-kube-apiserver-pub"
-    name = "${var.cluster_name}-kube-apiserver-pri"
BREAKS HERE
-data "aws_route53_zone" "selected" {
-  zone_id = "${local.route53_zone_id}"
-}
-
-data "aws_lb" "main" {
-  arn = "${local.lb_arn}"
-}
-
-data "aws_caller_identity" "current" {}
-
-## TODO Listeners as datasource, but in some cases there is not SSL Listener which would result in failure. Can only work when datasources can be conditionalised.
-
-## LOCALS
-  cluster_name = "${lookup(var.ecs_properties,"ecs_cluster_name")}"
-  cluster_id   = "${data.aws_ecs_cluster.this.arn}"
-
-  awsvpc_enabled = "${var.awsvpc_enabled}"
-  # For Fargate services the Service itself needs to have CPU defined
-  fargate_memory = "${local.fargate_enabled == true ? lookup(var.ecs_properties,"memory") : ""}"
-  fargate_cpu    = "${local.fargate_enabled == true ? lookup(var.ecs_properties,"cpu") : ""}"
-  # Load balancer related properties
-  lb_attached           = "${lookup(var.load_balancing_properties,"alb_attached", true)}"
-
-  container_port = "${lookup(var.container_properties[0], "port")}"
-
-  lb_vpc_id       = "${lookup(var.load_balancing_properties,"lb_vpc_id", "")}"
-  route53_zone_id = "${lookup(var.load_balancing_properties,"route53_zone_id", "")}"
-  route53_name    = "${var.name}.${data.aws_route53_zone.selected.name}"
-
-  health_uri          = "${lookup(var.load_balancing_properties,"health_uri", "/ping")}"
-  unhealthy_threshold = "${lookup(var.load_balancing_properties,"health_uri", "3")}"
-
-  scaling_enabled      = "${length(var.scaling_properties) > 0 ? true : false }"
-  desired_capacity     = "${lookup(var.capacity_properties,"desired_capacity", 2)}"
-  desired_min_capacity = "${lookup(var.capacity_properties,"desired_min_capacity", 2)}"
-  desired_max_capacity = "${lookup(var.capacity_properties,"desired_max_capacity", 2)}"
-  deployment_maximum_percent = "${lookup(var.capacity_properties,"deployment_maximum_percent", 200)}"
-  deployment_minimum_healthy_percent = "${lookup(var.capacity_properties,"deployment_minimum_healthy_percent", 0)}"
-}
-
-##
-## aws_lb_target_group inside the ECS Task will be created when the service is not the default forwarding service
-## It will not be created when the service is not attached to a load balancer like a worker
-
-resource "aws_lb_target_group" "service" {
-  name        = "${local.cluster_name}-${var.name}"
-  port        = 80
-  protocol    = "HTTP"
-  vpc_id      = "${local.lb_vpc_id}"
-  target_type = "${local.awsvpc_enabled == 1 ? "ip" : "instance"}"
-
-  health_check {
-    path                = "${local.health_uri}"
-    unhealthy_threshold = "${local.unhealthy_threshold}"
-  }
-}
-
-##
-## An aws_lb_listener_rule will only be created when a service has a load balancer attached
-resource "aws_lb_listener_rule" "host_based_routing" {
-  count = "${local.lb_attached == "1" ? 1 : 0}"
-
-  listener_arn = "${local.lb_listener_arn}"
-
-  action {
-    type             = "forward"
-    target_group_arn = "${aws_lb_target_group.service.arn}"
-  }
-
-  condition {
-    field  = "host-header"
-    values = ["${aws_route53_record.record.fqdn}"]
-  }
-}
-
-##
-## An aws_lb_listener_rule will only be created when a service has a load balancer attached
-resource "aws_lb_listener_rule" "host_based_routing-ssl" {
-  count = "${local.lb_attached == "1" ? 1 : 0}"
-
-  listener_arn = "${local.lb_listener_arn_https}"
-
-  action {
-    type             = "forward"
-    target_group_arn = "${aws_lb_target_group.service.arn}"
-  }
-
-  condition {
-    field  = "host-header"
-    values = ["${aws_route53_record.record.fqdn}"]
-  }
-## Route53 DNS Record
-
-resource "aws_route53_record" "record" {
-  zone_id = "${data.aws_route53_zone.selected.zone_id}"
-  name    = "${local.route53_name}"
-  type    = "CNAME"
-  ttl     = "300"
-  records = ["${data.aws_lb.main.dns_name}"]
-data "template_file" "task_definition" {
-  count = "${length(var.container_properties)}"
-
-  template = "${file("${"${path.module}/task-definition.json"}")}"
-
-  vars {
-    image_url        = "${lookup(var.container_properties[count.index], "image_url")}"
-    task_type        = "${lookup(var.container_properties[count.index], "task_type","")}"
-    region           = "${data.aws_region.current.name}"
-    cpu              = "${lookup(var.container_properties[count.index], "cpu")}"
-    mem              = "${lookup(var.container_properties[count.index], "mem")}"
-    envvars          = ""
-    container_name   = "${local.cluster_name}-${var.name}"
-    container_port   = "${local.container_port}"
-    host_port        = "${var.awsvpc_enabled == 1 ? local.container_port : "0" }"
-    discovery_name   = "${var.name}"
-    hostname_block   = "${var.awsvpc_enabled == 0 ? "\"hostname\":\"${local.cluster_name}-${var.name}-${count.index}\",\n" :""}"
-    log_group_region = "${data.aws_region.current.name}"
-    log_group_name   = "${aws_cloudwatch_log_group.app.name}"
-    log_group_stream = "${count.index}"
-  }
-resource "aws_ecs_task_definition" "app" {
-  family        = "${var.name}"
-  task_role_arn = "${aws_iam_role.ecs_tasks_role.arn}"
-  # Execution role ARN can be needed inside FARGATE
-  execution_role_arn = "${local.fargate_enabled ? join("",aws_iam_role.ecs_task_execution_role.*.arn) : "" }"
-  cpu    = "${local.fargate_enabled     ? local.fargate_cpu : "" }"
-  memory = "${local.fargate_enabled  ? local.fargate_memory : "" }"
-  container_definitions = "[${join(",",data.template_file.task_definition.*.rendered)}]"
-  network_mode          = "awsvpc"
-  network_mode          = "${local.awsvpc_enabled == 1 ? "awsvpc" : "bridge"}"
-  lifecycle {
-    ignore_changes = ["container_definitions", "placement_constraints"]
-  }
-
-  requires_compatibilities = ["${local.launch_type}"]
-resource "aws_ecs_service" "app-with-lb-awsvpc" {
-  count = "${(local.awsvpc_enabled == 1 ? 1 : 0 ) * (local.lb_attached == 1 ? 1 : 0)}"
-
-  name            = "${local.cluster_name}-${var.name}"
-  cluster         = "${local.cluster_id}"
-  task_definition = "${aws_ecs_task_definition.app.arn}"
-
-  desired_count = "${local.desired_capacity}"
-  launch_type   = "${local.fargate_enabled ? "FARGATE" : "EC2"}"
-
-  deployment_maximum_percent         = "${local.deployment_maximum_percent}"
-  deployment_minimum_healthy_percent = "${local.deployment_minimum_healthy_percent}"
-
-  load_balancer {
-    target_group_arn = "${aws_lb_target_group.service.id}"
-    container_name   = "${local.cluster_name}-${var.name}"
-    container_port   = "${local.container_port}"
-  }
-  lifecycle {
-    ignore_changes = ["desired_count", "task_definition", "revision"]
-  }
-
-  network_configuration {
-    subnets         = ["${var.awsvpc_subnets}"]
-    security_groups = ["${var.awsvpc_security_group_ids}"]
-  }
-}
-
-resource "aws_ecs_service" "app-with-lb" {
-  count           = "${(var.awsvpc_enabled == 0 ? 1 : 0 ) * (local.lb_attached == 1 ? 1 : 0)}"
-  name            = "${local.cluster_name}-${var.name}"
-  launch_type     = "${local.launch_type}"
-  cluster         = "${local.cluster_id}"
-  task_definition = "${aws_ecs_task_definition.app.arn}"
-
-  desired_count = "${local.desired_capacity}"
-
-  deployment_maximum_percent         = "${local.deployment_maximum_percent}"
-  deployment_minimum_healthy_percent = "${local.deployment_minimum_healthy_percent}"
-
-  load_balancer {
-    target_group_arn = "${aws_lb_target_group.service.id}"
-    container_name   = "${local.cluster_name}-${var.name}"
-    container_port   = "${local.container_port}"
-  }
-
-  lifecycle {
-    ignore_changes = ["desired_count", "task_definition", "revision"]
-  }
-}
-
-resource "aws_ecs_service" "app" {
-  count = "${1 - local.lb_attached}"
-
-  name            = "${local.cluster_name}-${var.name}"
-  launch_type     = "${local.launch_type}"
-  cluster         = "${local.cluster_id}"
-  task_definition = "${aws_ecs_task_definition.app.arn}"
-  desired_count   = "${local.desired_capacity}"
-
-  lifecycle {
-    ignore_changes = ["desired_count", "task_definition"]
-  }
-}
-
-###### CloudWatch Logs
-resource "aws_cloudwatch_log_group" "app" {
-  name              = "${local.cluster_name}/${var.name}"
-  retention_in_days = 14
BREAKS HERE
-// - Fix permission issue with helm listing configmaps from the kube-system namespace
-//    host        = "https://${module.linode_k8s.k8s_master_public_ip}"
-  version    = "3.0.2"
-  version = "1.54.0"
BREAKS HERE
-  version = "1.0.0"
BREAKS HERE
-
-  public_key = ""
BREAKS HERE
-  tags = "${merge(locals.instance_name, var.tags)}"
-  tags = "${merge(locals.instance_name, var.tags)}"
BREAKS HERE
-#################
-# Security group
-#################
-  count = "${var.create ? 1 : 0}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
-  security_group_id = "${aws_security_group.this.id}"
BREAKS HERE
-  source   = "./modules/network"
-  project  = "${var.project}"
-  region   = "${var.region}"
-  vpc_name = "${var.vpc_name}-${var.execution_id}"
-  tags     = "${var.bastion_tags}"
-  source   = "./modules/firewall"
-  project  = "${var.project}"
-  vpc      = "${module.network.network_self_link}"
-  net_tags = "${var.bastion_tags}"
-  master_ipv4_cidr_block = "10.0.90.0/28"
-  private_cluster        = true
BREAKS HERE
-    etcd_image_tag    = "${var.tectonic_versions["etcd"]}"
BREAKS HERE
-  tags = "${merge(local.tags,map("Name","${var.prefix}-${var.node_type}-${count.index}-${var.vm_version + element(module.roll.revision_list,count.index)}")}"
-  tags = "${merge(local.tags,map("Name","${var.prefix}-${var.node_type}-${count.index}-${var.vm_version + element(module.roll.revision_list,count.index)}")}"
BREAKS HERE
-    iam_role_id                   = "${aws_iam_role.workers.id}"    # Use the specified IAM role if set.
BREAKS HERE
-      var.tags
BREAKS HERE
-  timeouts {
-    create = "10m"
-  }
BREAKS HERE
- ami = "${var.ami_us_east_1_ubuntu1604}"
- availability_zone = "${var.avl-zone}"
- instance_type = "${var.in_type_ms_x}"
- key_name = "${var.aws_key_name}"
- subnet_id = "${aws_subnet.sn_admiral_setup.id}"
- vpc_security_group_ids = [
-   "${aws_security_group.sg_public_admiral_setup.id}"]
- count = 1
- root_block_device {
-   volume_type = "gp2"
-   volume_size = 50
-   delete_on_termination = true
- }
- tags = {
-   Name = "admiral_ric03uec_u1604_${count.index}_${var.install_version}"
- }
-utput "admiral_ric03uec_u1604" {
- value = "${formatlist("instance %v has private ip %v", aws_instance.admiral_ric03uec_u1604.*.id, aws_instance.admiral_ric03uec_u1604.*.private_ip)}"
BREAKS HERE
-  count  = "${var.enabled == "true" ? 1 : 0}"
-  count  = "${var.enabled == "true" ? 1 : 0}"
-  count  = "${var.enabled == "true" ? 1 : 0}"
-  count  = "${var.enabled == "true" ? 1 : 0}"
-  count  = "${var.enabled == "true" ? 1 : 0}"
-  count  = "${var.enabled == "true" ? 1 : 0}"
-  count  = "${var.enabled == "true" ? 1 : 0}"
BREAKS HERE
-  vpc_name = "kube-net"
-  vpc_name              = "kube-net"
BREAKS HERE
-  launch_configuration = "${var.create_lc ? element(aws_launch_configuration.this.*.name, 0) : var.launch_configuration}"
-    lc_name = "${var.create_lc ? element(aws_launch_configuration.this.*.name, 0) : var.launch_configuration}"
BREAKS HERE
-  required_version = ">= 0.11.7"
-  # And simple security group that opens the port to our simple web server
BREAKS HERE
-    private_ip_address = "${cidrhost(format("10.%d.0.0/16", floor(count.index / var.agent_count)), floor(count.index / length(var.locations)) + 5 + var.master_count)}"
BREAKS HERE
-    austrailia-southeast1 {
-      zone = "austrailia-southeast1-b"
BREAKS HERE
-  default     = "//subscriptions/[-\\w]+/resourceGroups/([-\\w]+)/providers/[.\\w]+/[.\\w]+/([.\\w-]+)/"
BREAKS HERE
-  bucket = "moj-cp-k8s-investigation-platform-terraform"
-  bucket = "moj-cp-k8s-investigation-kops"
BREAKS HERE
-  description = "The price to use for reserving spot instances."
-  description = "The tenancy of the instance. Must be one of: default or dedicated."
-  default     = "default"
BREAKS HERE
-  # Uses the same security group as the vault cluster
-  # Plus an additional one that opens the port to our simple web server
-  security_groups = [
-    "${module.vault_cluster.security_group_id}",
-    "${aws_security_group.auth_instance.id}",
-  ]
BREAKS HERE
-    "--verbosity 4",
BREAKS HERE
-    instance_id = "${element(aws_instance.nat.*.id, count.index)}"
-resource "aws_security_group" "nat" {
-  vpc_id = "${aws_vpc.default.id}"
-
-  tags {
-    Name = "sgNAT"
-  }
-}
-
-resource "aws_security_group_rule" "allow_port_ingress" {
-  count = "${length(split(",", var.nat_egress_ports))}"
-
-  type = "ingress"
-
-  cidr_blocks = ["${var.cidr_block}"]
-  from_port = "${element(split(",", var.nat_egress_ports), count.index)}"
-  to_port = "${element(split(",", var.nat_egress_ports), count.index)}"
-
-  protocol = "tcp"
-
-  security_group_id = "${aws_security_group.nat.id}"
-}
-
-resource "aws_security_group_rule" "allow_port_egress" {
-  count = "${length(split(",", var.nat_egress_ports))}"
-
-  type = "egress"
-
-  cidr_blocks = ["0.0.0.0/0"]
-  from_port = "${element(split(",", var.nat_egress_ports), count.index)}"
-  to_port = "${element(split(",", var.nat_egress_ports), count.index)}"
-
-  protocol = "tcp"
-  security_group_id = "${aws_security_group.nat.id}"
-resource "aws_instance" "nat" {
-  ami                         = "${var.nat_ami}"
-  availability_zone           = "${element(split(",", var.availability_zones), count.index)}"
-  instance_type               = "${var.nat_instance_type}"
-  key_name                    = "${var.key_name}"
-  monitoring                  = true
-  vpc_security_group_ids      = ["${aws_security_group.nat.id}"]
-  subnet_id                   = "${element(aws_subnet.public.*.id, count.index)}"
-  associate_public_ip_address = true
-  source_dest_check           = false
-  tags {
-    Name = "NATDevice"
-  }
BREAKS HERE
-  cloudwatch_awslogs_zookeeper_prefix = "zookeeper.${lower(var.project)}.${lower(var.company)}.io"
-  name = "/${local.cloudwatch_awslogs_zookeeper_prefix}/var/log/zookeeper/zookeeper.log"
-  name = "/${local.cloudwatch_awslogs_zookeeper_prefix}/var/log/messages"
-    awslogs_stream_prefix = "${local.cloudwatch_awslogs_zookeeper_prefix}"
BREAKS HERE
-  cidr_block          = "10.0.10.0/24"
-  cidr_block          = "10.0.11.0/24"
-  cidr_block          = "10.0.12.0/24"
-  cidr_block          = "10.0.20.0/24"
-  cidr_block          = "10.0.21.0/24"
-  cidr_block          = "10.0.22.0/24"
-  cidr_block                 = "10.0.30.0/24"
-  cidr_block                 = "10.0.31.0/24"
-  cidr_block                 = "10.0.32.0/24"
-  cidr_block                 = "10.0.40.0/24"
-  cidr_block                 = "10.0.41.0/24"
-  cidr_block                 = "10.0.42.0/24"
-  cidr_block                 = "10.0.50.0/24"
-  cidr_block                 = "10.0.51.0/24"
-  cidr_block                 = "10.0.52.0/24"
BREAKS HERE
- 
-    name          = "public"
-    ip_cidr_range = "10.64.0.0/22"
-    name          = "internal"
-    ip_cidr_range = "10.64.4.0/22"
- 
-    name = "internal-any-any-access"
-    name = "adminrouter-firewall"
-    name = "ssh"
BREAKS HERE
-    enabled    = true
-    mfa_delete = true
BREAKS HERE
-  name_prefix = "${var.cluster_name}"
-  name_prefix = "${var.cluster_name}"
-  name_prefix = "${var.cluster_name}"
-  name_prefix        = "${var.cluster_name}"
BREAKS HERE
-  namespace                      = "cp-haproxy-"
BREAKS HERE
-  value       = "${local.config_map_aws_auth}"
-  value       = "${local.kubeconfig}"
BREAKS HERE
-  cluster_security_group_id = "${var.cluster_security_group_id == "" ? aws_security_group.cluster.id : var.cluster_security_group_id}"
-  worker_security_group_id  = "${var.worker_security_group_id == "" ? aws_security_group.workers.id : var.worker_security_group_id}"
BREAKS HERE
-  value       = "${aws_cloudtrail.default.id}"
-  value       = "${aws_cloudtrail.default.home_region}"
-  value       = "${aws_cloudtrail.default.arn}"
BREAKS HERE
-    source      = "./lidop_config.yaml"
-      "sudo ansible-playbook -v /tmp/lidop/install/install.yml -e ' ",
-      "root_password=${var.password}",
-      "root_user=${var.user_name}",
-      "node=master",
-      "public_ipaddress=${element(var.master_public_ip, count.index)}",
-      "ipaddress=${element(var.master_private_ip, count.index)}",
-      "install_mode=online",
-      "dns_recursor=${var.dns_recursor}'",
-    source      = "./lidop_config.yaml"
-      "sudo ansible-playbook -v /tmp/lidop/install/install.yml -e ' ",
-      "root_password=${var.password}",
-      "root_user=${var.user_name}",
-      "node=worker",
-      "public_ipaddress=${element(var.worker_public_ips, count.index)}",
-      "ipaddress=${element(var.worker_private_ips, count.index)}",
-      "consul_ip=${element(var.master_private_ip, count.index)}",
-      "install_mode=online",
-      "dns_recursor=${var.dns_recursor}'",
BREAKS HERE
-data "aws_security_group" "foreign_sg" {
-  vpc_id = "${data.aws_vpc.foreign_vpc.id}" # Assumes only one sg for vpc
-}
-  source_security_group_id = "${data.aws_security_group.foreign_sg.id}"
BREAKS HERE
-
-# USERS
-resource "aws_iam_user" "example_admin" {
-  name          = "Example Administrator"
-  force_destroy = true
-}
-
-resource "aws_iam_user_group_membership" "example_admin" {
-  user = "${aws_iam_user.example_admin.name}"
-
-  groups = [
-    "${aws_iam_group.infosec_admins.name}",
-    "${aws_iam_group.prod_admins.name}",
-    "${aws_iam_group.non_prod_admins.name}",
-  ]
-}
-
-resource "aws_iam_user" "example_dev" {
-  name          = "Example Developer"
-  force_destroy = true
-}
-
-resource "aws_iam_user_group_membership" "example_dev" {
-  user = "${aws_iam_user.example_dev.name}"
-
-  groups = [
-    "${aws_iam_group.prod_developers.name}",
-    "${aws_iam_group.non_prod_developers.name}",
-  ]
-}
BREAKS HERE
-  additional_instance_tags_public_services_dc   = "${var.additional_instance_tags_sample}"
-  additional_instance_tags_private_services_dc  = "${var.additional_instance_tags_sample}"
-  additional_instance_tags_backoffice_dc        = "${var.additional_instance_tags_sample}"
-  additional_instance_tags_content_connector_dc = "${var.additional_instance_tags_sample}"
BREAKS HERE
-  log_destination = "${var.vpc_flow_logs_group_arn}"
BREAKS HERE
-
-  lifecycle {
-    prevent_destroy = "${var.protected}"
-  }
BREAKS HERE
-  cidr_blocks              = "${var.allowed_cidr}"
BREAKS HERE
-  subnet_id = "${aws_subnet.quorum_observer.id}"
-  subnet_id = "${aws_subnet.quorum_validator.id}"
-  subnet_id = "${aws_subnet.quorum_maker.id}"
BREAKS HERE
-  subnets                          = ["${var.subnets_elb}"]
-    healthy_threshold   = "${var.elb_healthy_threshold}"
-    unhealthy_threshold = "${var.elb_unhealthy_threshold}"
-    interval            = "${var.elb_interval}"
-    port                = "${var.elb_healthcheck_port}"
-    healthy_threshold   = "${var.elb_healthy_threshold}"
-    unhealthy_threshold = "${var.elb_unhealthy_threshold}"
-    interval            = "${var.elb_interval}"
-    port                = "${var.elb_healthcheck_port}"
BREAKS HERE
-  # apply amazon linux patces
-    content = <<EOF
-#!/bin/bash
-yum update -y
-EOF
-  # install awslogs and setup
-    content = "${data.template_file.zookeeper_userdata_awslogs.rendered}"
BREAKS HERE
-
BREAKS HERE
-// Reattach default subnets to the default netowrk ACL to avoid the known issue below.
-  subnet_ids             = ["${aws_default_subnet.default.*.id}"]
BREAKS HERE
-    source = "../instances"
-    label_prefix = "${var.label_prefix}"
-    node_type = "${var.node_type}"
-    node_count = "1"
-    node_class = "node"
-    group = "${var.group}"
-    private_ip = "true"
-    count = "${var.node_count}"
-    provisioner "remote-exec" {
-        inline = [
-            "set -e",
-            "export PATH=$${PATH}:/opt/bin",
-            "sudo ${data.external.kubeadm_join.result.command}",
-            "chmod +x /tmp/end.sh && sudo /tmp/end.sh",
-        ]
-        connection {
-            host = "${element(module.node.*.public_ip, var.node_count)}"
-            user    = "core"
-            timeout = "300s"
-        }
-}
BREAKS HERE
-  enabled            = "${var.codepipeline_enabled}"
-  source             = "git::https://github.com/cloudposse/terraform-aws-ecs-codepipeline.git?ref=tags/0.2.0"
-  name               = "${var.name}"
-  namespace          = "${var.namespace}"
-  stage              = "${var.stage}"
-  attributes         = "${var.attributes}"
-  github_oauth_token = "${var.github_oauth_token}"
-  repo_owner         = "${var.repo_owner}"
-  repo_name          = "${var.repo_name}"
-  branch             = "${var.branch}"
-  image_repo_name    = "${module.ecr.repository_name}"
-  service_name       = "${module.ecs_alb_service_task.service_name}"
-  ecs_cluster_name   = "${var.ecs_cluster_name}"
-  privileged_mode    = "true"
BREAKS HERE
-  value = "${module.lbaas.vip_address}"
BREAKS HERE
-  target_tags       = ["nat-${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"]
-  name              = "nat-gateway-${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"
-  name                   = "nat-${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"
-  tags                   = ["${compact(concat(list("nat-${var.region}"), var.tags))}"]
-  name    = "nat-${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"
-  source_tags = ["${compact(concat(list("nat-${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"), var.tags))}"]
-  target_tags = ["${compact(concat(list("nat-${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"), var.tags))}"]
-  name = "nat-${var.zone == "" ? lookup(var.region_params["${var.region}"], "zone") : var.zone}"
BREAKS HERE
-      "/tmp/terraform/create_application.sh ${var.region} ${aws_vpc.main.id} ${var.base_iam_role_name} ${var.username}-${var.prefix}-${var.vpc_name} ${aws_security_group.example_app.id} ${aws_security_group.vpc_sg.id} ${aws_security_group.mgmt_sg.id}"
BREAKS HERE
-  role        = "${aws_iam_role.ecs-role.name}"
-resource "aws_iam_role" "ecs-role" {
-resource "aws_iam_policy" "ecs-policy" {
-resource "aws_iam_policy" "custom-ecs-policy" {
-resource "aws_iam_policy_attachment" "attach-ecs" {
-  roles      = ["${aws_iam_role.ecs-role.name}"]
-  policy_arn = "${element(concat(aws_iam_policy.ecs-policy.*.arn, aws_iam_policy.custom-ecs-policy.*.arn), 0)}"
BREAKS HERE
-  value       = "${element(concat(aws_db_instance.this.*.address, list("")), 0)}"
-  value       = "${element(concat(aws_db_instance.this.*.arn, list("")), 0)}"
-  value       = "${element(concat(aws_db_instance.this.*.availability_zone, list("")), 0)}"
-  value       = "${element(concat(aws_db_instance.this.*.endpoint, list("")), 0)}"
-  value       = "${element(concat(aws_db_instance.this.*.hosted_zone_id, list("")), 0)}"
-  value       = "${element(concat(aws_db_instance.this.*.id, list("")), 0)}"
-  value       = "${element(concat(aws_db_instance.this.*.resource_id, list("")), 0)}"
-  value       = "${element(concat(aws_db_instance.this.*.status, list("")), 0)}"
-  value       = "${element(concat(aws_db_instance.this.*.name, list("")), 0)}"
-  value       = "${element(concat(aws_db_instance.this.*.username, list("")), 0)}"
-  value       = "${var.password}"
-  value       = "${element(concat(aws_db_instance.this.*.port, list("")), 0)}"
BREAKS HERE
-  zone_id = "${openstack_dns_zone_v2.tectonic.id}"
-  zone_id = "${openstack_dns_zone_v2.tectonic.id}"
BREAKS HERE
-  }
-
-  workers_group_defaults = "${merge(local.workers_group_defaults_defaults, var.workers_group_defaults)}"
-  workers_group_launch_template_defaults_defaults = {
-    name                                     = "count.index"                                 # Name of the worker group. Literal count.index will never be used but if name is not set, the count.index interpolation will be used.
-    ami_id                                   = "${data.aws_ami.eks_worker.id}"               # AMI ID for the eks workers. If none is provided, Terraform will search for the latest version of their EKS optimized worker AMI.
-    root_block_device_id                     = "${data.aws_ami.eks_worker.root_device_name}" # Root device name for workers. If non is provided, will assume default AMI was used.
-    asg_desired_capacity                     = "1"                                           # Desired worker capacity in the autoscaling group.
-    asg_max_size                             = "3"                                           # Maximum worker capacity in the autoscaling group.
-    asg_min_size                             = "1"                                           # Minimum worker capacity in the autoscaling group.
-    asg_force_delete                         = false                                         # Enable forced deletion for the autoscaling group.
-    instance_type                            = "m4.large"                                    # Size of the workers instances.
-    override_instance_type                   = "t3.large"                                    # Need to specify at least one additional instance type for mixed instances policy. The instance_type holds  higher priority for on demand instances.
-    on_demand_allocation_strategy            = "prioritized"                                 # Strategy to use when launching on-demand instances. Valid values: prioritized.
-    on_demand_base_capacity                  = "0"                                           # Absolute minimum amount of desired capacity that must be fulfilled by on-demand instances
-    on_demand_percentage_above_base_capacity = "100"                                         # Percentage split between on-demand and Spot instances above the base on-demand capacity
-    spot_allocation_strategy                 = "lowest-price"                                # The only valid value is lowest-price, which is also the default value. The Auto Scaling group selects the cheapest Spot pools and evenly allocates your Spot capacity across the number of Spot pools that you specify.
-    spot_instance_pools                      = 10                                            # "Number of Spot pools per availability zone to allocate capacity. EC2 Auto Scaling selects the cheapest Spot pools and evenly allocates Spot capacity across the number of Spot pools that you specify."
-    spot_max_price                           = ""                                            # Maximum price per unit hour that the user is willing to pay for the Spot instances. Default is the on-demand price
-    spot_price                               = ""                                            # Cost of spot instance.
-    placement_tenancy                        = "default"                                     # The tenancy of the instance. Valid values are "default" or "dedicated".
-    root_volume_size                         = "100"                                         # root volume size of workers instances.
-    root_volume_type                         = "gp2"                                         # root volume type of workers instances, can be 'standard', 'gp2', or 'io1'
-    root_iops                                = "0"                                           # The amount of provisioned IOPS. This must be set with a volume_type of "io1".
-    root_encrypted                           = ""                                            # root volume encryption for workers.
-    kms_key_id                               = ""                                            # KMS key ID used for encrypted block device. ASG must have access to this key. If not specified, the default KMS key will be used.
-    key_name                                 = ""                                            # The key name that should be used for the instances in the autoscaling group
-    pre_userdata                             = ""                                            # userdata to pre-append to the default userdata.
-    bootstrap_extra_args                     = ""                                            # Extra arguments passed to the bootstrap.sh script from the EKS AMI.
-    additional_userdata                      = ""                                            # userdata to append to the default userdata.
-    ebs_optimized                            = true                                          # sets whether to use ebs optimization on supported types.
-    enable_monitoring                        = true                                          # Enables/disables detailed monitoring.
-    public_ip                                = false                                         # Associate a public ip address with a worker
-    eni_delete                               = true                                          # Delete the ENI on termination (if set to false you will have to manually delete before destroying)
-    kubelet_extra_args                       = ""                                            # This string is passed directly to kubelet if set. Useful for adding labels or taints.
-    subnets                                  = "${join(",", var.subnets)}"                   # A comma delimited string of subnets to place the worker nodes in. i.e. subnet-123,subnet-456,subnet-789
-    autoscaling_enabled                      = false                                         # Sets whether policy and matching tags will be added to allow autoscaling.
-    additional_security_group_ids            = ""                                            # A comma delimited list of additional security group ids to include in worker launch config
-    protect_from_scale_in                    = false                                         # Prevent AWS from scaling in, so that cluster-autoscaler is solely responsible.
-    iam_instance_profile_name                = ""                                            # A custom IAM instance profile name. Used when manage_worker_iam_resources is set to false. Incompatible with iam_role_id.
-    iam_role_id                              = "${local.default_iam_role_id}"                # A custom IAM role id. Incompatible with iam_instance_profile_name.
-    suspended_processes                      = "AZRebalance"                                 # A comma delimited string of processes to to suspend. i.e. AZRebalance,HealthCheck,ReplaceUnhealthy
-    target_group_arns                        = ""                                            # A comma delimited list of ALB target group ARNs to be associated to the ASG
-    enabled_metrics                          = ""                                            # A comma delimited list of metrics to be collected i.e. GroupMinSize,GroupMaxSize,GroupDesiredCapacity
-    placement_group                          = ""                                            # The name of the placement group into which to launch the instances, if any.
-    service_linked_role_arn                  = ""                                            # Arn of custom service linked role that Auto Scaling group will use. Useful when you have encrypted EBS
-  workers_group_launch_template_defaults = "${merge(local.workers_group_launch_template_defaults_defaults, var.workers_group_launch_template_defaults)}"
BREAKS HERE
-  source     = "git::https://github.com/cloudposse/tf_label.git?ref=tags/0.2.0"
-    from_port       = "${var.port}"                    # Redis
-  tags   = "${module.label.tags}"
-  tags                          = "${module.label.tags}"
-  threshold           = "${var.alarm_cpu_threshold_percent}"
-  alarm_actions       = ["${var.alarm_actions}"]
-  depends_on          = ["aws_elasticache_replication_group.default"]
-  tags                = "${module.label.tags}"
-  threshold           = "${var.alarm_memory_threshold_bytes}"
-  alarm_actions       = ["${var.alarm_actions}"]
-  depends_on          = ["aws_elasticache_replication_group.default"]
-  tags                = "${module.label.tags}"
-
-  source    = "git::https://github.com/cloudposse/tf_hostname.git?ref=tags/0.1.0"
BREAKS HERE
-    preemptible     = "${lookup(var.node_pool[count.index], "preemptible", false)}"
-    machine_type    = "${lookup(var.node_pool[count.index], "machine_type", "n1-standard-1")}"
BREAKS HERE
-# these provide an empty file for data sources to read without exploding
-resource "local_file" "stdout" {
-  content  = ""
-  filename = "${path.module}/stdout.${null_resource.start.id}"
-}
-
-resource "local_file" "stderr" {
-  content  = ""
-  filename = "${path.module}/stderr.${null_resource.start.id}"
-}
-
-resource "local_file" "exitstatus" {
-  content  = ""
-  filename = "${path.module}/exitstatus.${null_resource.start.id}"
-}
-
-# this overwrites local_files
-  depends_on = ["local_file.stdout", "local_file.stderr", "local_file.exitstatus"]
-
-# on the first apply these will get the overridden contents
-data "local_file" "stdout" {
-  filename   = "${path.module}/stdout.${null_resource.start.id}"
-  depends_on = ["null_resource.shell", "local_file.stdout"]
-data "local_file" "stderr" {
-  filename   = "${path.module}/stderr.${null_resource.start.id}"
-  depends_on = ["null_resource.shell", "local_file.stderr"]
-data "local_file" "exitstatus" {
-  filename   = "${path.module}/exitstatus.${null_resource.start.id}"
-  depends_on = ["null_resource.shell", "local_file.exitstatus"]
-# first apply stores contents and then ignores the later empty contents
-    stdout     = "${data.local_file.stdout.content}"
-    stderr     = "${data.local_file.stderr.content}"
-    exitstatus = "${data.local_file.exitstatus.content}"
-
-output "stdout" {
-  value = "${chomp(null_resource.contents.triggers["stdout"])}"
-}
-
-output "stderr" {
-  value = "${chomp(null_resource.contents.triggers["stderr"])}"
-}
-
-output "exitstatus" {
-  value = "${chomp(null_resource.contents.triggers["exitstatus"])}"
-}
BREAKS HERE
-  count = var.create_vpc && length(var.public_subnets) > 0 && false == var.one_nat_gateway_per_az || length(var.public_subnets) >= length(var.azs) ? length(var.public_subnets) : 0
-  count = var.create_vpc && var.propagate_public_route_tables_vgw && var.enable_vpn_gateway || var.vpn_gateway_id != "" ? 1 : 0
-  count = var.create_vpc && var.propagate_private_route_tables_vgw && var.enable_vpn_gateway || var.vpn_gateway_id != "" ? length(var.private_subnets) : 0
BREAKS HERE
-  records = ["10 30288227.in1.mandrillapp.com", "20 30288227.in2.mandrillapp.com"]
-  records = ["10 30288227.in1.mandrillapp.com", "20 30288227.in2.mandrillapp.com"]
-  records = ["10 30288227.in1.mandrillapp.com", "20 30288227.in2.mandrillapp.com"]
-  records = ["10 30288227.in1.mandrillapp.com", "20 30288227.in2.mandrillapp.com"]
-  records = ["10 30288227.in1.mandrillapp.com", "20 30288227.in2.mandrillapp.com"]
BREAKS HERE
-  default = [""]
-  default = [""]
BREAKS HERE
-    source_image = "${var.source_image}"
-    source_image = "${var.source_image}"
BREAKS HERE
-    cidr_blocks = [ "${ var.cidr-vpc }" ]
BREAKS HERE
-  name               = "${local.cluster_type}-cluster"
-  subnetwork         = "${var.subnetwork}"
-  ip_range_pods      = "${var.ip_range_pods}"
BREAKS HERE
-
BREAKS HERE
-#provider "aws" {
-#  region  = "${var.aws_region}"
-#  profile = "${var.aws_master_profile}"
-#}
-
-resource "aws_guardduty_detector" "master" {
-  detector_id        = "${aws_guardduty_detector.master.id}"
-  detector_id        = "${aws_guardduty_detector.master.id}"
-  detector_id        = "${aws_guardduty_detector.master.id}"
-  detector_id        = "${aws_guardduty_detector.master.id}"
-  detector_id        = "${aws_guardduty_detector.master.id}"
-  detector_id        = "${aws_guardduty_detector.master.id}"
-  detector_id        = "${aws_guardduty_detector.master.id}"
-  detector_id        = "${aws_guardduty_detector.master.id}"
BREAKS HERE
-  website_endpoints = [
-    "${aws_s3_bucket.redirect.website_endpoint}",
-    "${aws_s3_bucket.main.website_endpoint}",
-    domain_name = "${element(local.website_endpoints, count.index)}"
-    custom_origin_config {
-      http_port                = "80"
-      https_port               = "443"
-      origin_keepalive_timeout = 5
-      origin_protocol_policy   = "http-only"
-      origin_ssl_protocols     = ["TLSv1", "TLSv1.1", "TLSv1.2"]
BREAKS HERE
-data "aws_region" "current" {
-  current = true
-}
BREAKS HERE
-  name = "${var.prefix}-k8sbook-${var.chap}-sp-aks-${var.cluster_type}"
-resource "null_resource" "aadsync_delay" {
-  // Wait for AAD async global replication
-  provisioner "local-exec" {
-    command = "sleep 90"
-  }
-
-  triggers = {
-    "before" = "${azuread_service_principal_password.aks.id}"
-  }
-}
-
-  depends_on = ["null_resource.aadsync_delay"]
-
-    client_secret = "${azuread_service_principal_password.aks.value}"
BREAKS HERE
-  private_ip                  = var.private_ip
-  ipv6_address_count          = var.ipv6_address_count
-  ipv6_addresses              = var.ipv6_addresses
-  private_ip                  = var.private_ip
-  ipv6_address_count          = var.ipv6_address_count
-  ipv6_addresses              = var.ipv6_addresses
BREAKS HERE
-
-  # user_data          = "${var.user_data}"
-  count = "${var.total_instances}"
-  name  = "${format("%s-%02d.%s.%s", var.name, count.index + 1, var.region, var.domain)}"
BREAKS HERE
-    command = "${local.command_chomped} 2>${path.module}/stderr.${self.id} >${path.module}/stdout.${self.id}; echo $? >${path.module}/exitstatus.${self.id}"
-    command = "rm ${path.module}/stdout.${self.id}"
-    command = "rm ${path.module}/stderr.${self.id}"
-    command = "rm ${path.module}/exitstatus.${self.id}"
BREAKS HERE
-  count = "${signum(length(var.codebuild_project))}"
BREAKS HERE
-#========================== demo VPC =============================
BREAKS HERE
-resource "aws_default_subnet" "default" {
-  count = "${length(data.aws_availability_zones.available.names)}"
-
-  availability_zone = "${data.aws_availability_zones.available.names[count.index]}"
-}
-
BREAKS HERE
-  source            = "../../modules/ec2-nat-instance"
-  name               = "${var.name}-web"
BREAKS HERE
-  activate_apis       = [
-    "container.googleapis.com"
BREAKS HERE
-    group                 = "${element(google_compute_instance_group.webservers.*.self_link, 0)}"
-    group                 = "${element(google_compute_instance_group.webservers.*.self_link, 1)}"
-  name               = "${var.name}-instance-group-manager-${var.count}"
-  target_pools       = [ "${google_compute_target_pool.webserver.self_link}" ]
-resource "google_compute_target_pool" "webserver" {
-  name          = "${var.name}-instance-pool"
-  project       = "${var.project}"
-  health_checks = [ "${google_compute_http_health_check.healthcheck.name}" ]
-}
-
-resource "google_compute_instance_group" "webservers" {
-  name    = "${var.name}-webservers-instance-group-${count.index}"
-  project = "${var.project}"
-  count   = "${var.count}"
-  zone    = "${element(var.zones, count.index)}"
-}
-
-  name    = "${var.name}-scaler"
BREAKS HERE
-resource "aws_instance" "test_instance_ric03uec_centos7_01" {
-  vpc_security_group_ids = [
-   "${aws_security_group.sg_private_ship_builds.id}"]
-
-  root_block_device {
-    volume_type = "gp2"
-    volume_size = 50
-    delete_on_termination = true
-  }
-
-  tags = {
-    Name = "test_instance_ric03uec_centos7_01_${var.install_version}"
-  }
-}
-
-output "test_instance_ric03uec_centos7_01" {
-  value = "${aws_instance.test_instance_ric03uec_centos7_01.private_ip}"
-}
-
-resource "aws_instance" "test_instance_ric03uec_centos7_02" {
-  ami = "${var.ami_us_east_1_centos7}"
-  availability_zone = "${var.avl-zone}"
-  instance_type = "${var.in_type_core}"
-  key_name = "${var.aws_key_name}"
-  subnet_id = "${aws_subnet.sn_ship_install.id}"
-    volume_size = 50
-    Name = "test_instance_ric03uec_centos7_02_${var.install_version}"
-output "test_instance_ric03uec_centos7_02" {
-  value = "${aws_instance.test_instance_ric03uec_centos7_02.private_ip}"
BREAKS HERE
-  source     = "git::https://github.com/cloudposse/tf_label.git?ref=tags/0.2.0"
BREAKS HERE
-    actions   = ["route53:ChangeResourceRecordSets"]
-    resources = ["arn:aws:route53:::hostedzone/${data.terraform_remote_state.cluster.hosted_zone_id}"]
BREAKS HERE
-resource "aws_cloudwatch_metric_alarm" "service_low" {
BREAKS HERE
-    actions   = "ssm:DescribeParameters"
BREAKS HERE
-  target_group_health_check_path = "/healthcheck"
BREAKS HERE
-  name               = "rabbitmq-${var.name}"
-  name = "rabbitmq-${var.name}"
-  name_prefix = "rabbitmq-${var.name}"
-  name        = "rabbitmq-${var.name}-nodes"
-  name_prefix          = "rabbitmq-${var.name}-"
-  name_prefix               = "rabbitmq-${var.name}-"
-    value               = "rabbitmq-${var.name}"
-  name = "rabbitmq-${var.name}-elb"
-    Name = "rabbitmq-${var.name}"
BREAKS HERE
-  source     = "git::https://github.com/cloudposse/tf_label.git?ref=tags/0.2.0"
-  source    = "git::https://github.com/cloudposse/tf_hostname.git?ref=tags/0.1.0"
BREAKS HERE
-resource "aws_route53_record" "18f_gov_federalist-landing-template-proxied_18f_gov_cname" {
-  name = "federalist-landing-template-proxied.18f.gov."
-resource "aws_route53_record" "18f_gov_federalist-landing-proxied_18f_gov_cname" {
-  name = "federalist-landing-proxied.18f.gov."
BREAKS HERE
-    zone_id = "Z2FDTNDATAQYW2"
-    zone_id = "Z2FDTNDATAQYW2"
-    zone_id = "Z2FDTNDATAQYW2"
BREAKS HERE
- * Note that there is a peculiarity with the EBS volume in that it
- * requires some manual setup the very first time to make it available
- * for use (unless a snapshot id is supplied):
- *
- * parted --script /dev/xvdf -- mklabel msdos
- * parted --script /dev/xvdf -- mkpart primary 0 -1
- * mkfs -t ext4 -F /dev/xvdf1
- * e2label /dev/xvdf1 gitlab
- *
- * After running the above code to initialise the EBS, terminate the instance
- * and the autoscaling group will bring up a new instance that will be running
- * gitlab once it is done initialising.
-variable "name" {
-  default = "gitlab-asg-test"
-}
-
-variable "region" {
-  default = "us-east-1"
-}
-
-variable "ssh_pubkey" {
-  default     = "./id_rsa.pub"
-  description = "The path to the SSH pub key to use"
-}
-
-variable "dns_zone_name" {
-  description = "The name of the DNS zone on Route53 (example.com), to create records in for gitlab"
-  type        = "string"
-}
-
-variable "ssl_arn" {
-  description = "The ARN of the SSL cert, see 'make upload-tls-certs'"
-  type        = "string"
-}
-
-variable "gitlab_name" {
-  description = "To generate the DNS record for gitlab, prefix the zone"
-  default     = "gitlab"
-  type        = "string"
-}
-
-variable "gitlab_registry_name" {
-  description = "To generate the DNS record for the docker registry, prefix the zone"
-  default     = "registry"
-  type        = "string"
-}
-
-variable "root_volume_size" {
-  default     = "30"
-  description = "GB of root data volume for the instance, make it larger than usual for docker builds"
-}
-
-variable "registry_bucket_name" {
-  description = "The name of the S3 bucket to write docker images to"
-  type        = "string"
-}
-
-resource "aws_elb" "gitlab" {
-  name            = "${var.name}"
-  subnets         = ["${module.vpc.public_subnet_ids[0]}"]
-  security_groups = ["${aws_security_group.gitlab-elb.id}"]
-
-  listener {
-    instance_port     = 8022
-    instance_protocol = "tcp"
-    lb_port           = 22
-    lb_protocol       = "tcp"
-  }
-
-  listener {
-    instance_port      = 80
-    instance_protocol  = "http"
-    lb_port            = 443
-    lb_protocol        = "https"
-    ssl_certificate_id = "${var.ssl_arn}"
-  }
-
-  health_check {
-    healthy_threshold   = 2
-    unhealthy_threshold = 2
-    timeout             = 3
-    target              = "TCP:80"
-    interval            = 30
-  }
-
-  tags {
-    Name = "${var.name}"
-  }
-resource "aws_security_group" "gitlab-elb" {
-  name        = "gitlab-elb"
-  vpc_id      = "${module.vpc.vpc_id}"
-  description = "Security group for the gitlab ELB"
-}
-module "elb-http-rule" {
-  source            = "../../modules/single-port-sg"
-  port              = 80
-  description       = "Allow ingress for HTTP, port 80 (TCP), thru the ELB"
-  cidr_blocks       = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.gitlab-elb.id}"
-
-module "elb-https-rule" {
-  source            = "../../modules/single-port-sg"
-  port              = 443
-  description       = "Allow ingress for HTTPS, port 443 (TCP), thru the ELB"
-  cidr_blocks       = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.gitlab-elb.id}"
-}
-
-module "elb-gitlab-ssh-rule" {
-  source            = "../../modules/single-port-sg"
-  port              = 22
-  description       = "Allow ingress for Git over SSH, port 22 (TCP), thru the ELB"
-  cidr_blocks       = ["0.0.0.0/0"]
-  security_group_id = "${aws_security_group.gitlab-elb.id}"
-}
-
-module "elb-open-egress-rule" {
-  source            = "../../modules/open-egress-sg"
-  security_group_id = "${aws_security_group.gitlab-elb.id}"
-  load_balancers        = ["${aws_elb.gitlab.name}"]
-  type    = "CNAME"
-  records = ["${aws_elb.gitlab.dns_name}"]
-  type    = "CNAME"
-  records = ["${aws_elb.gitlab.dns_name}"]
-##################
-## Outputs
-
-output "region" {
-  value       = "${var.region}"
-  description = "region deployed to"
-}
-
-output "gitlab_asg_name" {
-  value       = "${var.name}-gitlab-asg-${element(data.aws_availability_zones.available.names, 0)}"
-  description = "name of the Gitlab autoscaling group"
-}
-
-output "gitlab_server_name" {
-  value       = "${var.name}-gitlab-server-${element(data.aws_availability_zones.available.names, 0)}"
-  description = "name of the Gitlab server instance"
-}
-
-output "gitlab_url" {
-  value       = "${aws_route53_record.gitlab.name}"
-  description = "URL to gitlab"
-}
-
-output "registry_url" {
-  value       = "${aws_route53_record.registry.name}"
-  description = "URL to docker image registry"
-}
-
-// URL to S3 bucket where Docker images are stored
-output "registry_bucket_url" {
-  value = "${module.docker-registry-s3-storage.url}"
-}
-
-// Name of the S3 bucket where Docker images are stored
-output "registry_bucket_name" {
-  value = "${module.docker-registry-s3-storage.bucket_id}"
-}
BREAKS HERE
-  nomad_server_sg_id     = "${aws_security_group.sg_sample.id}"
-  consul_server_sg_id    = "${aws_security_group.sg_sample.id}"
BREAKS HERE
-    DOCKER_REGISTRY_SERVER_URL      = "${var.docker_registry_server_url}"
-    DOCKER_REGISTRY_SERVER_USERNAME = "${var.docker_registry_server_username}"
-    DOCKER_REGISTRY_SERVER_PASSWORD = "${var.docker_registry_server_password}"
-    APPINSIGHTS_INSTRUMENTATIONKEY  = "${var.app_insights_instrumentation_key}"
-    KEYVAULT_URI                    = "${var.vault_uri}"
-  count               = "${length(keys(var.app_service_name))}"
BREAKS HERE
-  version = "1.0.1"
BREAKS HERE
-variable kubenow_image_id {}
-variable master_ip {}
-# create bootstrap script file from template
-resource "template_file" "node_bootstrap" {
-  template = "${file("${path.root}/../bootstrap/node.sh")}"
-resource "aws_instance" "node" {
-  ami = "${var.kubenow_image_id}"
-  user_data = "${template_file.node_bootstrap.rendered}"
BREAKS HERE
-
-  alias {
-    name = "gsa-elb-ecs-stg-wild-diggov-1-1092638291.us-east-1.elb.amazonaws.com."
-    zone_id = "Z2FDTNDATAQYW2"
-    evaluate_target_health = false
-  }
BREAKS HERE
-resource "aws_elb" "k8s-api-elb" {
-  name = "haystack-k8s-api-elb"
-resource "aws_elb" "k8s-nodes-elb" {
-  name = "haystack-k8s-nodes-elb"
-resource "aws_route53_record" "k8s-api-elb-route53" {
-    "${aws_elb.k8s-api-elb.dns_name}"]
-resource "aws_route53_record" "k8s-nodes-elb-route53" {
-    "${aws_elb.k8s-nodes-elb.dns_name}"]
-    "aws_autoscaling_attachment.master-1-masters-haystack-k8s",
-    "aws_autoscaling_attachment.master-2-masters-haystack-k8s",
-    "aws_autoscaling_attachment.master-3-masters-haystack-k8s",
-    "aws_route53_record.k8s-api-elb-route53",
-    "aws_autoscaling_attachment.nodes-haystack-k8s"]
-resource "aws_autoscaling_attachment" "master-1-masters-haystack-k8s" {
-  elb = "${aws_elb.k8s-api-elb.id}"
-resource "aws_autoscaling_attachment" "master-2-masters-haystack-k8s" {
-  elb = "${aws_elb.k8s-api-elb.id}"
-resource "aws_autoscaling_attachment" "master-3-masters-haystack-k8s" {
-  elb = "${aws_elb.k8s-api-elb.id}"
-resource "aws_autoscaling_attachment" "nodes-haystack-k8s" {
-  elb = "${aws_elb.k8s-nodes-elb.id}"
BREAKS HERE
-      type = "ssh"
-      user = "${var.ssh_user}"
-      host = "${scaleway_server.swarm_manager.public_ip}"
BREAKS HERE
-    name                 = "count.index"                   # Name of the worker group. Literal count.index will never be used but if name is not set, the count.index interpolation will be used.
-    ami_id               = "${data.aws_ami.eks_worker.id}" # AMI ID for the eks workers. If none is provided, Terraform will search for the latest version of their EKS optimized worker AMI.
-    asg_desired_capacity = "1"                             # Desired worker capacity in the autoscaling group.
-    asg_max_size         = "3"                             # Maximum worker capacity in the autoscaling group.
-    asg_min_size         = "1"                             # Minimum worker capacity in the autoscaling group.
-    instance_type        = "m4.large"                      # Size of the workers instances.
-    spot_price           = ""                              # Cost of spot instance.
-    root_volume_size     = "100"                           # root volume size of workers instances.
-    root_volume_type     = "gp2"                           # root volume type of workers instances, can be 'standard', 'gp2', or 'io1'
-    root_iops            = "0"                             # The amount of provisioned IOPS. This must be set with a volume_type of "io1".
-    key_name             = ""                              # The key name that should be used for the instances in the autoscaling group
-    pre_userdata         = ""                              # userdata to pre-append to the default userdata.
-    additional_userdata  = ""                              # userdata to append to the default userdata.
-    ebs_optimized        = true                            # sets whether to use ebs optimization on supported types.
-    enable_monitoring    = true                            # Enables/disables detailed monitoring.
-    public_ip            = false                           # Associate a public ip address with a worker
-    kubelet_extra_args   = ""                              # This string is passed directly to kubelet if set. Useful for adding labels or taints.
-    subnets              = ""                              # A comma delimited string of subnets to place the worker nodes in. i.e. subnet-123,subnet-456,subnet-789
-    autoscaling_enabled  = false                           # Sets whether policy and matching tags will be added to allow autoscaling.
BREAKS HERE
-resource "docker_image" "alertmanager" {
-  name          = "${data.docker_registry_image.alertmanager.name}"
-  pull_triggers = ["${data.docker_registry_image.alertmanager.sha256_digest}"]
-}
BREAKS HERE
-    "error",
-    "slowquery",
BREAKS HERE
-  records = ["pages-redirects.app.cloud.gov."]
BREAKS HERE
-  access_key = "${var.aws_databricks_acct_access_key}"
-  secret_key = "${var.aws_databricks_acct_secret_key}"
-  region = "${var.aws_databrics_acct_region}"
BREAKS HERE
-  count                     = length(data.aws_route_tables.peer_vpc_rts.ids)
BREAKS HERE
-  tags = "${merge(map("Name", var.instance_count > 1 ? format("%s-%d", var.name, count.index+1) : var.name), var.tags)}"
-  tags = "${merge(map("Name", var.instance_count > 1 ? format("%s-%d", var.name, count.index+1) : var.name), var.tags)}"
BREAKS HERE
-  alias  = "${var.region}"
-
-  providers = {
-    "openstack" = "openstack.${var.region}"
-  }
BREAKS HERE
-    ports    = ["${element(split(",", element(var.backend_params, count.index)), 2)}"]
BREAKS HERE
-variable "backend_pool_ids" {
-  type = "list"
-}
-
BREAKS HERE
-  scope                = "${var.subscription_id}"
BREAKS HERE
-  description = "List of extenral etcd v3 servers to connect with (scheme://ip:port). Optionally use if providing external etcd."
BREAKS HERE
-module "keyvault_access_policy_default" "default" {
-  vault_name          = "${module.keyvault.keyvault_name}"
-  resource_group_name = "${var.global_resource_group_name}"
-  tenant_id           = "${data.azurerm_client_config.current.tenant_id}"
-  object_id           = "${data.azurerm_client_config.current.service_principal_object_id}"
-module "keyvault_access_policy_aks" "aks" {
-  vault_name          = "${module.keyvault.keyvault_name}"
-  resource_group_name = "${var.global_resource_group_name}"
-  tenant_id           = "${data.azurerm_client_config.current.tenant_id}"
-  object_id           = "${var.service_principal_id}"
-  key_permissions     = ["get"]
-  secret_permissions  = ["get"]
BREAKS HERE
-  kubeconfig_recreate      = ""
BREAKS HERE
-variable "git_ssh_command" {
-    default = "undefined"
-}
-
BREAKS HERE
-variable extra_disk_size { default=0 }
-variable extra_disk_type { default="pd-ssd" }
-variable extra_disk_name { default="extra-disk" }
-# Create extra disk (always due to limitation with Terraform GCE modules)
-resource "google_compute_disk" "extra_standard_disk" {
-  count = "${var.count}"
-  name = "${var.name_prefix}-extra-${format("%03d", count.index)}"
-  type = "${var.extra_disk_type}"
-  zone = "${var.zone}"
-  size = "${var.extra_disk_size <= 0 ? 1 : var.extra_disk_size}"
-}
-
-# Instance with extra disk
-
-  # Extra disk
-  disk {
-    disk = "${element(google_compute_disk.extra_standard_disk.*.name, count.index)}"
-    device_name = "${var.extra_disk_name}"
-    auto_delete = true
-  }
-  value = ["${list("google-${var.extra_disk_name}")}"]
BREAKS HERE
-    {
-        "Version": "2012-10-17",
-        "Statement": [
-            {
-                "Effect": "Allow",
-                "Action": "iam:CreateServiceLinkedRole",
-                "Resource": "arn:aws:iam::*:role/aws-service-role/*"
-            },
-            {
-                "Effect": "Allow",
-                "Action": [
-                    "ec2:DescribeAccountAttributes"
-                ],
-                "Resource": "*"
-            }
-        ]
-    }
BREAKS HERE
-    source      = "${path.module}/templates/wireguard@.service"
-    destination = "/etc/systemd/system/wireguard@.service"
-  }
-
-  provisioner "file" {
-      "systemctl is-enabled wireguard@${var.vpn_interface}.service || systemctl enable wireguard@${var.vpn_interface}.service",
-      "systemctl restart wireguard@${var.vpn_interface}.service",
-  value      = "wireguard@${var.vpn_interface}.service"
BREAKS HERE
-  value       = "${azurerm_app_service.appsvc.*.default_site_hostname}"
BREAKS HERE
-  resource_path           = "/"
BREAKS HERE
-    user_data           = "${base64encode(data.template_file.etcd-bootstrap.rendered)}"
-    tags                = "group:etcd"
BREAKS HERE
-#  
-#  
-#  
BREAKS HERE
-resource "aws_autoscaling_group" "cluster" {
-resource "aws_autoscaling_lifecycle_hook" "cluster_initial_hook" {
-  count                  = "${var.enable_init_hook == true ? 1 : 0}"
-  autoscaling_group_name = "${aws_autoscaling_group.cluster.name}"
-  name                   = "${coalesce(var.override_launch_hook_name, format("%s-%s-launch", var.cluster_name, var.dcos_role))}"
-  default_result         = "CONTINUE"
-  heartbeat_timeout      = "${var.asg_wait_time}"
-  lifecycle_transition   = "autoscaling:EC2_INSTANCE_LAUNCHING"
BREAKS HERE
-  count          = "${length(var.route_tables) >0 ? "${var.num_subnets}" : 0 }"
BREAKS HERE
-  count = "${length(var.nat_zones) * var.nat_count_per_zone}"
-  instance_template  = "${element(google_compute_instance_template.nat.*.self_link, count.index)}"
-  update_strategy    = "NONE"
BREAKS HERE
-  triggers {
-    instance_id = "${element(aws_instance.instance.*.id, count.index)}"
-  }
-
BREAKS HERE
-    id = "${null_resource.host_salt_configuration.0.id}"
-    address = "${openstack_networking_floatingip_v2.floating_ip.0.address}"
BREAKS HERE
-  depends_on = ["module.master_instance", "null_resource.masters_provisioner"]
BREAKS HERE
-  permission = "${var.everyone_permission}"
-  permission = "${var.team_permission}"
BREAKS HERE
-
-  metadata {
-    ssh_authorized_key = "${var.ssh_authorized_key}"
-  }
BREAKS HERE
-    destination = "/root"
-    destination = "/etc/salt/grains"
BREAKS HERE
-  auto_create_subnetworks = "true"
BREAKS HERE
-  subnet_id                   = "${element(var.subnet_ids, count.index - 1)}"
BREAKS HERE
-  domain_name_servers = "${var.dns_servers}"
-  ntp_servers         = "${var.ntp_servers}"
BREAKS HERE
- name = "lb-g-mktg-${var.install_version}"
BREAKS HERE
-  count = "${var.pitchfork["enabled"]?1:0}"
-  
-  count = "${local.count}"
-  count = "${local.count}"
BREAKS HERE
-    cluster_id            = "${sha256("${var.kube_apiserver_url}-${var.platform}")}"
-    platform              = "${var.platform}"
-    certificates_strategy = "${var.ca_generated == "true" ? "installerGeneratedCA" : "userProvidedCA"}"
-  content     = "${data.template_file.tectonic.rendered}"
-  content     = "${data.template_file.tectonic-rkt.rendered}"
BREAKS HERE
-  tags = {
-    key                 = "Environment"
-    value               = "development"
-    propagate_at_launch = true
-  }
BREAKS HERE
-hostname: ${replace("${aws_instance.instance.private_dns}", ".${var.region}.compute.internal", "")}
-domain: ${var.region}.compute.internal
BREAKS HERE
-    device_name = "xvda"
-    device_name = "xvdb"
BREAKS HERE
-      prefix        = "${var.stack_description}"
BREAKS HERE
-  depends_on = ["null_resource.shell"]
-  filename   = "${path.module}/stdout.${null_resource.shell.id}"
-  depends_on = ["null_resource.shell"]
-  filename   = "${path.module}/stderr.${null_resource.shell.id}"
-  depends_on = ["null_resource.shell"]
-  filename   = "${path.module}/exitstatus.${null_resource.shell.id}"
BREAKS HERE
-variable "patch_group_types" {
BREAKS HERE
-    # The variable casing and naming used here is used to match the
-    # upstream forseti templates more closely, reducing the amount
-    # of modifications needed to convert the Python templates into
-    # Terraform templates.
-  count     = "${var.folder_id != "" ? length(local.server_read_roles) : 0}"
-  role      = "${local.server_read_roles[count.index]}"
-  folder    = "${var.folder_id}"
-  member    = "serviceAccount:${google_service_account.forseti_server.email}"
-  count     = "${var.folder_id != "" && var.enable_write ? length(local.server_write_roles) : 0}"
-  role      = "${local.server_write_roles[count.index]}"
-  folder    = "${var.folder_id}"
-  member    = "serviceAccount:${google_service_account.forseti_server.email}"
BREAKS HERE
-  required_version = "> 0.9.11"
BREAKS HERE
-  value = "${aws_cloudtrail.default.id}"
-  value = "${aws_cloudtrail.default.home_region}"
-  value = "${aws_cloudtrail.default.arn}"
BREAKS HERE
-    tectonic_channel_operator    = "quay.io/coreos/tectonic-channel-operator:0.6.1"
BREAKS HERE
-  user_data = "${data.template_file.user_data_bootnode.rendered}"
BREAKS HERE
-  access_restriction_description    = "blocking public traffic to app service"
-  access_restriction_name           = "vnet_restriction"
-      service_name = "${element(keys(var.app_service_name), count.index)}"
-      vnet_subnet_id = "${var.vnet_subnet_id}"
-      access_restriction_name = "${local.access_restriction_name}"
-      access_restriction_description = "${local.access_restriction_description}"
-}
BREAKS HERE
-  default     = "amazon-eks-node-v*"
BREAKS HERE
-  count              = "${var.create}"
BREAKS HERE
-  name            = "${var.project}-listener"
-  protocol        = "HTTP"
-  protocol_port   = 80
-  loadbalancer_id = "${openstack_lb_loadbalancer_v2.loadbalancer.id}"
-  admin_state_up  = "true"
BREAKS HERE
-      "salt-call --local --file-root=/root/salt/ --output=quiet state.sls default",
-      "salt-call --local --file-root=/root/salt/ --retcode-passthrough --force-color state.highstate"
BREAKS HERE
-    bucket          = "${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}"
-    bucket          = "${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}"
BREAKS HERE
-variable "chamber_parameter_name" {
-  default = "/%s/%s"
-}
-
-module "chamber_parameters" {
-  source = "git::https://github.com/cloudposse/terraform-aws-ssm-parameter-store?ref=tags/0.1.5"
-
-  # These parameters correspond to the kops manifest template:
-  # Read more: <https://github.com/cloudposse/geodesic/blob/master/rootfs/templates/kops/default.yaml>
-
-  parameter_write = [
-    {
-      name        = "${format(var.chamber_parameter_name, local.chamber_service, "kops_cluster_name")}"
-      value       = "${module.kops_state_backend.zone_name}"
-      type        = "String"
-      overwrite   = "true"
-      description = "Kops cluster name"
-    },
-    {
-      name        = "${format(var.chamber_parameter_name, local.chamber_service, "kops_state_store")}"
-      value       = "s3://${module.kops_state_backend.bucket_name}"
-      type        = "String"
-      overwrite   = "true"
-      description = "Kops state store (S3 bucket) URL"
-    },
-    {
-      name        = "${format(var.chamber_parameter_name, local.chamber_service, "kops_state_store_region")}"
-      value       = "${module.kops_state_backend.bucket_region}"
-      type        = "String"
-      overwrite   = "true"
-      description = "Kops state store (S3 bucket) region"
-    },
-    {
-      name        = "${format(var.chamber_parameter_name, local.chamber_service, "kops_ssh_public_key_path")}"
-      value       = "${module.ssh_key_pair.public_key_filename}"
-      type        = "String"
-      overwrite   = "true"
-      description = "Kops SSH public key filename path"
-    },
-    {
-      name        = "${format(var.chamber_parameter_name, local.chamber_service, "kops_ssh_private_key_path")}"
-      value       = "${module.ssh_key_pair.private_key_filename}"
-      type        = "String"
-      overwrite   = "true"
-      description = "Kops SSH private key filename path"
-    },
-    {
-      name        = "${format(var.chamber_parameter_name, local.chamber_service, "kops_dns_zone")}"
-      value       = "${module.kops_state_backend.zone_name}"
-      type        = "String"
-      overwrite   = "true"
-      description = "Kops cluster name"
-    },
-    {
-      name        = "${format(var.chamber_parameter_name, local.chamber_service, "kops_network_cidr")}"
-      value       = "${var.network_cidr}"
-      type        = "String"
-      overwrite   = "true"
-      description = "CIDR block of the kops virtual network"
-    },
-    {
-      name        = "${format(var.chamber_parameter_name, local.chamber_service, "kops_private_subnets")}"
-      value       = "${local.private_subnets}"
-      type        = "String"
-      overwrite   = "true"
-      description = "Kops private subnet CIDRs"
-    },
-    {
-      name        = "${format(var.chamber_parameter_name, local.chamber_service, "kops_utility_subnets")}"
-      value       = "${local.utility_subnets}"
-      type        = "String"
-      overwrite   = "true"
-      description = "Kops utility subnet CIDRs"
-    },
-    {
-      name        = "${format(var.chamber_parameter_name, local.chamber_service, "kops_availability_zones")}"
-      value       = "${join(",", local.availability_zones)}"
-      type        = "String"
-      overwrite   = "true"
-      description = "Kops availability zones in which cluster will be provisioned"
-    },
-  ]
BREAKS HERE
-    iops        = "${var.root_volume_iops}"
BREAKS HERE
-  count = 2
-  count         = 2
-  bucket        = "${var.project_name}-${random_id.tf_bucket_id.*.dec[count.index]}"
BREAKS HERE
-  destination_cidr_block    = "${var.peer_cird_block}"
-  destination_cidr_block    = "${var.peer_cird_block}"
BREAKS HERE
-  default     = ""
-  default     = false
BREAKS HERE
-  vault_dns  = "${module.quorum_vault.vault_dns}"
-  vault_dns  = "${module.quorum_vault.vault_dns}"
-  vault_dns  = "${module.quorum_vault.vault_dns}"
-  vault_dns  = "${module.quorum_vault.vault_dns}"
-  vault_dns  = "${module.quorum_vault.vault_dns}"
-  vault_dns  = "${module.quorum_vault.vault_dns}"
-  vault_dns  = "${module.quorum_vault.vault_dns}"
-  vault_dns  = "${module.quorum_vault.vault_dns}"
-  vault_dns  = "${module.quorum_vault.vault_dns}"
-  vault_dns  = "${module.quorum_vault.vault_dns}"
-  vault_dns  = "${module.quorum_vault.vault_dns}"
-  vault_dns  = "${module.quorum_vault.vault_dns}"
-  vault_dns  = "${module.quorum_vault.vault_dns}"
-  vault_dns  = "${module.quorum_vault.vault_dns}"
BREAKS HERE
-  task_definition = "${aws_ecs_task_definition.web-api.family}:${max("${aws_ecs_task_definition.web-api.revision}", "${data.aws_ecs_task_definition.web-api.revision}")}"
-    security_groups = ["${var.security_groups_ids}", "${aws_security_group.ecs_sg.id}"]
-    subnets         = ["${var.subnets_ids}"]
-    target_group_arn = "${aws_alb_target_group.alb_target_group.arn}"
-  depends_on = ["aws_alb_target_group.alb_target_group"]
BREAKS HERE
-    bucket_prefix = "${var.stackname}-calculators-frontend-internal-elb"
BREAKS HERE
-output "function_name" {
-  description = "The unique name of your Lambda Function."
-  value       = "${aws_lambda_function.lambda.function_name}"
-}
-
-  value       = "${aws_lambda_function.lambda.arn}"
-  value       = "${aws_lambda_function.lambda.invoke_arn}"
BREAKS HERE
-MKTG Load balancer
BREAKS HERE
-  security_groups = [ "${module.kafka-security-groups.kafka_broker_security_group_ids}"]
BREAKS HERE
-    name = "S3-credentials-access"
-    name = "SQS-queue-access"
-    name = "Allow-autoscaling-complete-action"
-    name = "rancher_server_s3_policy"
-    name = "rancher_server_sqs_policy"
-    name = "complete_autoscaling_hooks"
BREAKS HERE
-  count = "${var.create ? 1 : 0}"
-  description = "Database parameter group for ${var.identifier}"
BREAKS HERE
-
-  name       = "my-keypair"
-  name            = "mynetwork"
-  region          = "SBG3"
-  name           = "port_private_instance"
-  name        = "my_private_instance"
BREAKS HERE
-  location                = "${local.west_rg_location}"
BREAKS HERE
-  user_data                   = "${template_cloudinit_config.cloud_config.rendered}"
BREAKS HERE
-      "wget -P /tmp ${local.jenkins_master_url}/jnlpJars/jenkins-cli.jar"
-      "/tmp/bootstrap.sh ${aws_instance.ec2_jenkins_slave.tags["Name"]}"
-  # Cleanup sensitive files
-      "sudo rm /tmp/secret",
-      "sudo rm /tmp/jenkins-cli.jar",
-      "sudo rm /tmp/secret.groovy"
BREAKS HERE
-    prefix                                 = "/"
-      days = 3
BREAKS HERE
